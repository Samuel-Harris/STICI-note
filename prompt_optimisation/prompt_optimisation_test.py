# -*- coding: utf-8 -*-
"""STICI-note_prompt_optimisation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xK_LLmKdwHRVbh6upaRW8NfcGNRsRifE
"""

# !pip install dspy
# !pip install langchain_huggingface
# !pip install chromadb
# !pip install transformers==4.39.0

# import torch
# torch.set_default_device("cuda")

from dspy.evaluate import Evaluate

import dspy
from dspy import Example, Prediction
from dspy.retrieve.chromadb_rm import ChromadbRM
from langchain_huggingface import HuggingFaceEmbeddings
from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction
import pandas as pd
from transformers import pipeline
from dspy import TypedPredictor

tiny_llama = dspy.HFModel(model="TinyLlama/TinyLlama-1.1B-Chat-v1.0")

embeddings_function = SentenceTransformerEmbeddingFunction(model_name="all-MiniLM-L6-v2")
retriever_model = ChromadbRM(
    "example_collection",
    'example_dbb',
    embedding_function=embeddings_function,
    k=5
)

dspy.settings.configure(lm=tiny_llama, rm=retriever_model)

# from google.colab import drive
# drive.mount('/content/drive')
#
# dir = "/content/drive/MyDrive/stici-note"
# documents_df = pd.read_csv(f"{dir}/documents.csv")
# no_answer_questions_df = pd.read_csv(f"{dir}/no_answer_train_validate_questions.csv")
# single_passage_questions_df = pd.read_csv(f"{dir}/single_passage_train_validate_questions.csv")
# multi_passage_questions_df = pd.read_csv(f"{dir}/multi_passage_train_validate_questions.csv")

no_answer_response = "The answer to your question is not in the provided document."
def generate_example(row, documents):
    return Example(question=row[1]["question"],
                   answer=row[1]["answer"] if "answer" in row[1] else no_answer_response,
                   document=documents[documents["index"] == row[1]["document_index"]]["text"].iloc[0]).with_inputs("question", "document")

# no_answer_questions = list(map(lambda row: generate_example(row, documents_df), no_answer_questions_df.iterrows()))
# single_passage_questions = list(map(lambda row: generate_example(row, documents_df), single_passage_questions_df.iterrows()))
# multi_passage_questions = list(map(lambda row: generate_example(row, documents_df), multi_passage_questions_df.iterrows()))
# validate_questions = no_answer_questions + single_passage_questions + multi_passage_questions

class GenerateAnswer(dspy.Signature):
    """Answer questions with short factoid answers."""

    context = dspy.InputField(desc="may contain relevant facts")
    question = dspy.InputField()
    answer = dspy.OutputField(desc="if there is an answer, it will consist of information from the context")

class RAG(dspy.Module):
    def __init__(self, num_passages=3):
        super().__init__()

        self.retrieve = dspy.Retrieve(k=num_passages)
        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)

    def forward(self, question, document):
        context = self.retrieve(question).passages
        prediction = self.generate_answer(context=context, question=question)
        return dspy.Prediction(context=context, answer=prediction.answer)

from dspy.teleprompt import BootstrapFewShotWithRandomSearch

# answer_embeddings = {example.question: embeddings_function([example.answer]) for example in validate_questions}

from sklearn.metrics.pairwise import cosine_similarity

# cosine_similarity(answer_embeddings[validate_questions[0].question], embeddings_function(["The juice runs dry."]))[0][0]

# Validation logic: check that the predicted answer is correct.
# Also check that the retrieved context does actually contain that answer.
def semantic_similarity(example: Example, pred: Prediction, trace=None) -> float:
    pred_embedding = embeddings_function([pred.answer])
    actual_embedding = answer_embeddings[example.question]
    return cosine_similarity(actual_embedding, pred_embedding)[0][0]

# Set up a basic teleprompter, which will compile our RAG program.
teleprompter = BootstrapFewShotWithRandomSearch(metric=semantic_similarity)

compilation_path = "../evaluation/compiled.json"

# Compile!
compiled_rag = teleprompter.compile(RAG(), trainset=validate_questions)
compiled_rag.save(compilation_path)

compiled_pipeline = RAG()
compiled_pipeline.load(path=compilation_path)

dir = "../data"
test_documents_df = pd.read_csv(f"{dir}/documents.csv")
no_answer_test_questions_df = pd.read_csv(f"{dir}/no_answer_test_questions.csv")
single_passage_test_questions_df = pd.read_csv(f"{dir}/single_passage_test_questions.csv")
multi_passage_test_questions_df = pd.read_csv(f"{dir}/multi_passage_test_questions.csv")

no_answer_test_questions = list(map(lambda row: generate_example(row, test_documents_df), no_answer_test_questions_df.iterrows()))
single_answer_test_questions = list(map(lambda row: generate_example(row, test_documents_df), single_passage_test_questions_df.iterrows()))
multi_passage_test_questions = list(map(lambda row: generate_example(row, test_documents_df), multi_passage_test_questions_df.iterrows()))
test_questions = no_answer_test_questions + single_answer_test_questions + multi_passage_test_questions

test_questions[0].inputs()

evaluator = Evaluate(devset=["test_questions"], display_progress=True)

evaluator(compiled_pipeline, semantic_similarity)

def g(arg=None):
    return arg

def f(x):
    return g(**x[0])

f([["a"]])

compiled_rag(question, document)