index,source_url,text
0,https://enterthegungeon.fandom.com/wiki/Bullet_Kin,"Bullet Kin
Bullet Kin are one of the most common enemies. They slowly walk towards the player, occasionally firing a single bullet. They can flip tables and use them as cover. They will also deal contact damage if the player touches them.

Occasionally, Bullet Kin will have assault rifles, in which case they will rapidly fire 8 bullets towards the player before reloading. When an assault rifle wielding bullet kin appears, there will often be more in the same room.

On some occasions the player will also encounter incapacitated Bullet Kin lying on the floor. These Bullet Kin are props and disintegrate upon touch. They can be found in mass quantity in Oubliette.

In the Black Powder Mine, they can also ride Minecarts. In fact, if there are any unoccupied Minecarts within the room, they will take priority by walking towards them to ride in.

Trivia
Bullet Kin wield Magnums. Assault-rifle wielding Bullet Kin wield AK-47s.
Incapacitated Bullet Kin can be found in the Oublilette and Cannon's boss room.
In the Oubliette and the boss fight against Agunim, some room props resemble Bullet Kin poking out from inside barrels. This is likely a visual joke on a bullet inside a gun barrel.
In the Portuguese translation of the game, they are known as ""Balùnculo"", a portmanteau of the words ""bala"" (bullet) and ""homúnculo"" (homunculus).
Bullet Kin makes a playable appearance in the platform fighting games Indie Pogo and Indie Game Battle.
Bullet Kin is also a crossover skin in the game Riverbond.
Bullet Kin also has a cameo as lower and upper parts of a skin in the game Fall Guys: Ultimate Knockout.
Veteran Bullet Kin
Veteran Bullet Kin are similar to regular Bullet Kin, but have a higher rate of fire, higher shot speed and attempt to predict the player's movements. They also run faster than normal Bullet Kin, allowing them to catch up with the player quickly if they attempt to take cover.

They fire 4 bullets in a row. If the player moves out of sight from one then the Veteran will pause his attack and then fire the remaining bullets once he has caught up.

Bandana Bullet Kin
Bandana Bullet Kin behave like regular Bullet Kin, but their fire rate is heavily increased. Bandana Bullet Kin also have a higher magazine size than Bullet Kin that wield AK-47s, making them more relentless.

Trivia
Bandana Bullet Kin wield Machine Pistols.

Tanker
Tankers behave like regular Bullet Kin, but have higher health and higher rate of fire. Tankers can be spawned by Treadnaught.

Their rate of fire is slightly lower than that of Bandana Bullet Kin, but they are just as relentless.

Trivia
Tankers wield AK-47s.
The Tanker's expression in his Ammonomicon profile resembles that of the Bullet's avatar when talking to an NPC.

Minelet
Minelets behave like regular Bullet Kin, but will occasionally hide under their hard hat, deflecting incoming projectiles. They will then pop out from underneath their hard hat, releasing a ring of bullets in all directions.

Trivia
Minelets are a possible reference to Mets from the Mega Man series because of their similar behavior. They both hide under their helmets to protect themselves and attack when they emerge.

Cardinal
Cardinals behave like regular Bullet Kin, but have 50% higher health and will occasionally pause to shoot a group of 5 bullets that will home in on players.

Though a minor effect, these bullets spin around each other as they travel, similar to Apprentice Gunjurers. This occasionally allows them to slip through corners as only some of the bullets will be destroyed.

Trivia
Although normally seen in the Abbey & Hollow, a single cardinal may be seen in the first floor, tending to a small cemetery filled with gravestones. He is the only enemy in that room.
""Of the gun"" is a play on the phrase ""of the cloth"", meaning a member of the clergy.

Shroomer
Shroomers behave like regular Bullet Kin, but have double health and fire two bullets in a V shape. Their bullets can be avoided by standing still, but this can jeopardise dodging the more accurate projectiles of any accompanying enemies. They may also spawn in Gungeon Proper, though rarely.

Trivia
Shroomers will misfire upon spawning, having to stand up after being spawned.

Ashen Bullet Kin
Ashen Bullet Kin have a higher rate of fire and higher shot speed than regular Bullet Kin. They seem to alternate between firing directly at the player and predicting their movements when shooting.

In some rooms of the Forge, Ashen Bullet Kin have the ability to spawn out of ashen statues, which allows them to catch the player off guard.

Trivia
The quote ""Cinder Fella"" is a clear wordplay between ""Cinderella"", the famous fairytale, and ""Fella"" a familiar term for a friend or a person that you consider close.
The French traduction of this quote ""Balle au bois dormant"" is also a wordplay between the fairytale ""La belle au bois dormant"" (Sleeping Beauty) and ""Balle"" (Bullet)
Like its normal counterpart, the Ashen Bullet Kin has a cameo as lower and upper parts of a skin in the game Fall Guys: Ultimate Knockout.

Mutant Bullet Kin
Mutant Bullet Kin behave like regular Bullet Kin, but have higher health and will occasionally stop to release a cone of poison creep. They are immune to Poison effects. The cone of poison can only be released horizontally, so attacking from above or below are the safer options.

Trivia
Its subtitle references Old Faithful, a geyser in Yellowstone National Park.

Fallen Bullet Kin
Fallen Bullet Kin walk towards the player, firing spreads of 3 fire-shaped bullets. They leave behind a small patch of fire upon death. Despite this, they are not immune to fire damage.

Notes
Fallen Bullet Kin will leave their pools of fire in the area where they took the blow that killed them. It will not be spawned where their death animation ends.
Trivia
Fallen Bullet Kin wield Pitchforks.
The sounds that Fallen Bullet Kin make are lower pitched versions of regular Bullet Kin.
These enemies can also be spawned by Lament Configurum.
A portrait of a Fallen Bullet Kin can be seen in the Abbey of the True Gun.
In the Portuguese translation of the game, they are known as ""Ex-Balùnculo"" (Ex-Bullet Kin), so in that version of the game, it is implied that they are no longer a type of bullet kin, this transformation may have happened through their death, where they were sent to the Sixth Chamber.

Keybullet Kin
Keybullet Kin run away from the player, and drop a key upon death. However, if the player does not manage to kill them in time, they will disappear.

Unlike other Bullet Kin, Keybullet Kin do not deal contact damage if they run into the player.

Jammed Keybullet Kin drop 2 keys instead of 1. These Jammed variations run faster and will take less time to teleport away from the player if they are not destroyed quickly.

If a Keybullet Kin is knocked into a pit, it will not drop a key.

The chances for a specific number of Keybullet Kin to spawn on a floor are:

0	1	2
50%	30%	20%
Trivia
Keybullet Kin may appear in boss arenas during the Boss Rush.
Keybullet Kin have a small chance to appear in elevator rooms at the start of a floor.
Killing 15 Keybullet Kin unlocks the Springheel Boots.
Keybullet Kin and Chance Kin's behavior is modeled after the Crystal Lizards from the Souls series and the Wandering Madness from Bloodborne. Both are harmless ""enemies"" that quickly run away from the player—often leading them directly into the path of danger—and despawn after a short time, with the promise of valuable loot if they are killed.

Chance Kin
Chance Kin run away from the player, and drop a random pickup upon death. However, if the player does not manage to kill them in time, they will disappear. Jammed Chance Kins have a chance to drop twice the loot.

The chances for a specific number of Chance Kin to spawn on a floor are:

0	1	2
50%	30%	20%
Trivia
Chance Kin may appear in boss arenas during Boss Rush.
Chance Kin have a small chance to appear in elevator rooms at the start of the floor.
The Chance Kin's subtitle is a reference to the common phrase ""No Second Chances.""
Chance Kin block player movement during their death animation.
Chance Kin can appear in the same room as a Keybullet Kin.
Keybullet Kin and Chance Kin's behavior is modeled after the Crystal Lizards from the Souls series and the Wandering Madness from Bloodborne. Both are harmless ""enemies"" that quickly run away from the player—often leading them directly into the path of danger—and despawn after a short time, with the promise of valuable loot if they are killed.

Confirmed
Confirmed are mysterious cloaked Bullet Kin. They stroll towards the player, occasionally stopping to fire four slithering lines of bullets at the player from under their hoods.

Confirmed do not appear in specific room layouts. Instead, they have a small chance to replace an enemy in any room. Only one Confirmed can appear on each floor.

Defeating ten Confirmed unlocks the Yellow Chamber.

Trivia
The splash art for Confirmed show them having dozens of red eye-like bullets residing within their cloaks. This bears resemblance to the High Priest's splash art.
The Confirmed are referred to by numerous other names in the game's code, such as 'Kaliber Cultist', and 'Faceless Cultist'.

Red-Caped Bullet Kin
Bullet Kin with red capes will rarely appear in random rooms after at least one Past has been killed. These Bullet Kin do not attack the player, and wander aimlessly. If it is the only enemy remaining in the room and it is left alone for long enough, it will disappear. After this happens 5 times, The Bullet is unlocked, and Red-Caped Bullet Kin stop spawning.

The chances that one will spawn on the six main floors are as follows:

1	2	3	4	5	6
8%	8%	12%	16%	20%	25%
A floor can only contain a maximum of one caped bullet (with one known exception outlined below). There is a 49.95% chance of one or more Red-Caped Bullet Kin appearing in a full run through the Forge, and a 62.46% chance on a run through Bullet Hell.

Trivia
Red-Caped Bullet Kin wield Magnums, but do not fire them or point them at the player.
Red-Caped Bullet Kin do not deal contact damage unless they are jammed.
Red-Caped Bullet Kin's design may be based on The Kid from I Wanna Be The Guy.
Rooms created by the Drill can have a Red-Caped Bullet Kin spawn inside them, even if a Red-Caped Bullet Kin has already appeared on that floor.
It's possible for Red-Caped Bullet Kin to appear in the Aimless Void and Secret Floors such as the Oubliette.
Red-Caped Bullet Kin are not attacked by companions.
Red-Caped Bullet Kin will teleport away if the room contains an enemy that cannot be killed, such as Gunreapers or Dead Blows."
1,https://www.dropbox.com/scl/fi/ljtdg6eaucrbf1aksw5rm/c2%20-%20session%2050%20-%20underground.docx?rlkey=ioqwgkd14i5xk20i3fp38nzgs&e=1&dl=0,"---The Paths through the Underground/Underdark---(9 days of travel)
Wandering through the dark tunnels, the rushing sounds of the underground river begin to fade as it diverges from the cavern. You walk on for miles, the smell of hard water and wet earth. Natural chambers and cavern passways are chained together by the stretches of burrowed earth left in the wake of this massive worm-like creature. Clusters of crystal and other beautiful minerals occasionally line the walls and ceilings of the chambers, glittering with the little light you have to shove back the darkness.

Day 1 goes without issue... sleep.

Day 2 – Ropers
After a few miles of winding tunnel, you emerge in a smaller grotto of stalactites and stalagmites dripping with condensation. Unsure if the same underground river, or another water source, is nearby, you can see quite a bit of ground water does funnel down into this area. Seeking the next burrowed entrance left by the Kryn...
---ENCOUNTER – Ropers x 2---
Day 3 goes without issue...sleep.

Day 4 - Kobold Trap
Part way into the journey, the path becomes a protracted tunnel, snaking through the rock for hours without end. Eventually, you begin to notice other smaller tunnels intersecting with the burrowed canal. They appear partially ruined by this fresher tunnel, many of them now filled or partially collapsed.

They are no more than 2-3 feet wide, and numerous (dozens).

In some of the rubble, you can find broken tools... a hammer, some soiled leather, a knife.

The tunnel finally seems to open into a small 15-foot high, 30ft long chamber of dirt and rock, where a rather rancid smell lingers. Glancing within, a handful of the smaller tunnels seem to intersect with it, and whomever enters first (if not Cad), their leg is SNARED by a noose and they must make a Dexterity Saving Throw (DC 15) or be lifted into the air to dangle from a small trap (restrained, DC 16 to escape). The snare also drags a cable tied to numerous pans and metal scraps, making a ruckus!

Chattering and tiny warcrys begin to fill the tunnel from all sides... as dozens of small kobolds rush into the room, and from behind!

-ENCOUNTER: Kobolds x 26, Kobold Inventor x 1-
“Loud food! Loud meal!”

When seeing the group, they bark and growl. (if noticed, they appear rather fearful)

“You! Give us stuffs! Give us foods! Drop things you have, or we stab stab!”

If asked about tunnel “Big worm eat through! Bring ingoeth! In and out, gone quick, leave mess!”

They must parlay with them, avoiding a battle with a significant trade, or intimidation. Otherwise, a fight ensues! Either way, two kobolds are too scared and freeze up. They are brothers Spurt and Bex, scavenger kobolds. They are timid, but know the tunnels well...ish?"
2,https://bytes-and-nibbles.web.app/bytes/stici-note-part-1-planning-and-prototyping,"Semantic and Textual Inference Chatbot Interface (STICI-Note) - Part 1: Planning and Prototyping

The start of my RAG to riches story

STICI-note

Published: Mon, 27 May 2024

Last modified: Tue, 04 Jun 2024

Introduction

In this three-part series, I will be talking you through how I built the Semantic and Textual Inference Chatbot Interface (or STICI-note for short), a locally run RAG chatbot that uses unstructured text documents to enhance its responses. I came up with the name when I was discussing this project with a friend and asked him whether he had any ideas of what to call it. He said, ""You should call it sticky because it sounds funny."" The name... stuck...

The code for this project is available here.

In this part, I will be planning the project from the tech stack to the techniques I will use, and I will be building a prototype. I will be discussing all of the choices I made and all of the choices I didn’t make, so I hope you find this insightful. Without further ado, let’s get started.

The Problem

In my spare time, I occasionally play Dungeons and Dragons (DnD), a tabletop roleplaying game, and the stories are often told over several months, so details can be easily lost or forgotten over time. I can write notes on my laptop, but sometimes regular text search does not always provide me with the results I want when trying to search for specific notes. Some common examples include when a keyword is used often (e.g., I might write a lot about the actions of “Miles Lawson,” but only one segment of text might describe who he is, making searching for information on his character like finding a needle in a haystack) or when I simply cannot think of the correct keyword to search (e.g., what if I search “silver” instead of “gold”?).

One day, I thought to myself that it’d be great if I had a tool that I could write my DnD notes into in an unstructured way and retrieve the information at any time with simple questions like “Who is Miles Lawson?” or “How much silver did I pay for a stay in ye olde tavern?”. This tool could be extended to be used for querying my notes on many things that are not available online (and therefore not searchable on a search engine), such as documentation on software that I build, notes on things that I’m learning about, such as AWS cloud infrastructure, and my diary of my deepest thoughts and feelings (at least I hope this is not available online). And thus, I decided to start working on STICI-note because the tools available online that do this cost money and run on the cloud, and I’m a tight-fisted guy who’s very sceptical about company promises to not sell your data.

Narrowing Down Features

As with all projects, I began by deciding what features I needed from this tool.

Required features:

Chatbot that you can ask questions and get answers in response (conversational memory is not required).
Information is taken from an unstructured text file.
It must be able to tell me if it doesn’t know the answer to my question.
Fast.
Efficient enough to run on my MacBook with other programs without any performance issues.
Locally run for privacy and to ensure it will always be free, runnable, and consistent.
Conversational memory is the memory of previous interactions given to an LLM. I decided not to require it as a feature because I just need the AI to answer my questions about the given text. It might be added as a feature in the future if I feel like I need it, but I do not plan to include it in the initial version of STICI-note.

I knew that limiting it to running on my M1 MacBook with 8 GB of memory would greatly limit the performance of the tool as I would not be able to access truly large language models like GPT-4 and Claude 3 Opus, but I decided to do it anyway primarily for privacy but also to remove dependencies on external organisations to reduce the maintenance required for the tool in the future.

Planning How to Evaluate and Compare Solutions

If you don’t evaluate a solution, how do you know whether it’s an effective solution? You don’t.

I next planned how I would evaluate different variations of the tool. While I do not evaluate anything in this part, I decided to sketch out a rough plan of how I would evaluate different solutions to encourage designing a testable AI in the same way that Test-Driven Development (TDD) encourages you to write testable code.

At first, I considered using Wikipedia pages as the data source and making my own questions about the content of the pages before I realised that this would lead to data leakage as many LLMs are trained on Wikipedia data.

An alternative dataset that I considered using for evaluation is the TREC 2024 RAG Corpus. This is a 28 GB corpus of text documents scraped from the internet. This corpus comes with thousands of queries, and the relevant documents for each query have been tagged as such. This is an amazing corpus for training and evaluating vector DataBase (DBs). Ignoring the fact that its questions do not come with answers, meaning I would have to write my own answers to use the document, there is one glaring flaw that makes it unusable for my use case: the documents are generally relatively short and describe a large variety of things. In my use case, I expect documents to be long and typically written about the same topic. If I were to use the corpus, I would have to stick documents together to present a realistic challenge in the semantic search of the vector DB vector space, but as each document will likely be about very different topics (e.g., one might be about aviation while another might be about economics), context retrieval would be unrealistically easy.

Another alternative evaluation dataset that I considered using was a synthetic dataset. By following a method like this, I can use an LLM to generate synthetic context and questions automatically. I decided not to do this as I was concerned that this would produce bad-quality data with a massive bias towards things an LLM might already know, despite the use case expecting data that the LLM does not already know.

Because the documents in my evaluation corpus need to be thousands of words in length while staying relevant to a topic and they need to include information that will not be in the LLM’s training data, I decided that it would be best to manually curate a small dataset to evaluate my models. I plan to create documents from sources on the internet like videogame wikis, people’s blogs, and scientific journals and write my own pairs of questions and answers about them. I will then evaluate the difference between the model’s answer and my answer using a semantic similarity score.

RAG vs a Really Big Context Window vs Infinite Context

To be able to answer questions about documents, the LLM would need to have access to information from the documents. I thought of three potential solutions for this:

Retrieval Augmented Generation (RAG)
An LLM with a really big context window
An LLM that supports infinite context length
Using an LLM with a really big context window such as Anthropic’s Claude 3 and Google’s Gemini 1.5 would certainly give me the best results as it would allow inference using completely unfiltered and uncompressed context as they can handle inputs of over 700,000 words, but these models are closed-source, and there is absolutely no chance of a model of this size fitting into my tiny M1 MacBook with 8 GB of memory.

By “an LLM that supports infinite context length,” I mean models like Mamba, Megalodon, and Infini-attention that compress context into a finite space. I decided not to use a model like this for two main reasons. Firstly, I have concerns about the performance. These architectures are in their infancy, and I do not expect them to outperform equivalently-sized traditional transformers. Secondly, as these architectures are very new and experimental, I do not expect much support for them, especially for Apple’s M-series of chips, which have their own graphics API, metal, that is required for GPU acceleration on my MacBook. These architectures are very interesting, and I would love to try them out, but for this project, I will have to settle for a more tried-and-tested approach.

The more tried and tested approach that I settled with is RAG. It is an incredibly popular technique for allowing LLMs to make use of information that is too big to fit in their context windows. This technique is known to perform very well, is incredibly well supported by LLM frameworks like LangChain and llamaindex, and works well in resource-constrained environments like on my laptop. Given all this, RAG was an obvious choice.

Optimising Models for Limited Memory Environments

Next, I decided to investigate what kinds of optimisation strategies were available to use to try to fit bigger models into my M1 chip, as bigger LLMs typically perform better (I know, a groundbreaking revelation). To optimise the LLMs that I use, I considered four different techniques:

Quantisation
Model pruning
Model/knowledge distillation
AirLLM
Quantisation is the most common method for making ML models smaller (and therefore faster and more capable of fitting into smaller spaces). It’s well known for improving speed and memory usage with little loss in accuracy in return, which makes it very popular for production-level AI. Quantising a model would require being able to fit it into your GPU, but I’m trying to quantise a model so that it can fit into my GPU, so without additional computing power, it’s a bit of a chicken and egg problem. Luckily, because this is such a popular technique, there are many quantised versions of large, high-performance LLMs available on HuggingFace that I can use, so there is no need to do this myself.

Model pruning is a less common method for reducing model sizes, but it is not a technique that one should overlook. This is a technique that can be combined with quantisation (or used on its own) to further reduce models at the expense of accuracy, but I do not plan to apply it myself due to its complexity and the fact that quantisation has the same effect. There are pruned models available on HuggingFace, but they don’t typically perform as well as equivalently sized quantised models, so I do not plan to use any unless they have particularly good evaluation results on a common LLM benchmark.

Model/knowledge distillation is another size reduction technique that I considered. Unlike the previous techniques, model distillation can actually improve accuracy in domain-specific tasks while making a smaller model. As with quantisation and model pruning, I will use pre-distiled models, but I will not distil any models myself due to the computing power it requires (which admittedly is far less than training a model from scratch) and the complexity it would add to the project.

The final optimisation technique that I considered, AirLLM, is quite different from the others in that instead of optimising the model weights, it optimises the model inference. Typically, LLMs are loaded onto the GPU in their entirety, requiring a lot of VRAM to run the larger, better-performing models. AirLLM is an open-source library that tackles this problem by using layered inference, an inference technique that involves loading layers individually when they are needed instead of all at once. This allows larger models to fit into smaller memory spaces without degrading performance. This method definitely has a high potential for accuracy, but I decided not to use it as I am concerned about compatibility and reliability issues as it is a new tool and the GitHub repo has been developed by a single person, so support for it is likely to be limited. Additionally, my M1 chip only has 8 GB of memory shared between the CPU and GPU, which is excellent for reducing data loading overhead costs, but it means that larger models that require AirLLM will be loaded directly from the SSD, so I am concerned that the model layer loading and unloading will become a massive bottleneck when doing inference on larger models. I will reconsider this option if I find that the models that can run on my MacBook do not have satisfactory accuracy.

What Models Even Run on My MacBook?

After getting an idea of what kinds of optimisation techniques were available, I decided to conduct some tests to find out what LLMs would actually run on my MacBook. You could argue that since I am only building a prototype right now, I only need to find one LLM that performs well on my MacBook, but I decided to find five models instead to give me an idea of what kinds of models I will be able to use. In particular, I wanted to know how big the models I could run were and what precision the weights would likely be.

I tested models that I had heard were good or showed decent results on the Hugging Face H4 Open LLM Leaderboard. I found LM Studio incredibly useful for testing out LLMs without having to write any code, which saved me a lot of time. Below are the five suitable models that I found that could run on my MacBook and were fast enough to satisfy me:

tinyllama-1.1b-chat-v1.0 Q6_K
Phi 3 Q4_K_M
bartowski/dolphin-2.8-experiment26-7b-GGUF Q3_K_L
mgonzs13/Mistroll-7B-v2.2-GGU
QuantFactory/Meta-Llama-3-8B-Instruct Q3_K_M
These models range from 1.17 GB up to 4.02 GB in size. I chose not to use any models that were any larger than 4 GB, as with only 8 GB of memory available, I expect that models that are any bigger would seriously impact the other applications that the user (i.e., me) is running on their device.

I will likely test out more models than this while testing out different configurations for the tool, but for the prototype, this is enough.

A Model Without a Framework is Like a Car Engine Without a Chassis

To run my models, I could have written a framework for loading, unloading, and executing the models, passing context and queries to the models, and integrating the vector DB (more on that later) with the inference model from scratch, but I didn’t because I’m not insane and I am not trying to learn how to make ML frameworks. A lot of university students (myself included) are conditioned to try to build things from scratch for fear of plagiarism and because they are used to building things from scratch as a learning exercise (a very effective one in my opinion), so it’s difficult to unlearn the DIY mindset, but it’s simply a lot quicker and a lot more reliable to use libraries than to reinvent the wheel. Saying that, I decided to use a relatively simple tech stack.

Python was an obvious choice for me, given that I have a lot of experience with it and that it has an abundance of support for machine learning applications. I decided to use LangChain to orchestrate my RAG process from the vector DB to the inference, as it is a flexible tool for composing NLP pipelines. It is very popular and reliable, and it includes a lot of tools that make developing NLP applications easier. I considered using LlamaIndex as it is built more specifically for RAG applications, but LangChain is more general-purpose, which I expect will make it more extensible for times when I might want to add more features in the future. Additionally, I am more likely to use LangChain again for other applications in the future, so the experience will be more useful. I also considered using LitGPT, but I had some issues getting it to work with the M1 chip’s Metal Performance Shaders (MPS), so I decided not to use it for fear of incompatibility. LitGPT is also intended more for training and fine-tuning LLMs, so it is likely not the best tool for simply deploying them in an application.

To run inference on my models, I will need another library to actually execute the model. As I am using a range of pre-trained models, I will mainly use HuggingFace’s transformers library and the Python bindings for llama.cpp library to load and execute models, as these provide simple interfaces for inference, and I don’t need the additional control that deep learning frameworks like TensorFlow, PyTorch, and JAX provide as I’m using pre-trained models. As I mentioned earlier, AirLLM is still on the table if I need better performance, but I will find out while evaluating models whether this is necessary.

Magnitude + Direction DBs

Since I was using RAG, I needed a vector DB. Deciding which one to use was the final step of the research and also the most difficult one, as vector DBs are the technology that I am least familiar with. For the vector DB, my main requirements were simple: it needed to be lightweight, locally runnable on a laptop, fast, and compatible with my MacBook. Lightweight and locally runnable sound like similar things, but I mean different things by each phrase. The locally runnable one is quite self-explanatory, but by lightweight, I mean quite minimal computation requirements that don’t add features like heavy amounts of redundancy and heavy caching, which are useful for large-scale systems, but will simply drain resources in my application that is designed to run alongside an LLM and the user’s other applications on 8 GB of memory shared between the CPU and GPU.

I considered sixteen different vector DBs, but there were three different solutions that stood out to me for my use case: Chroma, Qdrant, and Vespa. These were all lightweight vector DBs that fit all of my requirements, but I decided to pick Chroma out of the three of them because it has a very batteries included approach, which is very useful for getting an application working quickly.

Below is an overview of my planned tech stack. I did not include any UI, as there are many options for this, and I plan to decide on the UI when building the final product.

The tech stack and data flow that I plan to use for STICI-note.
The tech stack and data flow that I plan to use for STICI-note.

Making the Prototype (A.K.A. the Easy Part)

The final step was to actually build the prototype/proof of concept (PoC) to find out whether this idea can actually be implemented using the tech stack that I have picked. I decided to implement it using the smallest of the 5 models that I had found earlier, tinyllama-1.1b-chat-v1.0 Q6_K, to minimise the chances of having any memory issues.

You can find my prototype code here.

For the prototype, I decided to make very basic configurations for everything to develop it quickly, and I tested it by asking a question about Grace Hopper, one of the most influential programmers to have ever lived, in two versions of the prototype: one with Grace Hopper’s Wikipedia page in its vector DB and one with Alan Turing’s Wikipedia page in its vector DB.

The question was:

“What programming language was Grace Hopper known for creating?”

As expected, the version with Alan Turing’s Wikipedia page failed miserably, hallucinating its entire response:

Grace Hopper was a computer scientist who is best known for her work on developing an early version of BASIC, which was one of the first low-level programming languages designed for mainframe computers. Hopper worked at Digital Equipment Corporation (DEC) in the mid-1960s and was responsible for creating several key BASIC versions that were used by DEC's mainframe computers. In 1973, she left DEC to work on a new low-level programming language called PASCAL, which became one of the most popular programming languages of its time. However, her contributions to BASIC are still remembered today as it was one of the first widely used and easy-to-understand programming languages for mainframe computers.

This behaviour of hallucinating will need to be fixed in the final product, likely through prompt engineering.

As expected, the version with Grace Hopper’s Wikipedia page gave a much more accurate response but it unexpectedly contained far more spelling errors:

Grace Hopper is credited with creating the first computer programming languaige called FLOW-MATIC. This language was developed in 1954 and was based on English words. Hopper's work on this languaige led to a machine-indepenent programming language, COBOL, which used English words instead of symbols for input and output. Her compiler, A-0, also utilized the FLOW-MATIIC language as its basis for compiling programs into executable code. In 1952, Hopper published her first paper on the topic of compiler design, ""The Compiler"" in the IEEE Transactions on Computers, in which she discussed her work with A-0 and other programming languaiges.

Unfortunately, this response was not free from hallucinations either. Hopper’s A-0 compiler did not use the FLOW-MATIC programming language, and she never published a paper called “The Compiler” in IEEE Transactions on Computers (a real journal that is not mentioned in the Wikipedia page). It looks like hallucinations are likely to be a major issue for this tool, but that is a problem I will solve when refining the AI.

On the bright side, inference was ~120 tokens/second, so at least this model will output words much faster than I can read them.

Conclusion

In this blog, I built a locally run prototype for my chatbot for querying unstructured text documents. It doesn’t have a UI, and it hallucinates a lot, but it is nonetheless capable of querying unstructured text.

It’s such a shame that after I had done the research and written all of the code, while I was writing this blog, I read about llmware, a very promising Python framework for building RAG pipelines with small models (sound familiar?). It was even chosen for GitHub Accelerator 2024, a competition for open-source projects on GitHub where chosen projects are given funding, mentorship, and access to resources to help them grow their project. Since I had already built the prototype in LangChain, it didn’t make much sense to tear it down and rebuild it in a fancy new framework that wasn’t as tried-and-tested. I’d love to try the framework out one day if I build another RAG application after this one.

In the next part of the STICI-note blog series, I will be building an evaluation suite to test and compare different inference models and vector DB configurations, so stay tuned and follow me on LinkedIn to be notified when it comes out!"
3,https://github.com/llmware-ai/llmware,"llmware

Building Enterprise RAG Pipelines with Small, Specialized Models
llmware provides a unified framework for building LLM-based applications (e.g, RAG, Agents), using small, specialized models that can be deployed privately, integrated with enterprise knowledge sources safely and securely, and cost-effectively tuned and adapted for any business process.

llmware has two main components:

RAG Pipeline - integrated components for the full lifecycle of connecting knowledge sources to generative AI models; and

50+ small, specialized models fine-tuned for key tasks in enterprise process automation, including fact-based question-answering, classification, summarization, and extraction.

By bringing together both of these components, along with integrating leading open source models and underlying technologies, llmware offers a comprehensive set of tools to rapidly build knowledge-based enterprise LLM applications.

Most of our examples can be run without a GPU server - get started right away on your laptop.

Join us on Discord | Watch Youtube Tutorials | Explore our Model Families on Huggingface

New to RAG? Check out the Fast Start video series

Multi-Model Agents with SLIM Models - Intro-Video

Intro to SLIM Function Call Models
Can't wait? Get SLIMs right away:

from llmware.models import ModelCatalog

ModelCatalog().get_llm_toolkit()  # get all SLIM models, delivered as small, fast quantized tools
ModelCatalog().tool_test_run(""slim-sentiment-tool"") # see the model in action with test script included
Key features
Writing code withllmware is based on a few main concepts:

Model Catalog: Access all models the same way with easy lookup, regardless of underlying implementation.
Library: ingest, organize and index a collection of knowledge at scale - Parse, Text Chunk and Embed.
Query: query libraries with mix of text, semantic, hybrid, metadata, and custom filters.
Prompt with Sources: the easiest way to combine knowledge retrieval with a LLM inference.
RAG-Optimized Models - 1-7B parameter models designed for RAG workflow integration and running locally.
Simple-to-Scale Database Options - integrated data stores from laptop to parallelized cluster.
Agents with Function Calls and SLIM Models
Start coding - Quick Start for RAG
What's New?
-Best New Small RAG Model - BLING finetune of Phi-3 - ""bling-phi-3-gguf"" - see the video

-Web Services with Agent Calls for Financial Research - end-to-end scenario - video and example

-Voice Transcription with WhisperCPP - getting_started, using_sample_files, and analysis_use_case with great_speeches_video

-Phi-3 GGUF Streaming Local Chatbot with UI - setup your own Phi-3-gguf chatbot on your laptop in minutes - example with video

-Small, specialized, function-calling Extract Model - introducing slim-extract - video and example

-LLM to Answer Yes/No questions - introducing slim-boolean model - video and example

-Natural Language Query to CSV End to End example - using slim-sql model - video and example and now using Custom Tables on Postgres example

-Multi-Model Agents with SLIM models - multi-step Agents with SLIMs on CPU - video - example

-OCR Embedded Document Images Example - systematically extract text from images embedded in documents example

-Enhanced Parser Functions for PDF, Word, Powerpoint and Excel - new text-chunking controls and strategies, extract tables, images, header text - example

-Agent Inference Server - set up multi-model Agents over Inference Server example

-GGUF - Getting Started - check out examples - GGUF (example) and Videos video

-Optimizing Accuracy of RAG Prompts - check out example and videos - part I and part II

Getting Started
Step 1 - Install llmware - pip3 install llmware or pip3 install 'llmware[full]'

note: starting with v0.3.0, we provide options for a core install (minimal set of dependencies) or full install (adds to the core with wider set of related python libraries).
Step 2- Go to Examples - Get Started Fast with 100+ 'Cut-and-Paste' Recipes
Step 3 - Tutorial Videos - check out our Youtube channel for high-impact 5-10 minute tutorials on the latest examples.
✍Working with the llmware Github repository
The llmware repo can be pulled locally to get access to all the examples, or to work directly with the latest version of the llmware code.

git clone git@github.com:llmware-ai/llmware.git
We have provided a welcome_to_llmware automation script in the root of the repository folder. After cloning:

On Windows command line: .\welcome_to_llmware_windows.sh
On Mac / Linux command line: sh ./welcome_to_llmware.sh
Alternatively, if you prefer to complete setup without the welcome automation script, then the next steps include:

install requirements.txt - inside the /llmware path - e.g., pip3 install -r llmware/requirements.txt

install requirements_extras.txt - inside the /llmware path - e.g., pip3 install -r llmware/requirements_extras.txt (Depending upon your use case, you may not need all or any of these installs, but some of these will be used in the examples.)

run examples - copy one or more of the example .py files into the root project path. (We have seen several IDEs that will attempt to run interactively from the nested /example path, and then not have access to the /llmware module - the easy fix is to just copy the example you want to run into the root path).

install vector db - no-install vector db options include milvus lite, chromadb, faiss and lancedb - which do not require a server install, but do require that you install the python sdk library for that vector db, e.g., pip3 install pymilvus, or pip3 install chromadb. If you look in examples/Embedding, you will see examples for getting started with various vector DB, and in the root of the repo, you will see easy-to-get-started docker compose scripts for installing milvus, postgres/pgvector, mongo, qdrant, neo4j, and redis.

Note: we have seen recently issues with Pytorch==2.3 on some platforms - if you run into any issues, we have seen that uninstalling Pytorch and downleveling to Pytorch==2.1 usually solves the problem.

Data Store Options
Fast Start: use SQLite3 and ChromaDB (File-based) out-of-the-box - no install required
Speed + Scale: use MongoDB (text collection) and Milvus (vector db) - install with Docker Compose
Postgres: use Postgres for both text collection and vector DB - install with Docker Compose
Mix-and-Match: LLMWare supports 3 text collection databases (Mongo, Postgres, SQLite) and 10 vector databases (Milvus, PGVector-Postgres, Neo4j, Redis, Mongo-Atlas, Qdrant, Faiss, LanceDB, ChromaDB and Pinecone)
Meet our Models
SLIM model series: small, specialized models fine-tuned for function calling and multi-step, multi-model Agent workflows.
DRAGON model series: Production-grade RAG-optimized 6-7B parameter models - ""Delivering RAG on ..."" the leading foundation base models.
BLING model series: Small CPU-based RAG-optimized, instruct-following 1B-3B parameter models.
Industry BERT models: out-of-the-box custom trained sentence transformer embedding models fine-tuned for the following industries: Insurance, Contracts, Asset Management, SEC.
GGUF Quantization: we provide 'gguf' and 'tool' versions of many SLIM, DRAGON and BLING models, optimized for CPU deployment.
Using LLMs and setting-up API keys & secrets
LLMWare is an open platform and supports a wide range of open source and proprietary models. To use LLMWare, you do not need to use any proprietary LLM - we would encourage you to experiment with SLIM, BLING, DRAGON, Industry-BERT, the GGUF examples, along with bringing in your favorite models from HuggingFace and Sentence Transformers.

If you would like to use a proprietary model, you will need to provide your own API Keys. API keys and secrets for models, aws, and pinecone can be set-up for use in environment variables or passed directly to method calls.

Roadmap - Where are we going ...
Interested in contributing to llmware? Information on ways to participate can be found in our Contributors Guide. As with all aspects of this project, contributing is governed by our Code of Conduct.

Questions and discussions are welcome in our github discussions.

Release notes and Change Log
See also additional deployment/install release notes in wheel_archives

Thursday, June 6 - v0.3.1-WIP

Added module 3 to Fast Start example series examples 7-9 on Agents & Function Calls
Added reranker Jina model for in-memory semantic similarity RAG - see example
Changes merged into main branch - expected next pypi release at end of week
Tuesday, June 4 - v0.3.0

Added support for new Milvus Lite embedded 'no-install' database - see example.
Added two new SLIM models to catalog and agent processes - 'q-gen' and 'qa-gen'
Updated model class instantiation to provide more extensibility to add new classes in different modules
New welcome_to_llmware.sh and welcome_to_llmware_windows.sh fast install scripts
Enhanced Model class base with new configurable post_init and register methods
Created InferenceHistory to track global state of all inferences completed
Multiple improvements and updates to logging at module level
Note: starting with v0.3.0, pip install provides two options - a base minimal install pip3 install llmware which will support most use cases, and a larger install pip3 install 'llmware[full]' with other commonly-used libraries.
Wednesday, May 22 - v0.2.15

Improvements in Model class handling of Pytorch and Transformers dependencies (just-in-time loading, if needed)
Expanding API endpoint options and inference server functionality - see new client access options and server_launch
Saturday, May 18 - v0.2.14

New OCR image parsing methods with example
Adding first part of logging improvements (WIP) in Configs and Models.
New embedding model added to catalog - industry-bert-loans.
Updates to model import methods and configurations.
Sunday, May 12 - v0.2.13

New GGUF streaming method with basic example and phi3 local chatbot
Significant cleanups in ancillary imports and dependencies to reduce install complexity - note: the updated requirements.txt and setup.py files.
Defensive code to provide informative warning of any missing dependencies in specialized parts of the code, e.g., OCR, Web Parser.
Updates of tests, notice and documentation.
OpenAIConfigs created to support Azure OpenAI.
Sunday, May 5 - v0.2.12 Update

Launched ""bling-phi-3"" and ""bling-phi-3-gguf"" in ModelCatalog - newest and most accurate BLING/DRAGON model
New long document summarization method using slim-summary-tool example
New Office (Powerpoint, Word, Excel) sample files example
Added support for Python 3.12
Deprecated faiss and replaced with 'no-install' chromadb in Fast Start examples
Refactored Datasets, Graph and Web Services classes
Updated Voice parsing with WhisperCPP into Library
Monday, April 29 - v0.2.11 Update

Updates to gguf libs for Phi-3 and Llama-3
Added Phi-3 example and Llama-3 example and Quantized Versions to Model Catalog
Integrated WhisperCPP Model class and prebuilt shared libraries - getting-started-example
New voice sample files for testing - example
Improved CUDA detection on Windows and safety checks for older Mac OS versions
Monday, April 22 - v0.2.10 Update

Updates to Agent class to support Natural Language queries of Custom Tables on Postgres example
New Agent API endpoint implemented with LLMWare Inference Server and new Agent capabilities example
Tuesday, April 16 - v0.2.9 Update

New CustomTable class to rapidly create custom DB tables in conjunction with LLM-based workflows.
Enhanced methods for converting CSV and JSON/JSONL files into DB tables.
See new examples Creating Custom Table example
Tuesday, April 9 - v0.2.8 Update

Office Parser (Word Docx, Powerpoint PPTX, and Excel XLSX) - multiple improvements - new libs + Python method.
Includes: several fixes, improved text chunking controls, header text extraction and configuration options.
Generally, new office parser options conform with the new PDF parser options.
Please see Office Parsing Configs example
Wednesday, April 3 - v0.2.7 Update

PDF Parser - multiple improvements - new libs + Python methods.
Includes: UTF-8 encoding for European languages.
Includes: Better text chunking controls, header text extraction and configuration options.
Please see PDF Parsing Configs example for more details.
Note: deprecating support for aarch64-linux (will use 0.2.6 parsers). Full support going forward for Linux Ubuntu20+ on x86_64 + with CUDA.
Friday, March 22 - v0.2.6 Update

New SLIM models: summary, extract, xsum, boolean, tags-3b, and combo sentiment-ner.
New logit and sampling analytics.
New SLIM examples showing how to use the new models.
Thursday, March 14 - v0.2.5 Update

Improved support for GGUF on CUDA (Windows and Linux), with new prebuilt binaries and exception handling.
Enhanced model configuration options (sampling, temperature, top logit capture).
Added full back-level support for Ubuntu 20+ with parsers and GGUF engine.
Support for new Anthropic Claude 3 models.
New retrieval methods: document_lookup and aggregate_text.
New model: bling-stablelm-3b-tool - fast, accurate 3b quantized question-answering model - one of our new favorites.
Wednesday, February 28 - v0.2.4 Update

Major upgrade of GGUF Generative Model class - support for Stable-LM-3B, CUDA build options, and better control over sampling strategies.
Note: new GGUF llama.cpp built libs packaged with build starting in v0.2.4.
Improved GPU support for HF Embedding Models.
Friday, February 16 - v0.2.3 Update

Added 10+ embedding models to ModelCatalog - nomic, jina, bge, gte, ember and uae-large.
Updated OpenAI support >=1.0 and new text-3 embedding models.
SLIM model keys and output_values now accessible in ModelCatalog.
Updating encodings to 'utf-8-sig' to better handle txt/csv files with bom.
Supported Operating Systems: MacOS (Metal and x86), Linux (x86 and aarch64), Windows

note on Linux: we test most extensively on Ubuntu 22 and now Ubuntu 20 and recommend where possible
if you need another Linux version, please raise an issue - we will prioritize testing and ensure support.
Supported Vector Databases: Milvus, Postgres (PGVector), Neo4j, Redis, LanceDB, ChromaDB, Qdrant, FAISS, Pinecone, Mongo Atlas Vector Search

Supported Text Index Databases: MongoDB, Postgres, SQLite

Optional
Docker

To enable the OCR parsing capabilities, install Tesseract v5.3.3 and Poppler v23.10.0 native packages.

Change Log
Latest Updates - 19 Jan 2024 - llmware v0.2.0

Added new database integration options - Postgres and SQlite
Improved status update and parser event logging options for parallelized parsing
Significant enhancements to interactions between Embedding + Text collection databases
Improved error exception handling in loading dynamic modules
Latest Updates - 15 Jan 2024: llmware v0.1.15

Enhancements to dual pass retrieval queries
Expanded configuration objects and options for endpoint resources
Latest Updates - 30 Dec 2023: llmware v0.1.14

Added support for Open Chat inference servers (compatible with OpenAI API)
Improved capabilities for multiple embedding models and vector DB configurations
Added docker-compose install scripts for PGVector and Redis vector databases
Added 'bling-tiny-llama' to model catalog
Latest Updates - 22 Dec 2023: llmware v0.1.13

Added 3 new vector databases - Postgres (PG Vector), Redis, and Qdrant

Improved support for integrating sentence transformers directly in the model catalog

Improvements in the model catalog attributes

Multiple new Examples in Models & Embeddings, including GGUF, Vector database, and model catalog

17 Dec 2023: llmware v0.1.12

dragon-deci-7b added to catalog - RAG-finetuned model on high-performance new 7B model base from Deci
New GGUFGenerativeModel class for easy integration of GGUF Models
Adding prebuilt llama_cpp / ctransformer shared libraries for Mac M1, Mac x86, Linux x86 and Windows
3 DRAGON models packaged as Q4_K_M GGUF models for CPU laptop use (dragon-mistral-7b, dragon-llama-7b, dragon-yi-6b)
4 leading open source chat models added to default catalog with Q4_K_M
8 Dec 2023: llmware v0.1.11

New fast start examples for high volume Document Ingestion and Embeddings with Milvus.
New LLMWare 'Pop up' Inference Server model class and example script.
New Invoice Processing example for RAG.
Improved Windows stack management to support parsing larger documents.
Enhancing debugging log output mode options for PDF and Office parsers.
30 Nov 2023: llmware v0.1.10

Windows added as a supported operating system.
Further enhancements to native code for stack management.
Minor defect fixes.
24 Nov 2023: llmware v0.1.9

Markdown (.md) files are now parsed and treated as text files.
PDF and Office parser stack optimizations which should avoid the need to set ulimit -s.
New llmware_models_fast_start.py example that allows discovery and selection of all llmware HuggingFace models.
Native dependencies (shared libraries and dependencies) now included in repo to faciliate local development.
Updates to the Status class to support PDF and Office document parsing status updates.
Minor defect fixes including image block handling in library exports.
17 Nov 2023: llmware v0.1.8

Enhanced generation performance by allowing each model to specific the trailing space parameter.
Improved handling for eos_token_id for llama2 and mistral.
Improved support for Hugging Face dynamic loading
New examples with the new llmware DRAGON models.
14 Nov 2023: llmware v0.1.7

Moved to Python Wheel package format for PyPi distribution to provide seamless installation of native dependencies on all supported platforms.
ModelCatalog enhancements:
OpenAI update to include newly announced ‘turbo’ 4 and 3.5 models.
Cohere embedding v3 update to include new Cohere embedding models.
BLING models as out-of-the-box registered options in the catalog. They can be instantiated like any other model, even without the “hf=True” flag.
Ability to register new model names, within existing model classes, with the register method in ModelCatalog.
Prompt enhancements:
“evidence_metadata” added to prompt_main output dictionaries allowing prompt_main responses to be plug into the evidence and fact-checking steps without modification.
API key can now be passed directly in a prompt.load_model(model_name, api_key = “[my-api-key]”)
LLMWareInference Server - Initial delivery:
New Class for LLMWareModel which is a wrapper on a custom HF-style API-based model.
LLMWareInferenceServer is a new class that can be instantiated on a remote (GPU) server to create a testing API-server that can be integrated into any Prompt workflow.
03 Nov 2023: llmware v0.1.6

Updated packaging to require mongo-c-driver 1.24.4 to temporarily workaround segmentation fault with mongo-c-driver 1.25.
Updates in python code needed in anticipation of future Windows support.
27 Oct 2023: llmware v0.1.5

Four new example scripts focused on RAG workflows with small, fine-tuned instruct models that run on a laptop (llmware BLING models).
Expanded options for setting temperature inside a prompt class.
Improvement in post processing of Hugging Face model generation.
Streamlined loading of Hugging Face generative models into prompts.
Initial delivery of a central status class: read/write of embedding status with a consistent interface for callers.
Enhanced in-memory dictionary search support for multi-key queries.
Removed trailing space in human-bot wrapping to improve generation quality in some fine-tuned models.
Minor defect fixes, updated test scripts, and version update for Werkzeug to address dependency security alert.
20 Oct 2023: llmware v0.1.4

GPU support for Hugging Face models.
Defect fixes and additional test scripts.
13 Oct 2023: llmware v0.1.3

MongoDB Atlas Vector Search support.
Support for authentication using a MongoDB connection string.
Document summarization methods.
Improvements in capturing the model context window automatically and passing changes in the expected output length.
Dataset card and description with lookup by name.
Processing time added to model inference usage dictionary.
Additional test scripts, examples, and defect fixes.
06 Oct 2023: llmware v0.1.1

Added test scripts to the github repository for regression testing.
Minor defect fixes and version update of Pillow to address dependency security alert.
02 Oct 2023: llmware v0.1.0 Initial release of llmware to open source!!"
4,https://docs.marimo.io/recipes.html,"Recipes
This page includes code snippets or “recipes” for a variety of common tasks. Use them as building blocks or examples when making your own notebooks.

In these recipes, each code block represents a cell.

Control Flow
Show an output conditionally
Use cases. Hide an output until a condition is met (e.g., until algorithm parameters are valid), or show different outputs depending on the value of a UI element or some other Python object

Recipe.

Use an if expression to choose which output to show.

# condition is a boolean, True of False
condition = True
""condition is True"" if condition else None
Run a cell on a timer
Use cases.

Load new data periodically, and show updated plots or other outputs. For example, in a dashboard monitoring a training run, experiment trial, real-time weather data, …

Run a job periodically

Recipe.

Import packages

import marimo as mo
Create a mo.ui.refresh timer that fires once a second:

refresh = mo.ui.refresh(default_interval=""1s"")
# This outputs a timer that fires once a second
refresh
Reference the timer by name to make this cell run once a second

import random

# This cell will run once a second!
refresh

mo.md(""#"" + """" * random.randint(1, 10))
Require form submission before sending UI value
Use cases. UI elements automatically send their values to the Python when they are interacted with, and run all cells referencing the elements. This makes marimo notebooks responsive, but it can be an issue when the downstream cells are expensive, or when the input (such as a text box) needs to be filled out completely before it is considered valid. Forms let you gate submission of UI element values on manual confirmation, via a button press.

Recipe.

Import packages

import marimo as mo
Create a submittable form.

form = mo.ui.text(label=""Your name"").form()
form
Get the value of the form.

form.value
Stop execution of a cell and its descendants
Use cases. For example, don’t run a cell or its descendants if a form is unsubmitted.

Recipe.

Import packages

import marimo as mo
Create a submittable form.

form = mo.ui.text(label=""Your name"").form()
form
Use mo.stop to stop execution when the form is unsubmitted.

mo.stop(form.value is None, mo.md(""Submit the form to continue""))

mo.md(f""Hello, {form.value}!"")
Grouping UI elements together
Create an array of UI elements
Use cases. In order to synchronize UI elements between the frontend and backend (Python), marimo requires you to assign UI elements to global variables. But sometimes you don’t know the number of elements to make until runtime: for example, maybe you want o make a list of sliders, and the number of sliders to make depends on the value of some other UI element.

You might be tempted to create a Python list of UI elements, such as l = [mo.ui.slider(1, 10) for i in range(number.value)]: however, this won’t work, because the sliders are not bound to global variables.

For such cases, marimo provides the “higher-order” UI element mo.ui.array, which lets you make a new UI element out of a list of UI elements: l = mo.ui.array([mo.ui.slider(1, 10) for i in range(number.value)]). The value of an array element is a list of the values of the elements it wraps (in this case, a list of the slider values). Any time you interact with any of the UI elements in the array, all cells referencing the array by name (in this case, “l”) will run automatically.

Recipe.

Import packages.

import marimo as mo
Use mo.ui.array to group together many UI elements into a list.

import random

# instead of random.randint, in your notebook you'd use the value of
# an upstream UI element or other Python object
array = mo.ui.array([mo.ui.text() for i in range(random.randint(1, 10))])
array
Get the value of the UI elements using array.value

array.value
Create a dictionary of UI elements
Use cases. Same as for creating an array of UI elements, but lets you name each of the wrapped elements with a string key.

Recipe.

Import packages.

import marimo as mo
Use mo.ui.dictionary to group together many UI elements into a list.

import random

# instead of random.randint, in your notebook you'd use the value of
# an upstream UI element or other Python object
dictionary = mo.ui.dictionary({str(i): mo.ui.text() for i in range(random.randint(1, 10))})
dictionary
Get the value of the UI elements using dictionary.value

dictionary.value
Embed a dynamic number of UI elements in another output
Use cases. When you want to embed a dynamic number of UI elements in other outputs (like tables or markdown).

Recipe.

Import packages

import marimo as mo
Group the elements with mo.ui.dictionary or mo.ui.array, then retrieve them from the container and display them elsewhere.

import random

n_items = random.randint(2, 5)

# Create a dynamic number of elements using `mo.ui.dictionary` and
# `mo.ui.array`
elements = mo.ui.dictionary(
    {
        ""checkboxes"": mo.ui.array([mo.ui.checkbox() for _ in range(n_items)]),
        ""texts"": mo.ui.array(
            [mo.ui.text(placeholder=""task ..."") for _ in range(n_items)]
        ),
    }
)

mo.md(
    f""""""
    Here's a TODO list of {n_items} items\n\n
    """"""
    + ""\n\n"".join(
        # Iterate over the elements and embed them in markdown
        [
            f""{checkbox} {text}""
            for checkbox, text in zip(
                elements[""checkboxes""], elements[""texts""]
            )
        ]
    )
)
Get the value of the elements

elements.value
Create a hstack (or vstack) of UI elements with on_change handlers
Use cases. Arrange a dynamic number of UI elements in a hstack or vstack, for example some number of buttons, and execute some side-effect when an element is interacted with, e.g. when a button is clicked.

Recipe.

Import packages

import marimo as mo
Create buttons in mo.ui.array and pass them to hstack – a regular Python list won’t work. Make sure to assign the array to a global variable.

import random


# Create a state object that will store the index of the
# clicked button
get_state, set_state = mo.state(None)

# Create an mo.ui.array of buttons - a regular Python list won't work.
buttons = mo.ui.array(
    [
        mo.ui.button(
            label=""button "" + str(i), on_change=lambda v, i=i: set_state(i)
        )
        for i in range(random.randint(2, 5))
    ]
)

mo.hstack(buttons)
Get the state value

get_state()
Create a table column of buttons with on_change handlers
Use cases. Arrange a dynamic number of UI elements in a column of a table, and execute some side-effect when an element is interacted with, e.g. when a button is clicked.

Recipe.

Import packages

import marimo as mo
Create buttons in mo.ui.array and pass them to mo.ui.table. Make sure to assign the table and array to global variables

import random


# Create a state object that will store the index of the
# clicked button
get_state, set_state = mo.state(None)

# Create an mo.ui.array of buttons - a regular Python list won't work.
buttons = mo.ui.array(
    [
        mo.ui.button(
            label=""button "" + str(i), on_change=lambda v, i=i: set_state(i)
        )
        for i in range(random.randint(2, 5))
    ]
)

# Put the buttons array into the table
table = mo.ui.table(
    {
        ""Action"": [""Action Name""] * len(buttons),
        ""Trigger"": list(buttons),
    }
)
table
Get the state value

get_state()
Create a form with multiple UI elements
Use cases. Combine multiple UI elements into a form so that submission of the form sends all its elements to Python.

Recipe.

Import packages.

import marimo as mo
Use mo.ui.form and Html.batch to create a form with multiple elements.

form = mo.md(
   r""""""
   Choose your algorithm parameters:

   - $\epsilon$: {epsilon}
   - $\delta$: {delta}
   """"""
).batch(epsilon=mo.ui.slider(0.1, 1, step=0.1), delta=mo.ui.number(1, 10)).form()
form
Get the submitted form value.

form.value
Working with buttons
Create a button that triggers computation when clicked
Use cases. To trigger a computation on button click and only on button click, use mo.ui.run_button().

Recipe.

Import packages

import marimo as mo
Create a run button

button = mo.ui.run_button()
button
Run something only if the button has been clicked.

mo.stop(not button.value, ""Click 'run' to generate a random number"")

import random
random.randint(0, 1000)
Create a counter button
Use cases. A counter button, i.e. a button that counts the number of times it has been clicked, is a helpful building block for reacting to button clicks (see other recipes in this section).

Recipe.

Import packages

import marimo as mo
Use mo.ui.button and its on_click argument to create a counter button.

# Initialize the button value to 0, increment it on every click
button = mo.ui.button(value=0, on_click=lambda count: count + 1)
button
Get the button value

button.value
Create a toggle button
Use cases. Toggle between two states using a button with a button that toggles between True and False. (Tip: you can also just use mo.ui.switch.)

Recipe.

Import packages

import marimo as mo
Use mo.ui.button and its on_click argument to create a toggle button.

# Initialize the button value to False, flip its value on every click.
button = mo.ui.button(value=False, on_click=lambda value: not value)
button
Toggle between two outputs using the button value.

mo.md(""True!"") if button.value else mo.md(""False!"")
Re-run a cell when a button is pressed
Use cases. For example, you have a cell showing a random sample of data, and you want to resample on button press.

Recipe.

Import packages

import marimo as mo
Create a button without a value, to function as a trigger.

button = mo.ui.button()
button
Reference the button in another cell.

# the button acts as a trigger: every time it is clicked, this cell is run
button

# Replace with your custom lgic
import random
random.randint(0, 100)
Run a cell when a button is pressed, but not before
Use cases. Wait for confirmation before executing downstream cells (similar to a form).

Recipe.

Import packages

import marimo as mo
Create a counter button.

button = mo.ui.button(value=0, on_click=lambda count: count + 1)
button
Only execute when the count is greater than 0.

# Don't run this cell if the button hasn't been clicked, using mo.stop.
# Alternatively, use an if expression.
mo.stop(button.value == 0)

mo.md(f""The button was clicked {button.value} times"")
Reveal an output when a button is pressed
Use cases. Incrementally reveal a user interface.

Recipe.

Import packages

import marimo as mo
Create a counter button.

button = mo.ui.button(value=0, on_click=lambda count: count + 1)
button
Show an output after the button is clicked.

mo.md(""#"" + """" * button.value) if button.value > 0 else None
Caching
Cache expensive computations
Use case. Because marimo runs cells automatically as code and UI elements change, it can be helpful to cache expensive intermediate computations. For example, perhaps your notebook computes t-SNE, UMAP, or PyMDE embeddings, and exposes their parameters as UI elements. Caching the embeddings for different configurations of the elements would greatly speed up your notebook.

Recipe.

Use functools to cache function outputs given inputs.

import functools

@functools.cache
def compute_predictions(problem_parameters):
   # replace with your own function/parameters
   ...
Whenever compute_predictions is called with a value of problem_parameters it has not seen, it will compute the predictions and store them in a cache. The next time it is called with the same parameters, instead of recomputing the predictions, it will return the previously computed value from the cache.

See our best practices guide to learn more."
5,https://towardsdatascience.com/how-to-maximize-your-impact-as-a-data-scientist-3881995a9cb1,"How to Maximize Your Impact as a Data Scientist

One of the hardest pills to swallow as an Individual Contributor (IC) at work is that nobody cares about the hard work you put in. They don’t even care about your output; they care about the impact you drive.

What’s the difference? Your output is the analysis you deliver, or the lines of code you write. Your impact is the decision your analysis helps the CEO make, or the revenue the new product feature is generating.


Image by author
If you want to establish yourself as a high performer and accelerate your career as a Data Scientist, it’s key to focus on impact.

In this post I’ll go over the following:
Why prioritizing impact matters not just for managers, but also ICs
Why focusing on impact is hard
How to maximize your impact
How to overcome common challenges in driving real impact
Let’s dive in.

Get an email whenever Torsten Walbaum publishes.
Get an email whenever Torsten Walbaum publishes. By signing up, you will create a Medium account if you don't already…
medium.com

Why should I focus on impact; isn’t that my manager’s job?
Of course you can leave it to your manager to worry about impact. But stepping up comes with some real benefits for your career:

Reduced frustration & burn-out: Putting a lot of work into a project and then feeling like it didn’t move the needle is one of the most frustrating feelings in any job.
Promotions: Promotions are heavily tied to impact. And if you want to become a manager, you’ll need to show that you understand what drives business outcomes and can allocate resources accordingly.
Internal opportunities: People around you notice if you are having an outsized impact, and you’ll increase your chances of receiving internal offers. My promotion to Director happened because the CMO noticed my work on the BizOps team and asked me to move into the Marketing org to build out a Strategy & Analytics team.
External opportunities: Prospective employers don’t focus on what responsibilities you had, but what your impact was. After all, they are trying to figure out how you can help their business.
Why isn’t everyone doing this?
Because it’s hard.

We are used to thinking about inputs and outputs rather than impact in our daily lives (“I went to the gym” or “I did three loads of laundry”) and we carry that mindset over to our jobs.

More importantly, it gives us a sense of control. It’s fully under your control to work hard on the project, and maybe to create the final deliverable, but you can’t guarantee that it will actually move the business forward.

It can also feel like we’re doing someone else’s job. You built the dashboard; now it’s the other team’s problem how they’re going use it and get value from it. You can definitely take this stance; but don’t you want to see your work move the needle?

Lastly, sometimes it’s unclear what impact even looks like for our role because we feel too disconnected from the business outcomes; I’ll get into this below.

How can I become more impact-focused?
Step 1: Understand what impact looks like for your role and measure your success accordingly
Stop thinking about productivity metrics like “I launched 5 experiments” or “I built this model” and hold yourself accountable to driving impact.

But what does that look like for a Data Scientist? For other roles it’s easy; Account Executives have sales quotas and Growth Marketing Managers have lead generation targets.

But Data Science, at its core, is a function that supports other teams. As a result, there are two levels of impact:


Image by author
Did your work change anything for the better for your business partners? E.g.:

Did your analysis change the roll-out strategy of the new product?
Did your model improve forecast accuracy?
Does your dashboard save the team hours every week that they used to spend on manual data pulls?
Did your work help move the needle on downstream business metrics? E.g.:

You’re a Marketing Data Scientist? Assume you’re on the hook for hitting lead and opportunity targets, and improving Marketing efficiency
You’re doing Analytics for the Customer Support org? Start obsessing about response times and satisfaction scores.
You don’t have to be solely responsible for something in order to take (partial) credit for it. If you provided the analysis that resulted in a pricing change that saved the company millions, then you deserve part of the credit for that impact.

You might not feel the consequences of missing these downstream targets as immediately as your stakeholders, but since your long-term career trajectory is still tied to driving impact, it helps to adopt this outcome-focused mindset.

Once you start doing this, you’ll notice more inefficiencies you can help address, or new opportunities for growth.

Step 2: Ensure your work solves a real business problem
You’ll likely know this situation: Instead of approaching you with a problem, people ask you for a specific deliverable. An analysis, a model, a dashboard.

If you blindly execute what they ask, you might realize too late that it won’t lead to tangible business impact. Maybe the problem they are trying to solve is not that important in the grand scheme of things, or there is a better way to approach it.

So what can you do?
Act like an owner. Understand the actual problem behind the request, and ask yourself what business priority this supports.

If you are early in your career then your manager should ideally help with this. But don’t rely on this: Managers don’t always do a perfect job, and you’ll be the one to feel the consequences of badly scoped work.

This requires you to understand company level priorities and the priorities of other orgs and teams. Take notes during All Hands meetings etc. to understand the big picture, and get your hands on other team’s planning materials to get an idea of what they’re trying to accomplish in the next 1–2 quarters.

Step 3: Ensure there is buy-in for your work
Even if your work directly supports company-level priorities, you’ll be in for a bad time if key stakeholders are not bought in.

You don’t want to be in a situation where you finish the work and then realize that another team is blocking the implementation because they have concerns you didn’t address. To avoid this, you’ll:

Need to understand whose support you need, and
Get them onboard from the get-go
This is a complex topic in itself; I’ll write a separate deep dive on how to drive alignment and get buy-in from other teams in the near future.

Step 4: Focus your time on the highest-impact thing
No matter what role you’re in, you’re likely juggling multiple priorities. To maximize your impact, you need to ensure you spend the majority of your time on the most important thing.

As with many things, this is easier said than done though, so let’s talk about what that looks like concretely.

Ad-hoc requests vs. strategic work
It’s easy to get caught up in the craziness of daily business only to realize you didn’t make any progress on the big, strategic project you actually care about.

This is all too common; none of us get to sit in our ivory tower and chip away at our projects undisturbed. Plus, ad-hoc work is impactful, too; while it’s less exciting than strategic projects, it’s what keeps the business running.

Still, if you find yourself spending the majority of your time fielding these ad-hoc issues, it’s time to talk to your manager. I’m sure your manager would rather help protect your bandwidth than have you 1) miss your deadlines on your key projects and 2) quit eventually from frustration.


Image by author
Don’t cry over spilled milk
Another common challenge comes from the sunk cost fallacy. You invested a lot of time into a project, but it doesn’t seem to be going anywhere. Maybe you realized the premise didn’t make as much sense as you thought, or the priorities of the business have changed since you started the work.

Instead of talking to your manager and stakeholders about changing the scope of the project or abandoning it altogether, you’re doubling down to get it over the finish line. After all, you don’t want all of your effort to go to waste. Sound familiar?

Economists (and Poker players) figured out a long time ago that this is a dangerous trap. When prioritizing your time, ignore how much effort your already put in and focus on where the next hour of work will yield the most impact.

Things to watch out for (“impact killers”)
How do you minimize the odds of wasting time on a project that won’t lead to impact? There are a few warning signs:

“Academic” projects: Any time a project is pitched to you along the lines of “This would be interesting to understand” you should be careful; projects that purely improve the understanding of an issue without tying it back to the business are a waste of time and source of frustration in my experience
Overly ambitious project scope: At Uber, everyone always wanted to understand what the “best” driver incentive type is. Many people worked on this over the years, but it never led anywhere. There was no simple “one-size-fits-all” answer to this question, and the projects that led to actual impact were much more concrete, tactical optimizations
The customer or deliverable are not defined: If it’s not clear who the end user of your work is (are you doing this for your manager, leadership, or another team?), or you’re unsure what exactly you’re supposed to deliver, it should raise a red flag. This is typically a sign that the project needs more scoping work before someone should start running with it
Common Challenges and How to Address Them
We talked about general frameworks to maximize impact. But how do you make actual, specific projects more impactful?

Many times, projects fail close to the finish line. Impact doesn’t materialize automatically, so you need to put in the final bit of work to ensure your work gets adopted. Doing this has an extremely high return on the time you invest since you already did the hard work to produce the deliverable and “only” need to close the loop with stakeholders.


Image by author
To make things more tangible, I am going to go through a few types of common deliverables, touch on where they typically fail to create impact and propose what you can do about it:

1. You create a comprehensive analysis but nobody is acting on it
Problem: This is common with analyses that don’t have a clear recommendation. If you simply outline the data and potential paths forward, you are expecting your audience to do all of the heavy lifting.

Solution: Your work starts adding real value for them once you take that work off their plate. Always give a clear recommendation; you can caveat it and show alternatives in the appendix, but you need to take a stance.

2. You ran an experiment but nobody is using the results
Problem: Many experiments conclude with a metrics read-out by Data Science. More often than not, this is a “metrics dump” with a lot of information, but little interpretation or context.

Solution: Help your business partners interpret the results, and tell them how it affects what they care about.

How should they think about the statistical significance or lack thereof?
Is the observed lift good compared to other changes you tested and shipped?
What is your recommendation for next steps? What does the experiment result mean for this person or team specifically?
Remember, you are the subject matter expert and shouldn’t expect non-analytical audiences to interpret raw experiment data. Telling your stakeholders what the result means for them will increase chances they will act on it.

3. You built a predictive model, but the team you built it for is not using it
Problem: When predictive models don’t get used, it’s often because of a lack of trust in the model output.

ML models themselves tend to be black boxes, and if teams don’t understand how the outputs were generated and whether they are reliable, they are hesitant to rely on them. Even if your model is not using ML and lives in a spreadsheet: If people don’t know how it works, they’ll be suspicious.

Solution: It’s all about involving stakeholders in the process and building trust.

Involve stakeholders in the model development from the get-go to get them comfortable and address any concerns early on
Demystify the output; for example, you can extract the top model features and explain them
Sanity-check predictions and compare them to intuition. For example, if you forecast sales but your model predicts a different seasonality pattern from previous years, you’ll need to be able to explain why, or you’ll lose trust. In my experience, this is more impactful than just sharing performance metrics like the accuracy of the model
Having a structured playbook for how to do this will make your life easier, so I’ll cover this in a separate post in the near future.

4. You created a dashboard but nobody is looking at it
Problem: If a dashboard doesn’t get used, it’s likely one of these things is true:

The dashboard doesn’t directly address an urgent business use case
You didn’t involve your stakeholders along the way (e.g. by sharing mock-ups and drafts for feedback) and the final product is not what they were hoping for
The dashboard is complex and your users don’t understand how to get what they need
Solution: To address #1 and #2, start with user research to understand pain points and potential use cases of the dashboard, and involve your stakeholders during development.

With regards to #3, a simpler dashboard that users are comfortable with beats a more advanced one that doesn’t get used. If you cannot (or don’t want to) simplify the dash further, you’ll need to train your users on the functionality and shadow them to understand any points of friction.

A dashboard is not done when you ship it for the first time, but needs to be improved over time based on users’ needs and feedback.

Closing Thoughts
Focusing on impact is scary since we leave the world of controllable inputs behind, but it’s what ultimately gets you promotions and new job opportunities.

And isn’t it nice when your work actually feels like it moves the needle?

For more hands-on analytics advice, consider following me here on Medium, on LinkedIn or on Substack."
6,https://ec.europa.eu/commission/presscorner/detail/en/QANDA_21_1683,"Why do we need to regulate the use of Artificial Intelligence?
The potential benefits of Artificial Intelligence (AI) for our societies are manifold from improved medical care to better education. Faced with the rapid technological development of AI, the EU decided to act as one to harness these opportunities.

The EU AI Act is the world's first comprehensive AI law. It aims to address risks to health, safety and fundamental rights. The regulation also protects democracy, rule of law and the environment.

While most AI systems will pose low to no risk, certain AI systems create risks that need to be addressed to avoid undesirable outcomes.

For example, the opacity of many algorithms may create uncertainty and hamper the effective enforcement of the existing legislation on safety and fundamental rights. Responding to these challenges, legislative action was needed to ensure a well-functioning internal market for AI systems where both benefits and risks are adequately addressed.

This includes applications such as biometric identification systems or AI decisions touching on important personal interests, such as in the areas of recruitment, education, healthcare, or law enforcement.

Recent advancements in AI gave rise to ever more powerful Generative AI. So-called “general-purpose AI models” that are being integrated in numerous AI systems are becoming too important for the economy and society not to be regulated. In light of potential systemic risks, the EU puts in place effective rules and oversight.

Which risks will the new AI rules address?
The uptake of AI systems has a strong potential to bring societal benefits, economic growth and enhance EU innovation and global competitiveness. However, in certain cases, the specific characteristics of certain AI systems may create new risks related to user safety and fundamental rights. Some powerful AI models that are being widely used could even pose systemic risks.

This leads to legal uncertainty for companies and potentially slower uptake of AI technologies by businesses and citizens, due to the lack of trust. Disparate regulatory responses by national authorities would risk fragmenting the internal market.

To whom does the AI Act apply?
The legal framework will apply to both public and private actors inside and outside the EU as long as the AI system is placed on the Union market or its use affects people located in the EU.

It can concern both providers (e.g. a developer of a CV-screening tool) and deployers of high-risk AI systems (e.g. a bank buying this screening toolImporters of AI systems will also have to ensure that the foreign provider has already carried out the appropriate conformity assessment procedure, bears a European Conformity (CE) marking and is accompanied by the required documentation and instructions of use.

In addition, certain obligations are foreseen for providers of general-purpose AI models, including large generative AI models.

Providers of free and open-source models are exempted from most of these obligations. This exemption does not cover obligations for providers of general purpose AI models with systemic risks.

Obligations also do not apply to research, development and prototyping activities preceding the release on the market, and the regulation furthermore does not apply to AI systems that are exclusively for military, defence or national security purposes, regardless of the type of entity carrying out those activities.

What are the risk categories?
The Commission proposes a risk–based approach, with four levels of risk for AI systems, as well as an identification of risks specific to general purpose models:

Minimal risk: All other AI systems can be developed and used subject to the existing legislation without additional legal obligations. The vast majority of AI systems currently used or likely to be used in the EU fall into this category. Voluntarily, providers of those systems may choose to apply the requirements for trustworthy AI and adhere to voluntary codes of conduct.
High-risk: A limited number of AI systems defined in the proposal, potentially creating an adverse impact on people's safety or their fundamental rights (as protected by the EU Charter of Fundamental Rights), are considered to be high-risk. Annexed to the Act is the list of high-risk AI systems, which can be reviewed to align with the evolution of AI use cases.
These also include safety components of products covered by sectorial Union legislation. They will always be considered high-risk when subject to third-party conformity assessment under that sectorial legislation.
Unacceptable risk: A very limited set of particularly harmful uses of AI that contravene EU values because they violate fundamental rights and will therefore be banned:
Social scoring for public and private purposes;
Exploitation of vulnerabilities of persons, use of subliminal techniques;
Real-time remote biometric identification in publicly accessible spaces by law enforcement, subject to narrow exceptions (see below);
Biometric categorisation of natural persons based on biometric data to deduce or infer their race, political opinions, trade union membership, religious or philosophical beliefs or sexual orientation. Filtering of datasets based on biometric data in the area of law enforcement will still be possible;
Individual predictive policing;
Emotion recognition in the workplace and education institutions, unless for medical or safety reasons (i.e. monitoring the tiredness levels of a pilot);
Untargeted scraping of internet or CCTV for facial images to build-up or expand databases.
Specific Transparency risk: For certain AI systems specific transparency requirements are imposed, for example where there is a clear risk of manipulation (e.g. via the use of chatbots). Users should be aware that they are interacting with a machine.
In addition, the AI Act considers systemic risks which could arise from general-purpose AI models, including large generative AI models. These can be used for a variety of tasks and are becoming the basis for many AI systems in the EU. Some of these models could carry systemic risks if they are very capable or widely used. For example, powerful models could cause serious accidents or be misused for far-reaching cyberattacks. Many individuals could be affected if a model propagates harmful biases across many applications.

How do I know whether an AI system is high-risk?
Together with a clear definition of ‘high-risk', the Act sets out a solid methodology that helps identifying high-risk AI systems within the legal framework. This aims to provide legal certainty for businesses and other operators.

The risk classification is based on the intended purpose of the AI system, in line with the existing EU product safety legislation. It means that the classification of the risk depends on the function performed by the AI system and on the specific purpose and modalities for which the system is used.

Annexed to the Act is a list of use cases which are considered to be high-risk. The Commission will ensure that this list is kept up to date and relevant. Systems on the high-risk list, that perform narrow procedural tasks, improve the result of previous human activities, do not influence human decisions or do purely preparatory tasks are not considered high-risk. However, an AI system shall always be considered high-risk if it performs profiling of natural persons.

What are the obligations for providers of high-risk AI systems?
Before placing a high-risk AI system on the EU market or otherwise putting it into service, providers must subject it to a conformity assessment. This will allow them to demonstrate that their system complies with the mandatory requirements for trustworthy AI (e.g. data quality, documentation and traceability, transparency, human oversight, accuracy, cybersecurity and robustness). This assessment has to be repeated if the system or its purpose are substantially modified.

AI systems being safety components of products covered by sectorial Union legislation will always be deemed high-risk when subject to third-party conformity assessment under that sectorial legislation. Also, for biometric systems a third-party conformity assessment is always required.

Providers of high-risk AI systems will also have to implement quality and risk management systems to ensure their compliance with the new requirements and minimise risks for users and affected persons, even after a product is placed on the market.

High-risk AI systems that are deployed by public authorities or entities acting on their behalf will have to be registered in a public EU database, unless those systems are used for law enforcement and migration. The latter will have to be registered in a non-public part of the database that will be only accessible to relevant supervisory authorities.

Market surveillance authorities will support post-market monitoring through audits and by offering providers the possibility to report on serious incidents or breaches of fundamental rights obligations of which they have become aware. Any market surveillance authority may authorise placing on the market of specific high-risk AI for exceptional reasons.

In case of a breach, the requirements will allow national authorities to have access to the information needed to investigate whether the use of the AI system complied with the law.

What are examples for high-risk use cases as defined in Annex III?
Certain critical infrastructures for instance in the fields of road traffic and the supply of water, gas, heating and electricity;
Education and vocational training, e.g. to evaluate learning outcomes and steer the learning process and monitoring of cheating;
Employment, workers management and access to self-employment, e.g. to place targeted job advertisements, to analyse and filter job applications, and to evaluate candidates;
Access to essential private and public services and benefits (e.g. healthcare), creditworthiness evaluation of natural persons, and risk assessment and pricing in relation to life and health insurance;
Certain systems used in the fields of law enforcement, border control, administration of justice and democratic processes;
Evaluation and classification of emergency calls;
Biometric identification, categorisation and emotion recognition systems (outside the prohibited categories);
Recommender systems of very large online platforms are not included, as they are already covered in other legislation (DMA/DSA).
How are general-purpose AI models being regulated?
General-purpose AI models, including large generative AI models, can be used for a variety of tasks. Individual models may be integrated into a large number of AI systems.

It is important that a provider wishing to build upon a general-purpose AI model has all the necessary information to make sure its system is safe and compliant with the AI Act.

Therefore, the AI Act obliges providers of such models to disclose certain information to downstream system providers. Such transparency enables a better understanding of these models.

Model providers additionally need to have policies in place to ensure that that they respect copyright law when training their models.

In addition, some of these models could pose systemic risks, because they are very capable or widely used.

For now, general purpose AI models that were trained using a total computing power of more than 10^25 FLOPs are considered to carry systemic risks, given that models trained with larger compute tend to be more powerful. The AI Office (established within the Commission) may update this threshold in light of technological advances, and may furthermore in specific cases designate other models as such based on further criteria (e.g. number of users, or the degree of autonomy of the model).

Providers of models with systemic risks are therefore mandated to assess and mitigate risks, report serious incidents, conduct state-of-the-art tests and model evaluations, ensure cybersecurity and provide information on the energy consumption of their models.

For this, they are asked to engage with the European AI Office to draw up Codes of Conduct as the central tool to detail out the rules in cooperation with other experts. A scientific panel will play a central role in overseeing general-purpose AI models.

Why is 10^25 FLOPs an appropriate threshold for GPAI with systemic risks?
This threshold captures the currently most advanced GPAI models, namely OpenAI's GPT-4 and likely Google DeepMind's Gemini.

The capabilities of the models above this threshold are not yet well enough understood. They could pose systemic risks, and therefore it is reasonable to subject their providers to the additional set of obligations.

FLOP is a first proxy for model capabilities, and the exact FLOP threshold can be updated upwards or downwards by the European AI Office, e.g. in the light of progress in objectively measuring model capabilities and of developments in the computing power needed for a given performance level.

The AI Act can be amended to update the FLOP threshold (by means of a delegated act).

Is the AI Act future-proof?
The Regulation introduces different level of risks and provides clear definitions, including for GPAI.

The legislation sets result-oriented requirements for high-risk AI systems but leaves the concrete technical solutions and operationalisation primarily to industry-driven standards that will ensure that the legal framework is flexible to be adapted to different use cases and to enable new technological solutions.

In addition, the AI Act can be amended by delegated and implementing acts, including to update the FLOP threshold (delegated act), to add criteria for classifying the GPAI models as presenting systemic risks (delegated act), to amend modalities to establish regulatory sandboxes and elements of the real-world testing plan (implementing acts).

How does the AI Act regulate biometric identification?
The use of real-time remote biometric identification in publicly accessible spaces (i.e. facial recognition using CCTV) for law enforcement purposes is prohibited, unless used in one of the following cases:

Law enforcement activities related to 16 specified crimes;
Targeted search for specific victims, abduction, trafficking and sexual exploitation of human beings, and missing persons; or
The prevention of threat to the life or physical safety of persons or response to the present or foreseeable threat of a terror attack.
The list of the 16 crimes contains:

Terrorism;
Trafficking in human beings;
Sexual exploitation of children and child sexual abuse material;
Illicit trafficking in narcotic drugs and psychotropic substances;
Illicit trafficking in weapons, munitions and explosives;
Murder;
Grievous bodily injury;
Illicit trade in human organs and tissue;
Illicit trafficking in nuclear or radioactive materials;
Kidnapping, illegal restraint and hostage-taking;
Crimes within the jurisdiction of the International Criminal Court;
Unlawful seizure of aircraft/ships;
Rape;
Environmental crime;
Organised or armed robbery;
Sabotage, participation in a criminal organisation involved in one or more crimes listed above.
Real-time remote biometric identification by law enforcement authorities would be subject to prior authorisation by a judicial or independent administrative authority whose decision is binding. In case of urgency, authorisation can be done within 24 hours; if the authorisation is rejected all data and output needs to be deleted.

It would need to be preceded by prior fundamental rights impact assessment and should be notified to the relevant market surveillance authority and the data protection authority. In case of urgency, the use of the system may be commenced without the registration.

Usage of AI systems for post remote biometric identification (identification of persons in previously collected video material) of persons under investigation requires prior authorisation by a judicial authority or an independent administrative authority, and notification of the data protection and market surveillance authority.

Why are particular rules needed for remote biometric identification?
Biometric identification can take different forms. It can be used for user authentication i.e. to unlock a smartphone or for verification/authentication at border crossings to check a person's identity against his/her travel documents (one-to-one matching).

Biometric identification could also be used remotely, for identifying people in a crowd, where for example an image of a person is checked against a database (one-to-many matching).

Accuracy of systems for facial recognition can vary significantly based on a wide range of factors, such as camera quality, light, distance, database, algorithm, and the subject's ethnicity, age or gender. The same applies for gait and voice recognition and other biometric systems. Highly advanced systems are continuously reducing their false acceptance rates.

While a 99% accuracy rate may sound good in general, it is considerably risky when the result leads to the suspicion of an innocent person. Even a 0.1% error rate is a lot if it concerns tens of thousands of people.

How do the rules protect fundamental rights?
There is already a strong protection for fundamental rights and for non-discrimination in place at EU and Member State level, but complexity and opacity of certain AI applications (‘black boxes') pose a problem.

A human-centric approach to AI means to ensure AI applications comply with fundamental rights legislation. Accountability and transparency requirements for the use of high-risk AI systems, combined with improved enforcement capacities, will ensure that legal compliance is factored in at the development stage.

Where breaches occur, such requirements will allow national authorities to have access to the information needed to investigate whether the use of AI complied with EU law.

Moreover, the AI Act requires that deployers that are bodies governed by public law or private operators providing public services and operators providing high-risk systems to conduct a fundamental rights impact assessment.

What is a fundamental rights impact assessment? Who has to conduct such an assessment, and when?
The use of a high-risk AI system may produce an impact on fundamental rights. Therefore, deployers that are bodies governed by public law or private operators providing public services, and operators providing high-risk systems shall perform an assessment of the impact on fundamental rights and notify the national authority of the results.

The assessment shall consist of a description of the deployer's processes in which the high-risk AI system will be used, of the period of time and frequency in which the high-risk AI system is intended to be used, of the categories of natural persons and groups likely to be affected by its use in the specific context, of the specific risks of harm likely to impact the affected categories of persons or group of persons, a description of the implementation of human oversight measures and of measures to be taken in case of the materialization of the risks.

If the provider already met this obligation through the data protection impact assessment, the fundamental rights impact assessment shall be conducted in conjunction with that data protection impact assessment.

How does this regulation address racial and gender bias in AI?
It is very important that AI systems do not create or reproduce bias. Rather, when properly designed and used, AI systems can contribute to reduce bias and existing structural discrimination, and thus lead to more equitable and non-discriminatory decisions (e.g. in recruitment).

The new mandatory requirements for all high-risk AI systems will serve this purpose. AI systems must be technically robust to guarantee that the technology is fit for purpose and false positive/negative results are not disproportionately affecting protected groups (e.g. racial or ethnic origin, sex, age etc.).

High-risk systems will also need to be trained and tested with sufficiently representative datasets to minimise the risk of unfair biases embedded in the model and ensure that these can be addressed through appropriate bias detection, correction and other mitigating measures.

They must also be traceable and auditable, ensuring that appropriate documentation is kept, including of the data used to train the algorithm that would be key in ex post investigations.

Compliance system before and after they are placed on the market will have to ensure these systems are regularly monitored and potential risks are promptly addressed.

When will the AI Act be fully applicable?
Following its adoption by the European Parliament and the Council, the AI Act shall enter into force on the twentieth day following that of its publication in the official Journal. It will be fully applicable 24 months after entry into force, with a graduated approach as follows:

6 months after entry into force, Member States shall phase out prohibited systems;
12 months: obligations for general purpose AI governance become applicable;
24 months: all rules of the AI Act become applicable including obligations for high-risk systems defined in Annex III (list of high-risk use cases);
36 months: obligations for high-risk systems defined in Annex II (list of Union harmonisation legislation) apply.
How will the AI Act be enforced?
Member States hold a key role in the application and enforcement of this Regulation. In this respect, each Member State should designate one or more national competent authorities to supervise the application and implementation, as well as carry out market surveillance activities.

To increase efficiency and to set an official point of contact with the public and other counterparts, each Member State should designate one national supervisory authority, which will also represent the country in the European Artificial Intelligence Board.

Additional technical expertise will be provided by an advisory forum, representing a balanced selection of stakeholders, including industry, start-ups, SMEs, civil society and academia.

In addition, the Commission will establish a new European AI Office, within the Commission, which will supervise general-purpose AI models, cooperate with the European Artificial Intelligence Board and be supported by a scientific panel of independent experts.

Why is a European Artificial Intelligence Board needed and what will it do?
The European Artificial Intelligence Board comprises high-level representatives of competent national supervisory authorities, the European Data Protection Supervisor, and the Commission. Its role is to facilitate a smooth, effective and harmonised implementation of the new AI Regulation.

The Board will issue recommendations and opinions to the Commission regarding high-risk AI systems and on other aspects relevant for the effective and uniform implementation of the new rules. Finally, it will also support standardisation activities in the area.

What are the tasks of the European AI Office?
The AI Office has as its mission to develop Union expertise and capabilities in the field of artificial intelligence and to contribute to the implementation of Union legislation of artificial intelligence in a centralised structure.

In particular, the AI Office shall enforce and supervise the new rules for general purpose AI models. This includes drawing up codes of practice to detail out rules, its role in classifying models with systemic risks and monitoring the effective implementation and compliance with the Regulation. The latter is facilitated by the powers to request documentation, conduct model evaluations, investigate upon alerts and request providers to take corrective action.

The AI Office shall ensure coordination regarding artificial intelligence policy and collaboration between involved Union institutions, bodies and agencies as well as with experts and stakeholders. In particular, it will provide a strong link with the scientific community to support the enforcement, serve as international reference point for independent experts and expert organisations and facilitate exchange and collaboration with similar institutions across the globe.

What is the difference between the AI Board, AI Office, Advisory Forum and Scientific Panel of independent experts?
The AI Board has extended tasks in advising and assisting the Commission and the Member States.

The AI Office is to be established within the Commission and shall work to develop Union expertise and capabilities in the field of artificial intelligence and to contribute to the implementation of Union legislation of artificial intelligence. Particularly, the AI Office shall enforce and supervise the new rules for general purpose AI models.

The Advisory Forum will consist of a balanced selection of stakeholders, including industry, start-ups, SMEs, civil society and academia. It shall be established to advise and provide technical expertise to the Board and the Commission, with members appointed by the Board among stakeholders.

The Scientific Panel of independent experts supports the implementation and enforcement of the Regulation as regards GPAI models and systems, and the Member States would have access to the pool of experts.

What are the penalties for infringement?
When AI systems are put on the market or in use that do not respect the requirements of the Regulation, Member States will have to lay down effective, proportionate and dissuasive penalties, including administrative fines, in relation to infringements and communicate them to the Commission.

The Regulation sets out thresholds that need to be taken into account:

Up to €35m or 7% of the total worldwide annual turnover of the preceding financial year (whichever is higher) for infringements on prohibited practices or non-compliance related to requirements on data;
Up to €15m or 3% of the total worldwide annual turnover of the preceding financial year for non-compliance with any of the other requirements or obligations of the Regulation, including infringement of the rules on general-purpose AI models;
Up to €7.5m or 1.5% of the total worldwide annual turnover of the preceding financial year for the supply of incorrect, incomplete or misleading information to notified bodies and national competent authorities in reply to a request;
For each category of infringement, the threshold would be the lower of the two amounts for SMEs and the higher for other companies.
In order to harmonise national rules and practices in setting administrative fines, the Commission, counting on the advice of the Board, will draw up guidelines.

As EU Institutions, agencies or bodies should lead by example, they will also be subject to the rules and to possible penalties; the European Data Protection Supervisor will have the power to impose fines to them.

What can individuals do that are affected by a rule violation?
The AI Act foresees a right to lodge a complaint with a national authority. On this basis national authorities can launch market surveillance activities, following the procedures of the market surveillance regulations.

Additionally, the proposed AI Liability Directive aims to provide persons seeking compensation for damage caused by high-risk AI systems with effective means to identify potentially liable persons and obtain relevant evidence for a damage claim. For this purpose, the proposed Directive provides for the disclosure of evidence about specific high-risk AI systems that are suspected of having caused damage.

Moreover, the revised Product Liability Directive will ensure that compensation is available to individuals who suffer death, personal injury or property damage that is caused by a defective product in the Union and clarify that AI systems and products that integrate AI systems are also covered by existing rules.

How do the voluntary codes of conduct for high-risk AI systems work?
Providers of non-high-risk applications can ensure that their AI system is trustworthy by developing their own voluntary codes of conduct or adhering to codes of conduct adopted by other representative associations.

These will apply simultaneously with the transparency obligations for certain AI systems.

The Commission will encourage industry associations and other representative organisations to adopt voluntary codes of conduct.

How do the codes of practice for general purpose AI models work?
The Commission invites providers of general-purpose AI models and other experts to jointly work on a code of practice.

Once developed and approved for this purpose, these codes can be used by the providers of general-purpose AI models to demonstrate compliance with the relevant obligations from the AI Act, following the example of the GDPR.

This is especially relevant to detail out the rules for providers of general-purpose AI model with systemic risks, to ensure future-proof and effective rules for risk assessment and mitigation as well as other obligations.

Does the AI Act contain provisions regarding environmental protection and sustainability?
The objective of the AI proposal is to address risks to safety and fundamental rights, including the fundamental right to a high-level environmental protection. Environment is also one of the explicitly mentioned and protected legal interests.

The Commission is asked to request European standardisation organisations a standardisation deliverable on reporting and documentation processes to improve AI systems resource performance, such as reduction of energy and other resources consumption of the high-risk AI system during its lifecycle, and on energy efficient development of general-purpose AI models.

Furthermore, the Commission by two years after the date of application of the Regulation and every four years thereafter, is asked to submit a report on the review of the progress on the development of standardisation deliverables on energy efficient development of general-purpose models and asses the need for further measures or actions, including binding measures or actions.

In addition, providers of general purpose AI models, which are trained on large data amounts and therefore prone to high energy consumption, are required to disclose energy consumption.

The Commission is asked to develop an appropriate methodology for this assessment.

In case of general purpose AI models with systemic risks, energy efficiency furthermore needs to be assessed.

How can the new rules support innovation?
The regulatory framework can enhance the uptake of AI in two ways. On the one hand, increasing users' trust will increase the demand for AI used by companies and public authorities. On the other hand, by increasing legal certainty and harmonising rules, AI providers will access bigger markets, with products that users and consumers appreciate and purchase. Rules will apply only where strictly needed and in a way that minimises the burden for economic operators, with a light governance structure.

The AI Act further enables the creation of regulatory sandboxes and real world testing, which provide a controlled environment to test innovative technologies for a limited time, thereby fostering innovation by companies, SMEs and start-ups in compliance with the AI Act. These, together with other measures such as the additional Networks of AI Excellence Centres and the Public-Private Partnership on Artificial Intelligence, Data and Robotics, and access to Digital Innovation Hubs and Testing and Experimentation Facilities will help build the right framework conditions for companies to develop and deploy AI.

Real world testing of High-Risk AI systems can be conducted for a maximum of 6 months (which can be prolonged by another 6 months). Prior to testing, a plan needs to be drawn up and submitted it to the market surveillance authority, which has to approve of the plan and specific testing conditions, with default tacit approval if no answer has been given within 30 days. Testing may be subject to unannounced inspections by the authority.

Real world testing can only be conducted given specific safeguards, e.g. users of the systems under real world testing have to provide informed consent, the testing must not have any negative effect on them, outcomes need to be reversible or disregardable, and their data needs to be deleted after conclusion of the testing. Special protection is to be granted to vulnerable groups, i.e. due to their age, physical or mental disability.

Besides the AI Act, how will the EU facilitate and support innovation in AI?
The EU's approach to Artificial Intelligence is based on excellence and trust, aiming to boost research and industrial capacity while ensuring safety and the protection of fundamental rights. People and businesses should be able to enjoy the benefits of AI while feeling safe and protected. The European AI Strategy aims at making the EU a world-class hub for AI and ensuring that AI is human-centric and trustworthy. In April 2021, the Commission presented its AI package, including: (1) a review of the Coordinated Plan on Artificial Intelligence and (2) its proposal for a regulation laying down harmonised rules on AI.

With the Coordinated Plan on AI the European Commission has adopted a comprehensive strategy to promote the development and adoption of AI in Europe. It focuses on creating enabling conditions for AI development and uptake, ensuring excellence thrives from the lab to the market, increasing the trustworthiness of AI, and building strategic leadership in high-impact sectors.

The Commission aims to leverage the activities of Member States by coordinating and harmonizing their efforts, to foster a cohesive and synergistic approach towards AI development and adoption. The Commission also put in place the European AI Alliance platform, which brings together stakeholders from academia, industry, and civil society to exchange knowledge and insights on AI policies.

Moreover, the Coordinated plans foresees several measures that aim to unlock data resources, foster critical computing capacity, increase research capacities, support a European network of Testing and Experimentation Facilities (TEFS) and support SMEs through European Digital Innovation Hubs (EDIHs).

What is the international dimension of the EU's approach?
The AI Act and the Coordinated Plan on AI are part of the efforts of the European Union to be a global leader in the promotion of trustworthy AI at international level. AI has become an area of strategic importance at the crossroads of geopolitics, commercial stakes and security concerns.

Countries around the world are choosing to use AI as a way to signal their desires for technical advancement due to its utility and potential. AI regulation is only emerging and the EU will take actions to foster the setting of global AI standards in close collaboration with international partners in line with the rules-based multilateral system and the values it upholds. The EU intends to deepen partnerships, coalitions and alliances with EU partners (e.g. Japan, the US, India, Canada, South Korea, Singapore, or the Latin American and Caribbean region) as well as multilateral (e.g. OECD, G7 and G20) and regional organisations (e.g. Council of Europe).

*Updated on 14/12/2023"
7,https://bg3.wiki/wiki/The_Emperor,"The Emperor is a mind flayer who appears in Baldur's Gate 3. It[note 1] plays a key role in the main story, but its identity is intentionally obscured until later parts of the game, allowing the player to ultimately decide for themselves if they want to know more about it, and whether or not it is trustworthy.


Contents
Overview
Identity
Personal quest
Recruitment
Romance
History
Events of Baldur's Gate 3
Act Two finale
Act Three
Elfsong Tavern
The Wyrmway
Endings
List of interactions
Conversation scenes
Identity revealed
Regarding Duke Stelmane
On conclusion of Visit the Emperor's Old Hideout
Romance
Achievements
Gallery
Notes
Footnotes
References
Overview
Identity
The Emperor plays a key role in the main story of Baldur's Gate 3, and as part of this role its identity and personal background are kept obfuscated for much of the game. It very carefully divulges information that it deems necessary, sometimes arguing that the player is not ready for the answer yet, or that it will reveal specific information in the future.

During Acts One and Two, the Emperor only ""meets"" with the player as the Dream Guardian. At the beginning of Act Three, the player finally meets the Emperor face to face, an event which reveals that it is a mind flayer.

Through all three Acts, the Emperor generally serves as a guide, and unlikely ally to the party, having the means to protect their minds from the influence of the Absolute, through the use of the prisoner within the Astral Prism.

""Don't let my form deceive you. I am the one that's been protecting you. I am the one that came to you in your dreams. Help me.
— The Emperor, during Act 3
Personal quest
After reaching the Elfsong Tavern in Act Three, the Emperor will initiate the quest Visit the Emperor's Old Hideout, in which the player can better get to know the Emperor. It discloses some of its past, during its time in the city and from before it became illithid.

Recruitment
The Emperor can appear in multiple combat encounters as a controllable ally, a neutral ally, or an enemy. It cannot, however, become a full member of the player's party or camp.

Romance
The Emperor can have a romance with the player during Act Three. See Romance.

History
Details about the Emperor's personal history are intentionally obfuscated during most of the game, but the player has the opportunity to learn more about it through conversations, interactions with other characters, reading books, and completing specific side quests.

Ico knownSpells lvl 03.png Act 3 Spoilers! This section reveals details about the story of Baldur's Gate 3.


An Adventurer, I came from Baldur's Gate, though I was never one to be constrained by circumstance. I longed for more.

That longing brought me to Moonrise Towers on a search for treasure. To a colony of mind flayers who caught me and changed me.
The Emperor was once Balduran, an adventurer who founded a coastal village called Grey Harbour. After securing enough money to fund the building of the Wall that led to Baldur's Gate being founded, he felt the call of the sea once more. On the voyage, and following a shipwreck, Balduran made his way to Moonrise Towers in search of fortune. There, he found a coven of mind flayers who infected him with an illithid tadpole. As a record of his interrogation by Enver Gortash during the planning phases of the Absolute Hoax states, he spent ten years under the thrall of the Moonrise Elder Brain.

After Balduran was reborn as an illithid and broke free from the Elder Brain the Absolute, it returned to Baldur's Gate, living in the shadows and feeding on the brains of criminals. Initially struggling with its identity as a mind flayer, Balduran eventually embraced its new form.

Balduran's new acceptance of its illithid form caused a wedge to form between it and its close companion, the dragon Ansur. Ansur attempted to kill Balduran as it slept, believing this would be a merciful death. The Emperor sensed the attempt, and in its struggle to protect itself from being murdered, it killed Ansur in self-defence. [1]

After Ansur's death, Balduran came to be called the Emperor as it used its newfound psychic influence to rule Baldur's Gate from the shadows. For the next four centuries, it made its haven under the Elfsong tavern, keeping various sentimental knick knacks from its time as Balduran.


I had the fortune of meeting Duke Stelmane. We formed a partnership
During those four centuries, it also came to be associated with the Knights of the Shield, a lawful and neutral evil conglomerate of politicians and merchants manipulating events behind the scenes. Duke Stelmane was a major figure of this secret society, acting as the Emperor's envoy while it secretly kept her enthralled. [note 2]

Sometime before the events of the game, Enver Gortash and the Dark Urge captured the Emperor, and brought it back under the thrall of the Moonrise Elder Brain, who was now wearing the Crown of Karsus and had become the Netherbrain masquerading as the Absolute. The Netherbrain, sought to have all three Chosen of the Dead Three killed, and specifically picked the Emperor, unbeknown to it, to lead a team of illithids on a nautiloid to search for and steal from the Githyanki the Astral Prism containing their prince, Orpheus.[2]
Events of Baldur's Gate 3
Act Two finale
Main article: Help Your Protector
On the way to Baldur's Gate, the party will be ambushed by a group of Gish'ra warriors while resting at Wyrm's Lookout. Entering the portal to the Astral Prism, the party will hear their Dream Guardian calling out for help. However, when the party reaches them, it is only to discover that the true identity of their visitor is the illithid known as the Emperor.

After defending the Emperor, it will explain how it used the power of the Prism and Orpheus to protect the party from the Absolute, and recite to the party its history as an adventurer and finding freedom from the Absolute. The Emperor will offer the party an Astral Touched Tadpole, which causes the user to transform into a partial-illithid. It insists the path of the mind flayer is preferable, regardless of the player's view on them.

Though this may seen contradictory to its previous promise as the Dream Guardian; to ensure the party do not become mind flayers, this promise refers to the player becoming a mind flayer unwillingly because of the Elder Brain. The Emperor is in favour of the player becoming a mind flayer of their own volition and without the influence of the Elder Brain.

Act Three
Elfsong Tavern
Main article: Visit the Emperor's Old Hideout
As the party nears the Elfsong, the Emperor will remark that the tavern is the location of its old hideout. The hideout proper is in the basement, past the Knights of the Shield's hideout. In it, the player will find various sentimental knick knacks from the Emperor's previous life, before becoming an illithid.

Around the room is its old dog Rascal's collar, its favourite recipe (fiddlehead soup), its first adventuring sword, and part of a cutlery set from its mother; the butter knife having been lost during its last shipwreck on the Isle of Balduran, inside the wreck of the Wandering Eye ship.

There are also some more illithid-adequate items such as chains for its preferred prey - allegedly criminals and lawbreakers - and jars for brains.

The Wyrmway
See also: Wyrmway and The Blade of Frontiers
Once the party completes the Wyrmway trials, they will find the corpse of Ansur the Dragon. Interacting with his body will awaken Ansur's spirit, which briefly possesses the player in order to communicate. As Ansur's introduction concludes, he will detect the Emperor within the Astral Prism.

Ico knownSpells lvl 03.png Act 3 Spoilers! This section reveals details about the story of Baldur's Gate 3.

Ansur will reveal that the Emperor in fact was formerly Balduran, the founder of Baldur's Gate. Furthermore, he explains that while the Emperor initially did not want to become a mind flayer, it eventually fully embraced its new form, and its comfort with this caused a rift between the Emperor and Ansur. After ""exhausting all possibility of reversing (the Emperor's) condition"", Ansur was agonizing and the Emperor (as seen in the letter on Ansur's body) attempted to convince him to leave. Ansur then attempted to murder the Emperor during its sleep as a mercy killing, and the Emperor killed Ansur in self-defense.

This development is somewhat foreshadowed when the player first meets The Emperor in their true form, as the song that plays during the encounter is a variation of The Elf Song, which prominently features Balduran in its lyrics.
Endings
Ico knownSpells lvl 03.png Act 3 Spoilers! This section reveals details about the story of Baldur's Gate 3.

Let the Emperor use the Netherstones
The Emperor unless convinced otherwise is mostly concerned with its survival and prosperity. Should the player allow it to wield the Netherstones, it will follow through on destroying the Elder Brain, at the cost of letting it ""assimilate"" with Orpheus.
If the player suggests to the Emperor to take control of the Netherbrain, it will mention that the thought of becoming the Absolute did cross its mind. But unless otherwise persuaded, it will refuse, claiming that whoever becomes the leader of the Cult of the Absolute will be in an open war with the Githyanki, which is a war it is not certain it will survive. The Emperor will destroy the Netherbrain, and the parasites within its control in this ending.
The Emperor controls the Netherbrain
It is also possible, after suggesting it to take control of the Netherbrain, to persuade it. In this scenario, it does not free the player or their party, instead making them mindless thralls and assuming absolute control of them, continuing the Grand Design.
Orpheus is freed
If the player frees Orpheus, the Emperor will abandon the party, and side with the Netherbrain for the sake of its own survival, as it believes that Orpheus will kill it.
Attack the Emperor
The Emperor can be attacked and killed when it first reveals itself to be a mind flayer. This will result in the influence of the Netherbrain taking over control of the party, ending the game.
List of interactions
See Dream Guardian to read about its previous conversations with the player when it was in disguise.
Charm Person Icon.png Romance Spoilers This section reveals details about romance and may contain mature themes.

Players have a limited number of opportunities to interact with the Emperor, and as such, opportunities for conversation are much more limited compared to that of companions.

Conversation scenes are available, but only occur during Act 3, after its ""true"" identity is revealed to the player, and all scenes require a long rest to trigger. The Emperor will occasionally also talk to the player as they walk through different locations in Baldur's Gate.

Conversation scenes
Known conversation opportunities with the Emperor currently include the following cases, but each scene appears to have multiple outcomes that affect the tone of all subsequent conversations.

Depending on the player's choices, the Emperor's behaviour has many possible states. The more the player treats the Emperor like a ""person"", the more it will act as such, compared to other illithids. The more the player treats The Emperor like a monstrosity with hostile intent, the more it will respond to the player with threatening language and visions of it acting like a hostile illithid.

Identity revealed
During Help Your Protector at the start of Act 3, a conversation is automatically triggered when the player ventures far enough into the Astral Plane. A combat encounter in some form is inevitable from this conversation, and then another set of conversation options are available after the combat resolves. The Emperor will have nothing further to say when this conversation ends, even if the player tries to interact with it further.

Regarding Duke Stelmane
When the player first explores the Rivington area, being in proximity to certain characters or objects will ""inform"" the player about the recent death of Duke Belynne Stelmane. This will trigger a line of ambient commentary from The Emperor. The next time a Long Rest is triggered, the player may trigger a scene discussing The Emperor's reactions in more depth. Certain dialogue choices made during earlier conversations seem to disqualify the player from this scene. If the player does not long rest before completing the quest Visit the Emperor's Old Hideout, this scene will be skipped entirely.

On conclusion of Visit the Emperor's Old Hideout
This scene may be available to trigger (by long resting) after the player completes the quest Visit the Emperor's Old Hideout.

Possible states for this scene appear to vary heavily depending on the player's choices in prior conversation scenes, with the general differentiating factor being the ""attitude"" the player appears to express towards illithids, and towards the Emperor, through their selected options in these prior scenes.

If the player tried to kill the Emperor in Act One, by choosing the dialogue option ""You do a great impression of a human. But you're not fooling me."" , the Emperor offers to share memories through a vision. This vision shows Stelmane paralysed in pain, being brainwashed, and turning into the Emperor's puppet. Her face emotionless, and the Emperor puppeteering her gestures to get a sense of company. Such was its true relationship with Duke Stelmane. [note 2]

The Emperor uses this memory to frighten the player. It gives them orders, and threatens to make them half-illithid even if they refuse.

Romance
In terms of game mechanics, it is technically possible to romance the Emperor. [note 3]

If the player chooses to reject its advances, the Emperor's attitude in conversation will change in a way that appears to be reactively appropriate to the way it was treated. For example, if the option ""Absolutely not, you freak!"" is chosen at any opportunity, the Emperor's treatment of the player takes a much more hostile tone in all future interactions.

Players have a limited number of opportunities to interact with the Emperor, and as such, opportunities for romantically-styled interactions are much more limited compared to the other primary companions.

If the player visits Crèche Y'llek prior to the start of Act 3, killing the Dream Guardian will subsequently lock the player out of romancing the Emperor, and from interacting with it in general.

There are many possible ways to interact with the Emperor in the available conversation scenes. It currently seems that the primary way to unlock ""romantic"" options is by choosing dialogue that generally treats the Emperor more like ""any other person"", and does not show explicit hostility towards its actions, or its illithid characteristics.

The player does not need to accept the powers of the Astral-Touched Tadpole to unlock this option. The Emperor seems to take offence to destroying the tadpole, but more testing is needed to determine if this has any effect on the available scenes.

The scene that occurs after completing Visit the Emperor's Old Hideout is generally regarded as the ""primary"" romantic scene. As long as the player is receptive to the Emperor's advances, conclusions to this scene will allow the player to engage in more intimate activities with it.

Conversation options that acknowledge this romance (after the primary scene has concluded) appear to exist in a limited number of places. For example, it is possible to tell Raphael ""I don't want any part of this — the Emperor is my lover."" during a specific conversation, if initiated after the romance scene has happened.

Engaging in the primary scene has no effect on other ongoing romances, even when romancing Lae'zel, who is generally hostile to illithids.

Achievements
A-Mind Blown.jpg
Mind Blown
Romance the Emperor.


Gallery
They called me The Emperor
They called me The Emperor


The Emperor feeding on criminals
The Emperor feeding on criminals


Character portrait by Edward Vanderghote
Character portrait by Edward Vanderghote


The Emperor's model
The Emperor's model

Notes
The Emperor's existence confirms the Dream Guardian as being an illithid influence, albeit in a different way.
In Early Access, the Dream Guardian (known then as Dream Visitor) was implied to be a mental manifestation of the player's tadpole, as it eased them towards using their powers more, as well as showing them a future of domination and control.
In the Full Release, the Emperor plays a similar role, in the sense that it also encourages the player to expand their potential through using the tadpole's power, but it is much more passive. In addition, its interests seem to be aligned against the Absolute.
Footnotes
 The Emperor, like other mind flayers, is addressed using the ""it"" pronoun. It is incidentally referred to as ""he"" in-game, and ""they"" in the game's files, possibly due to an oversight, or characters conflating its current and previous identities.
 The Emperor's vision of its control over Belynne Stelmane is corroborated by the 5e module,  Baldur's Gate: Descent into Avernus. In it, Stelmane is described as having a secret, mental battle against a mind flayer. This mind flayer is very likely the Emperor itself, and as a result, puts its entire ""alliance"" with Stelmane into question. It is very possible the Emperor and Stelmane did not have a proper alliance at all, and rather, the Emperor enthralled her for its needs. Whether this was always the case, or if they had a genuine alliance beforehand, isn't fully clear.
 This romance behaves somewhat differently from that with companions, as the Emperor generally cannot be interacted with outside of cutscenes, and romantic progression is limited to the final act of the game.
References
 Dialogue with Ansur.
 The Netherbrain's dialogue to the player at the Morphic Pool."
8,https://whattocook.substack.com/p/so-into-northern-spain,"so into northern spain!
our magical urban-plus-outdoor-adventure itinerary
CAROLINE CHAMBERS
MAY 29, 2024

I recently returned from a trip to Northern Spain with two of my best friends, Lily and Nellie, sponsored by elsewhere, a travel company that works with local experts to create truly unique travel experiences.

On our last day in Spain, we had an incredible private tour of the Guggenheim Museum Bilbao, and then partook in our favorite trip ritual: hopping around from pintxo bar to pintxo bar (pintxos = small little plates of food — similar to tapas but they are typically sitting on the bar and you point to what you want), drinking txakoli (a lightly effervescent white wine that is very, very popular in the region), and playing gin rummy.

We then headed back to our hotel room, crammed onto the bed with, well, another bottle of txakoli, and recorded what I can only assume was the greatest podcast episode of all time. We recapped the full trip, what we loved, what we would change. We shared the most absurd moments. We laughed so hard and made so much fun of each other in a way that only really old friends who have just spent 12 days straight together can do. I’ve been so excited to share it.

But… the recording failed. I have no idea what happened, but I blame the txakoli.

I thought about re-recording with them remotely, but the magic of that moment, being there together on the last night of a truly perfect trip, is gone. So instead, I’ll share the highlights here! Full itinerary with ALL the details is here.


We spent two nights in Madrid and could have spent at least four. There’s so much to see and do here, and we barely scratched the surface. Our highlights were shopping in the Chueca and Salamanca neighborhoods — Malababa, Soeur, and Sessùn were some of our favorite shops. We had a wonderful meal at Charrúa Madrid and fun cocktails at Ficus Bar. You can find all of our favorite shops, tapas bars, and many places that we didn’t get to explore but wanted to on my Spain map here.


strolling the charming, windy streets of chueca!

San Sebastián is the coolest town — worth a trip to Spain all on its own. It’s a coastal town on the Northern coast of Spain, right next to the French border, and it fully charmed us. It’s that perfect European blend of old meets new — hip young surfers going to 200-year-old pintxo bars to meet up with their friends after surfing all afternoon, 100-year-old pastry shops next door to chic Spanish design ateliers. We spent three nights there and loved every minute, but these were our highlights:

cooking in a private gastronomic club
When you’re wandering the cobblestone streets of San Sebastián looking for your next pintxo, you might find yourself stumbling through the doorway of a friendly looking restaurant, only to be turned away. “Disculpa, privado!” they’ll tell you — “Sorry, private!”

You’ve stumbled into a sociedad gastronómica — one of San Sebastián’s 100+ private gastronomic clubs. Each one has its own personality and offering, but at its core is this: it’s a private social club centered around cooking and eating with fellow members. Members reserve a time to cook — depending on the size of the club, three to four members can cook at a time — and they can invite guests to join them. Members will head to one of the many local markets, grab their food, and then bring it back to the shared kitchen to cook for/with their friends and family.


post-lunch txakoli on the deck at the club
The only hitch? You have to be with a member to go inside of one! This was the coolest tour that elsewhere set up for us — Jani, our guide, was an incredibly cool young mom and entrepreneur (she owns her own tour company) and is a member of several clubs. She picked us up at our hotel, took us to her favorite local market and farmer’s market to grab ingredients, and then back to her club, which was a stone building with wooden beams and felt like a scene out of a Game of Thrones banquet. We cooked a local fish dish, a tomato salad, and seared white asparagus, and she taught us all about Basque cuisine and the local gastronomic clubs.

We spent the entire afternoon cooking and eating and drinking txakoli on the deck of her club overlooking the city. A cannot miss experience if you find yourself in San Seb! Here’s a great Saveur article if you want to read more about the clubs.

pintxo hopping
Pick three or four pintxos bars that are close together, and hop from bar to bar, eating a snack and having a glass of txakoli at each one. This is always my favorite way to explore a new place on limited time — an appetizer at one spot, a meal at another, after-dinner drinks at a third, so the pintxos culture really allows you to see a lot of places in one night!

My map has a lot of great pintxos bars saved, but here’s an especially great hop (these are all on my map):

Start at Ganbara and grab whatever looks good — we had delightful fried padrón peppers.

Walk over to Txepetxa Taberna for a little sandwich — my favorites were the simple ones filled with local jámon and idiazábal cheese (similar to manchego).

The cheesy risotto at Borda Berri was phenomenal.

Finish your night at Otaegui, the oldest bakery in the city, for pantxineta, a delightful puff pastry and custard-filled local treat, and a slice of Basque cheesecake.


pintxo hopping around the old town!
surfing
Nellie is a big surfer and after a full 24 hours of pestering, finally convinced me to go out with her. There are tons of surf shops that rent wetsuits and boards lining the beach, so it was really easy to get into the water. The waves were absolutely perfect. Gentle, clear water, no getting-stuck-in-a-washing-machine vibes when you fall of your board. It was so, so much fun.



My favorite part of the trip was our four days on the camino. The Camino de Santiago, also known as the Way of Saint James, is a well-trod pilgrimage route that traverses Spain, culminating at the Cathedral of Santiago de Compostela in Galicia. The camino has origins back to the medieval era and has four routes connecting different parts of Spain to Galicia. The routes consist of trails, modern roads, and original medieval stone pathways…


that pass through everything from modern city to medieval village…


to small fishing town…


to stunning wilderness landscapes.


The flysch was truly breathtaking. If you wind up in San Sabastián and want to walk just one day of the camino, walk from Orio to Zumaia, all the way out past the town to see the flysch, then sleep at Hotel Iturregi that night!
The Camino Francés is the most popular route, but my friends and I did a small portion of the Camino Norte, which San Sebastián is right on. The Norte is challenging and stunningly beautiful, with steep, rugged terrain, but with plenty of stopping places for pintxos and a glass of wine along the way. It’s hard to sum up what a wonderful experience hiking it was.

We originally wanted to hike directly from one location to the next — to hike from one hotel to the next without ever getting in a car — 15 to 20 miles per day on the camino. However, the hotels along the route are rustic, more rustic than we were up for, so we wound up switching to stay at a gorgeous hotel in a txakoli vineyard, and just took cabs to and from the camino for our last two days.

Hiking the camino was a bucket list experience — we hiked through fishing villages and past a truly breathtaking flysch rock formation, a cult tried to recruit us by beckoning us in with warm tea and cake, we talked about everything, we talked about nothing, we laughed so hard it hurt. I cannot wait to get back to the camino one day.


Bilbao was an excellent urban re-introduction at the end of four days on the camino. We really didn’t know what to expect, and we were pleasantly surprised. I wouldn’t make a trip to Spain specifically for Bilbao, but I’d certainly make an effort to visit if I was in Northern Spain.

The Guggenheim alone could occupy an entire day — it’s a massive collection of modern and contemporary art. The architecture itself is reason alone to visit. We loved touring the museum with a private tour guide who regaled us with detailed information about every single piece.


We strolled along the river from the Guggenheim all the way to the seven streets area, where we bopped between shops and pintxo bars for the rest of the day. Admittedly, we enjoyed a lot of Italian food in Bilbao as we were feeling a little burnt out on Basque food by this point in the trip. No shame in dabbling in a little pizza while in Spain! You’ll find lots of Basque and Italian recs on my map!

After two nights in Bilbao, we headed home. It was a truly magical trip that we’ll still be laughing and reminiscing about when we’re old ladies.



I love electrolyte powders but so many brands have a lot of sodium in them, which can make me feel really bloated. Ultima is lower-sodium and the flavors are great!

Lily had a lanyard strap iPhone case that I was really jealous of. Kind of dorky looking but who cares, it was so convenient to have her phone handy for photos and looking up directions!

I will never stop yappin’ about my Owala water bottle. Having the ability to drink out of the straw or tilt it and chug it never gets old!

I brought disposable cameras for each of us and I can’t wait to get the film back!! Mattis saw them and has been begging me to get him one — it would be fun to give your kids a disposable camera to capture a summer trip!



Pan-fried dover sole with buttery tomato sauce and corn risotto. The reviews on this week’s recipe are RAVING! It really is so good and fancy feeling. You have to make this one ASAP, especially if you travel somewhere with access to great fish this summer!

Sunshine curry was a cult fave last summer — of yours and of mine! I love making a huge batch and eating it for lunch all week long (a perk of the fact that my kids won’t touch it, though I know many kids love this one!).

My summer tomato galette is the stuff of summer produce dreams. Buy a store-bought pie crust to make it even easier, but it’s worth making the crust from scratch if you have 15 extra minutes! PS: Make the boursin summer squash galette while you’re at it. Two summery galettes + a simple salad of romaine tossed with salt, pepper, really good extra-virgin olive oil, and balsamic, with shaved Parm, and chopped almonds would be such a chic little meal.

Chicken panzanella is one of my all-time faves. Crisp croutons, juicy tomatoes, perfectly cooked chicken, balsamic, olive oil, basil, with big hunks of melty goat cheese. It’s the perfect summer lunch or dinner! Don’t let your croutons burn!


I loved reading about all of your happy places in the comments of last week’s post! My number one takeaway is that I need to spend more time in Maine — so many of your happy places are there! Including this week’s winner, Heather.


Dreamy! And now you can bring your new pair of Lake Pajamas to Boothbay Harbor this summer! Email me your size and address!

Do you have any Spain recs to add? Anyone taking an exciting trip this summer? We’re staying local for most of the summer so that we can enjoy our new cabin, minus our annual two-week Southern voyage to DeBordieu, SC to visit George’s family and Bald Head Island, NC to visit mine!"
9,https://dmtalkies.com/the-zone-of-interest-ending-explained-and-summary-2023-film/,"‘The Zone Of Interest’ Ending Explained & Film Summary: What Happens To Rudolf And Hedwig Hoss?
PUBLISHED

FEBRUARY 21, 2024
BY
SOURYA SUR ROY
0COMMENTS
The Zone Of Interest Ending Explained Film Summary Hedwig Ross, Rudolph Ross
Credits: A24
The Zone of Interest is a new historical drama film by English filmmaker Jonathan Glazer that manages to recreate a terrible moment from history with a unique and devastating effect. Loosely adapted from Martin Amis’ novel of the same name, the film’s plot follows the Hoss family, who live right beside the Auschwitz concentration camp, going about their usual lives with no concern for the terrible crimes being committed right outside. The Zone of Interest is all about subtle, indirect expressions that are poignant enough to pierce through the visual layer, successfully making the viewer all the more uncomfortable with every passing minute.

Spoiler Alert

Plot Summary: What Is The Film About?
The Zone of Interest opens with a noticeably long black screen, with only a soft sound being eerily stretched in the background, perhaps preparing us for what is to unfold on screen over the next hundred or so minutes. When the visuals come on, though, there is nothing unusual or out of the ordinary, as a family is seen spending some personal time by the forested banks of a river. This is a secluded spot reserved only for the family, and it seems to be their most common way of spending leisure time. As the girls are led by a nanny through the bushes, possibly for some lesson in gardening and wildlife, the boys jump into the river along with their father. Sometime later, the family reunites and leaves the riverbank, driving away in two black, sinister-looking cars. On that very night, the father of the house is seen going around, switching off all the lights, before going to bed.

While there is really nothing odd in this whole presentation of a family spending a day with themselves, the chilling reality of the matter is revealed when the film introduces the particular lot. The family is that of Rudolf Hoss, a notorious real figure from history, infamous for being a distinguished SS officer and the commander of the Auschwitz concentration camp. Most of the entire film, and the whole of the opening scene, actually takes place in Auschwitz, meaning that the leisurely picnic of the big family literally took place only a few miles away from the spot of the ongoing genocide. This is the very premise of The Zone of Interest, for it shows the tumultuous time of history from the perspective of the Hoss family, mainly the patriarch Rudolf and his wife, Hedwig.

The couple lives in an idyllic resort with their two sons and three daughters, the youngest still a baby, right on the other side of the high walls of the concentration camp. Despite the inhuman torture and killing going on right outside the walls that separate their lives, the Hoss family members are not perturbed by the matter at all. Instead, they are rather accustomed to Auschwitz, cherishing their time and accepting it as their new home.

How Does The Film Powerfully Present The Harrowing Events Of The Holocaust?
The most remarkable thing about The Zone of Interest is how it manages to say so much without directly saying it, combining the visual and the aural through a unique dissonance. With regards to the visuals, meaning scenes that play out to take forward the mostly simple and common story, the camera hardly ever leaves the confines of the host house. While some exceptions take place towards the latter part of the film, when Rudolf is transferred to a different concentration camp and he is seen at his new post, almost no scene of the camp in Auschwitz is seen. But the audio track picks up on numerous cries, lashes, and sounds that clearly come from the outside world but are heavily ignored. There is only one brief scene in which we are shown a side-angled close-up of Rudolf while he is at his workplace, which is a camp intended to kill Jews by the thousands. Indeed, the man is shot looking at the work he is rather proud of doing, amidst thick smoke bellowing out and loud cries and shrieks of helpless people. Rudolf certainly has no reservations about overseeing a genocide, but the film particularly shines with respect to how it uses the very usual to highlight the horrific context in the backdrop.

Early on in the film, Rudolf’s family and his subordinates celebrate the man’s birthday with a fancy cake, and all the Nazi soldiers come to his house to greet him. This merrymaking literally takes place all while hundreds, if not thousands, of families, are kept locked in the concentration camps and forced into the gas chambers. But nobody seems to notice, or rather, everyone pretends to look through the entire matter, as if nothing shocking is in the works. Rudolf is also seen meeting with a businessman in his house, who comes to show the commander plans and designs for a new, more effective gas chamber that he wants to build for his government. Rudolf goes through the plans without any hesitation and then also reports about this businessman’s portfolio to his higher authorities, convinced that sturdier and better-designed gas chambers are needed to take his beloved nation and his government forward. The Zone of Interest does not really differentiate between evil-doers and those supporting such evil, but Rudolf is definitely in the first category, as he clearly enjoys the torture and killing of people.

What comes as even a bigger shock is the reaction of his wife, Hedwig, for she does not react to any of these massacres either. Rather, the woman is extremely accustomed to the life of the commander’s wife, and she enjoys the perks it brings along. She often receives luxury and expensive items that have been taken away from the prisoners, and on one particular occasion, she is seen receiving a fancy fur coat, since the Nazis did not differentiate between the rich and the poor among their targets. Hedwig immediately throws the coat on her body and tries it out in front of the mirror, only to realize that there is still lipstick lying inside one of the pockets. The presence of the lipstick would obviously be a bold reminder to anyone of the previous owner of the coat and the atrocious torture she must be subjected to at present. However, Hedwig has been wired to not think like that, and instead of any guilt or remorse, she feels rather excited to try on the lipstick, which is now hers as well.

Hedwig maintains a calm and composed nature, without any worry in the world, as she focuses on her gardening and getting a pool built for her children in their compound. The thick, dark smoke from the chimneys of the gas chambers on one side and from the steam engine train that brings in Jewish prisoners every day on the other does not affect the woman at all. The irony of the matter is all the more glaring when Hedwig is absolutely livid that her husband has to be transferred away from Auschwitz. She decides to stay back at the place along with her children because she is unwilling to uproot the life she had built there, including the fancy garden and the greenhouse, and shift somewhere else, which is probably too cold for her comfort as well. The fact that thousands were being faced with worse persecution and millions more would be uprooted, killed, or left disbanded very easily eludes her thought. In this regard, Jonathan Glazer’s film is a really fascinating note on not just the Holocaust but also the effect of systematized violence and the tendency of the masses to side with the oppressors in any given scenario.

The Hoss children are also equally desensitized to seeing murder and killing around them. The boys play around with toy soldiers, all waging war against invisible enemies. Shockingly, they are also seen collecting and playing with gold teeth, which are literally the remains of people who had been killed in the camp. One of the daughters does seem to feel something odd about their house, or she simply sleepwalks as a habit and sits by the door as if waiting for someone to arrive. Nonetheless, this young girl would also grow accustomed to the situation one day and not find anything strange about it. The Jewish prisoners are allowed to get close to the house and the family, as many of them are given the task of cleaning the boots and bringing supplies to the place. But there is also a clear distinction that the Nazi commander maintains from them, which highlights the pure hatred breeding inside his perspective of the people. As soon as Rudolf finds a skull and some ashes in the river that he and his sons were bathing in, he scurries back to his house, and the children are scrubbed clean with utmost precision. In another instance, it is suggested that Rudolf forces himself upon a helpless prisoner woman, but he ensures that he scrubs his private parts before retiring for the night.

The only exception to the unaffected response by the entire family is by Hedwig’s mother, who finds it bizarre that her daughter, her husband, and their children can really live at such a place. The elderly woman definitely has no sympathies for the prisoners, though, but she is rather unable to live with so many signs and reminders of death all around. The stench of burning human bodies and the ash flying around keep her up all night, but the very same elements are like playthings for the two young boys who still lay awake in their room. On a similar night, filled with reminders of the ongoing genocide, Hedwig is seen asking Rudolf to take her on a romantic trip, in the most romantic conversation between the couple in the film. Ultimately, the mother leaves the house unannounced very early the next morning, only leaving behind a note for her daughter. Hedwig simply tosses the note into her furnace insignificantly, almost insulted that someone would find her beloved home distasteful or discomforting. Even after Rudolf leaves for Oranienburg, Hedwig stays at their Auschwitz house with the children.

What Do The Scenes In Infrared Signify?
The Zone of Interest also sparsely presents a few scenes, in which an unacquainted young girl is seen going around Auschwitz, hiding apples and other meager food items inside the trenches. She is clearly doing this extremely dangerous work only to help the prisoners and ease their suffering in whatever little way she can. But interestingly, these scenes are in infrared, or negative, although only as long as the girl is in the outside world. As soon as she returns home, the visuals turn normal, then switch to infrared when she or her mother step out on the balcony. The family is revealed to be Polish locals who have no interest in Nazi ideals and dream of liberation one day. However, the mere fact that the family is still alive, irrespective of whether they are Jewish or not, suggests that they also have to work as collaborators for the Nazis to a certain degree. This was definitely the case with numerous non-Jews during the Nazi occupation who had to work for the horrific authorities despite not wanting to. Going by that logic, the significance of the use of infrared might be in stating how the family cannot be themselves as soon as they step out of their house or into the open balcony as well. Although the girl takes on the dangerous responsibility of helping the prisoners, she still cannot express her true self in public, leading to her being shown in infrared. Another perspective is that the girl and her mother truly stand out in this horrific world solely because of their generous actions. Therefore, in a film like this, in which the Nazis and the enablers are the “normal” people, anyone with any sense of humanity has to be visually differentiated from the Hoss family members.

What Happens To Rudolf Hoss?
During The Zone of Interest‘s ending, Rudolf is seen in his Berlin office as he telephones Hedwig and tells her about his excitement for the concentration camps being built. Rudolf had been given the responsibility of overseeing a new Nazi order in which Hungarian Jews were to be arrested and killed. Although Hedwig refuses to be part of this very direct talk of violence, for she prefers such matters to be in the background, the commander still feels thrilled. He is seen walking down the stairs from his office when suddenly bouts of violent retching hit him on two occasions. In the middle of these two instances, The Zone of Interest briefly moves to modern times, and various reminders of the Holocaust are seen being maintained at the Auschwitz-Birkenau State Museum, right before the place is opened to public visitors. The last scene returns to the past once more, and Rudolf is seen feeling slightly odd, as if someone is watching him, as he continues down the stairs.

The Zone of Interest‘s ending scene seems to suggest that deep in his conscience, Rudolf Hoss does know that his actions can only make one retch, and almost like a fortune-teller, he has an uneasy feeling that his legacy will go down terribly in history. The scene of the museum is a fast jolt back to the right perspective, which had been missing throughout the film. Throughout the entire duration of The Zone of Interest, Rudolf, his family, and his professional associates had all been extremely invested in hiding the evidence and changing the narrative, but ultimately, the thousands of shoes or the torn, ragged uniforms still exist as reminders of the horrible genocide."
10,https://www.loonyparty.com/about/policy-proposals/,"manicfesto proposals
26m tonnes of waste plastic bottles are discarded every year in the UK of which only 45% are recycled. The Loony Party has the answer.. Stop making them..

Before you ask…We have found an alternative. Its called glass.

Some of our Proposals for other elections
Along with the existing Government policy for levelling up the North with the South             we will provide free Spirit Levels to all

We will reduce inflation by giving everyone free pins.

To make trains safer, we will fit them all with cushions on the front.

Any possible schemes thought up by Government, Council , NHS etc,  such as closure of Hosptitals, workplace parking levy etc will be preceded with a Public Consultation which we will then ignore.

We will combat corruption in public life by taking part in it openly, we will                              introduce the Board of Bribery who will set standardised rates?. #sleaze for the many not just the few

We propose to prevent identity theft instantly by calling everyone Chris.

All political and electoral leaflets will be printed on soft paper so that it may be recycled in the appropriate manner.

In an effort to reduce the problems faced by the NHS , it is proposed to reduce                   pregnancy from nine to seven months ?

To protect pets and people of a nervous disposition we would introduce silent fireworks.

With Government helped finance, AstraZeneca should buy out Pfizer, then, as we would have the rights to Viagra, the economy may stay up longer.

Redundant Red Phones boxes will be converted to bijou accommodation to ease the housing shortage.

To make things fairer we will introduce a Court of Human Lefts.

General Election 2022 Manicfesto
General Election 2022 Manicfesto  —— For the Manic, Not the Few

We pledge to fight this election on an invisible platform so that people cannot see the floors in our policies.

Once in Government, we will replace the Foreign Secretary with a British one!

Waitng Lists

We will reduce hospital waiting lists by using a smaller font.

Immigration

We will reduce net migration by making sure that any nets are secured more firmly to the ground.

Inflation

We will reduce inflation by giving everyone free pins

Government Policy

When formulating Policies the Government relies heavily on Expert Advise. Remember  – Experts built the Titanic

The Loony Party will also take into account the opinion of “Dave on Facebook”

Energy Policy

We will get rid of the Energy Price Cap and replace it with a Top Hat (This will also help our Millenery Industry)
2. We will get rid of all Standing Charges. (We are quite capable of sitting down and freezing to death)
3. All the hot air spoken in Parliament will be redirected to the Gas Distribution Networks.
Stressful times in the House
In order to calm down the passions and stresses currently exhibited in Parliament, the Loony Party would make all M.P’s have half an hours compulsory Tai chi everyday.
This would counteract the other 23 ½ hours Chi Ting they do for the rest of the time

Corruption

We in The Loony Party are quite willing to accept bribes , and inducements from the Government in exchange that we don’t stand in the election.
We will combat corruption in public life by taking part in it openly, we will also introduce the Board of Bribery who will set standardised rates?
Northern Powerhouse

The Loony party will invest millions in the Northern Powerhouse.
For clarification all parties agree that, as normal, the North starts at Hadrians Wall and ends where Scotland starts

Brexit

The Border in Northern Ireland would be made out of sponge to prevent a Hard Border
We will renegotiate to stay and lead the E.U and then sack the other 27 countries
Identity Theft

We propose to prevent identity theft instantly by calling everyone Dave.

Play Grounds

We will redevelop Playgrounds for all age groups.

Civil Service

The Civil Service will be extended to all branches of government, because a little politeness goes a long way.

Culture

The British Museum should have a Daddy’s section alongside the current Mummy exhibition.??

Transport

We will only paint yellow lines where you CAN park. Potholes deeper than 3 inches will be marked with a yellow plastic duck .

Elections

All political and electoral leaflets will be printed on soft Toilet paper so that it may be recycled in the appropriate manner. ??

NHS

In an effort to reduce the problems faced by the NHS , it is proposed to reduce pregnancy from nine to seven months ?

Animal Welfare

To protect pets and people of a nervous disposition we would introduce silent fireworks.?

General Election 2019 Manicfesto
General Election 2019 Manicfesto  —— For the Manic, Not the Few

We pledge to fight this election on an invisible platform so that people cannot see the floors in our policies.

Stressful times in the House
In order to calm down the passions and stresses currently exhibited in Parliament, the Loony Party would make all M.P’s have half an hours compulsory Tai chi everyday.
This would counteract the other 23 ½ hours Chi Ting they do for the rest of the time

Corruption

We in The Loony Party are quite willing to accept bribes , and inducements from the Government in exchange that we don’t stand in the election.
We will combat corruption in public life by taking part in it openly, we will also introduce the Board of Bribery who will set standardised rates?
Northern Powerhouse

The Loony party will invest millions in the Northern Powerhouse.
For clarification all parties agree that, as normal, the North starts at Hadrians Wall and ends where Scotland starts

Brexit

The Border in Northern Ireland would be made out of sponge to prevent a Hard Border
We will renegotiate to stay and lead the E.U and then sack the other 27 countries
Identity Theft

We propose to prevent identity theft instantly by calling everyone Dave.

Play Grounds

We will redevelop Playgrounds for all age groups.

Civil Service

The Civil Service will be extended to all branches of government, because a little politeness goes a long way.

Culture

The British Museum should have a Daddy’s section alongside the current Mummy exhibition.??

Transport

We will rename the current Oyster cards, ‘Sardine Cards’ to better reflect the experience when travelling on public transport
2. We will only paint yellow lines where you CAN park. Potholes deeper than 3 inches will be marked with a yellow plastic duck .
Elections

All political and electoral leaflets will be printed on soft Toilet paper so that it may be recycled in the appropriate manner. ??

NHS

In an effort to reduce the problems faced by the NHS , it is proposed to reduce pregnancy from nine to seven months ?

Animal Welfare

To protect pets and people of a nervous disposition we would introduce silent fireworks.?

Policies
We encourage everyone, even current politicians, to submit ideas to our world famous #Manicfesto! The following are some of the most recent from our wonderful Twitter followers…

Once in Government, anyone applying for 7 figure salary positions with the World Health Organisation or as Govt Health Advisors, will have to answer 15 correct questions on “WHO wants to be a Millionaire”.
In Brexit Trade Deals: Germany will be required to pay for treatment of Measles, and Spain will be required to pay for cases of Spanish Flu. The French will pay for all accidents resulting from kissing & broken letters & the Dutch will split all future expenses 50/50.
We will place in law measures to stop panic buying as COVID19 restrictions take hold. Shoppers will only be permitted to buy one panic per person.
It is evident that the 10pm pub curfew is not working , We propose that pubs ask people to leave in alphabetical order.
Shamefully Lord Sutch has never been allowed to take his place in the House of Lords. Nor were Duke Ellington, Count Basie or Lord Rockingham We will end this discrimination against musicians.
To unite the population, we will surround the UK with a large cardboard box so people can be both in and/or out of the EU. This will be known as Schrodinger’s Brexit.
To get more children reading, fish and chips will once again be wrapped in newspaper.
Once in Government we will introduce the Ministry of Clarity. The role of this Ministry will ensure that only the clearest clarity is made clear and the unclear clarity is cleared out. We hope that our position on this is now clear to all.
In Government, we will complete a 5 year Parliament in only 4 years. This policy not only ensures a 20% saving for the public purse but also gives everyone in the UK a year off from listening to our politicians.
The MOT is an annual test to ensure that your car is roadworthy. We will introduce a ROT, an annual test to make sure all roads are car worthy.
And from 1st January 2021, passports will be issued in the colour of political voting. Tories will be Blue, Labour will be Red, Greens will be green. Official Loonies will have leopard spots, and Lib Dem’s will be invisible.
Chinners

Foreign Policy
We will Admit Shamima Begum back to the country only when she accepts Screaming Lord Sutch as her saviour.

Ministry of Info
We will create a New Ministry of Information. It shall consist of the former board of directors of Cambridge Analytica. They already know everything.

Brexit Proposals
We will Send Noel Edmonds to negotiate Brexit because he understands Deal or No Deal.
There will be no need for a backstop to the Brexit negotiations. We’ll have Alec Stewart as wicket-keeper.
James Wallace

Educational Funding
The Loony Party proposes that all Schools would have a Jumble sale or fete or other fundraising event at least twice per month to help raise funds for those little extras. . . such as Desks, Books, paper, pens , etc
R.U. Seerius

Pensions – triple lock
In keeping with the Labour Party’s latest bid to get one or two pensioners to vote for them they have brought out a new policy guaranteeing the Triple lock on pensions until 2025 if they get voted in.
The Loony party of course will go one better and buy a padlock, and as its now safer than a bank,  new mattresses for all pensioners on less than 20p per week.
R.U. Seerius"
11,https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/,"Which GPU(s) to Get for Deep Learning: My Experience and Advice for Using GPUs in Deep Learning
2023-01-30 by Tim Dettmers 1,664 Comments

Deep learning is a field with intense computational requirements, and your choice of GPU will fundamentally determine your deep learning experience. But what features are important if you want to buy a new GPU? GPU RAM, cores, tensor cores, caches? How to make a cost-efficient choice? This blog post will delve into these questions, tackle common misconceptions, give you an intuitive understanding of how to think about GPUs, and will lend you advice, which will help you to make a choice that is right for you.

This blog post is designed to give you different levels of understanding of GPUs and the new Ampere series GPUs from NVIDIA. You have the choice: (1) If you are not interested in the details of how GPUs work, what makes a GPU fast compared to a CPU, and what is unique about the new NVIDIA RTX 40 Ampere series, you can skip right to the performance and performance per dollar charts and the recommendation section. The cost/performance numbers form the core of the blog post and the content surrounding it explains the details of what makes up GPU performance.

(2) If you worry about specific questions, I have answered and addressed the most common questions and misconceptions in the later part of the blog post.

(3) If you want to get an in-depth understanding of how GPUs, caches, and Tensor Cores work, the best is to read the blog post from start to finish. You might want to skip a section or two based on your understanding of the presented topics.

Contents  hide
Overview
How do GPUs work?
The Most Important GPU Specs for Deep Learning Processing Speed
Tensor Cores
Matrix multiplication without Tensor Cores
Matrix multiplication with Tensor Cores
Matrix multiplication with Tensor Cores and Asynchronous copies (RTX 30/RTX 40) and TMA (H100)
Memory Bandwidth
L2 Cache / Shared Memory / L1 Cache / Registers
Estimating Ada / Hopper Deep Learning Performance
Practical Ada / Hopper Speed Estimates
Possible Biases in Estimates
Advantages and Problems for RTX40 and RTX 30 Series
Sparse Network Training
Low-precision Computation
Fan Designs and GPUs Temperature Issues
3-slot Design and Power Issues
Power Limiting: An Elegant Solution to Solve the Power Problem?
RTX 4090s and Melting Power Connectors: How to Prevent Problems
8-bit Float Support in H100 and RTX 40 series GPUs
Raw Performance Ranking of GPUs
GPU Deep Learning Performance per Dollar
GPU Recommendations
Is it better to wait for future GPUs for an upgrade? The future of GPUs.
Question & Answers & Misconceptions
Do I need PCIe 4.0 or PCIe 5.0?
Do I need 8x/16x PCIe lanes?
How do I fit 4x RTX 4090 or 3090 if they take up 3 PCIe slots each?
How do I cool 4x RTX 3090 or 4x RTX 3080?
Can I use multiple GPUs of different GPU types?
What is NVLink, and is it useful?
I do not have enough money, even for the cheapest GPUs you recommend. What can I do?
What is the carbon footprint of GPUs? How can I use GPUs without polluting the environment?
What do I need to parallelize across two machines?
Is the sparse matrix multiplication features suitable for sparse matrices in general?
Do I need an Intel CPU to power a multi-GPU setup?
Does computer case design matter for cooling?
Will AMD GPUs + ROCm ever catch up with NVIDIA GPUs + CUDA?
When is it better to use the cloud vs a dedicated GPU desktop/server?
Version History
Acknowledgments
Related
Related Posts
Overview
This blog post is structured in the following way. First, I will explain what makes a GPU fast. I will discuss CPUs vs GPUs, Tensor Cores, memory bandwidth, and the memory hierarchy of GPUs and how these relate to deep learning performance. These explanations might help you get a more intuitive sense of what to look for in a GPU. I discuss the unique features of the new NVIDIA RTX 40 Ampere GPU series that are worth considering if you buy a GPU. From there, I make GPU recommendations for different scenarios. After that follows a Q&A section of common questions posed to me in Twitter threads; in that section, I will also address common misconceptions and some miscellaneous issues, such as cloud vs desktop, cooling, AMD vs NVIDIA, and others.

How do GPUs work?
If you use GPUs frequently, it is useful to understand how they work. This knowledge will help you to undstand cases where are GPUs fast or slow. In turn, you might be able to understand better why you need a GPU in the first place and how other future hardware options might be able to compete. You can skip this section if you just want the useful performance numbers and arguments to help you decide which GPU to buy. The best high-level explanation for the question of how GPUs work is my following Quora answer:

Read Tim Dettmers‘ answer to Why are GPUs well-suited to deep learning? on Quora
This is a high-level explanation that explains quite well why GPUs are better than CPUs for deep learning. If we look at the details, we can understand what makes one GPU better than another.

The Most Important GPU Specs for Deep Learning Processing Speed
This section can help you build a more intuitive understanding of how to think about deep learning performance. This understanding will help you to evaluate future GPUs by yourself. This section is sorted by the importance of each component. Tensor Cores are most important, followed by memory bandwidth of a GPU, the cache hierachy, and only then FLOPS of a GPU.

Tensor Cores
Tensor Cores are tiny cores that perform very efficient matrix multiplication. Since the most expensive part of any deep neural network is matrix multiplication Tensor Cores are very useful. In fast, they are so powerful, that I do not recommend any GPUs that do not have Tensor Cores.

It is helpful to understand how they work to appreciate the importance of these computational units specialized for matrix multiplication. Here I will show you a simple example of A*B=C matrix multiplication, where all matrices have a size of 32×32, what a computational pattern looks like with and without Tensor Cores. This is a simplified example, and not the exact way how a high performing matrix multiplication kernel would be written, but it has all the basics. A CUDA programmer would take this as a first “draft” and then optimize it step-by-step with concepts like double buffering, register optimization, occupancy optimization, instruction-level parallelism, and many others, which I will not discuss at this point.

To understand this example fully, you have to understand the concepts of cycles. If a processor runs at 1GHz, it can do 10^9 cycles per second. Each cycle represents an opportunity for computation. However, most of the time, operations take longer than one cycle. Thus we essentially have a queue where the next operations needs to wait for the next operation to finish. This is also called the latency of the operation.

Here are some important latency cycle timings for operations. These times can change from GPU generation to GPU generation. These numbers are for Ampere GPUs, which have relatively slow caches.

Global memory access (up to 80GB): ~380 cycles
L2 cache: ~200 cycles
L1 cache or Shared memory access (up to 128 kb per Streaming Multiprocessor): ~34 cycles
Fused multiplication and addition, a*b+c (FFMA): 4 cycles
Tensor Core matrix multiply: 1 cycle
Each operation is always performed by a pack of 32 threads. This pack is termed a warp of threads. Warps usually operate in a synchronous pattern — threads within a warp have to wait for each other. All memory operations on the GPU are optimized for warps. For example, loading from global memory happens at a granularity of 32*4 bytes, exactly 32 floats, exactly one float for each thread in a warp. We can have up to 32 warps = 1024 threads in a streaming multiprocessor (SM), the GPU-equivalent of a CPU core. The resources of an SM are divided up among all active warps. This means that sometimes we want to run fewer warps to have more registers/shared memory/Tensor Core resources per warp.

For both of the following examples, we assume we have the same computational resources. For this small example of a 32×32 matrix multiply, we use 8 SMs (about 10% of an RTX 3090) and 8 warps per SM.

To understand how the cycle latencies play together with resources like threads per SM and shared memory per SM, we now look at examples of matrix multiplication. While the following example roughly follows the sequence of computational steps of matrix multiplication for both with and without Tensor Cores, please note that these are very simplified examples. Real cases of matrix multiplication involve much larger shared memory tiles and slightly different computational patterns.

Matrix multiplication without Tensor Cores
If we want to do an A*B=C matrix multiply, where each matrix is of size 32×32, then we want to load memory that we repeatedly access into shared memory because its latency is about five times lower (200 cycles vs 34 cycles). A memory block in shared memory is often referred to as a memory tile or just a tile. Loading two 32×32 floats into a shared memory tile can happen in parallel by using 2*32 warps. We have 8 SMs with 8 warps each, so due to parallelization, we only need to do a single sequential load from global to shared memory, which takes 200 cycles.

To do the matrix multiplication, we now need to load a vector of 32 numbers from shared memory A and shared memory B and perform a fused multiply-and-accumulate (FFMA). Then store the outputs in registers C. We divide the work so that each SM does 8x dot products (32×32) to compute 8 outputs of C. Why this is exactly 8 (4 in older algorithms) is very technical. I recommend Scott Gray’s blog post on matrix multiplication to understand this. This means we have 8x shared memory accesses at the cost of 34 cycles each and 8 FFMA operations (32 in parallel), which cost 4 cycles each. In total, we thus have a cost of:

200 cycles (global memory) + 8*34 cycles (shared memory) + 8*4 cycles (FFMA) = 504 cycles

Let’s look at the cycle cost of using Tensor Cores.

Matrix multiplication with Tensor Cores
With Tensor Cores, we can perform a 4×4 matrix multiplication in one cycle. To do that, we first need to get memory into the Tensor Core. Similarly to the above, we need to read from global memory (200 cycles) and store in shared memory. To do a 32×32 matrix multiply, we need to do 8×8=64 Tensor Cores operations. A single SM has 8 Tensor Cores. So with 8 SMs, we have 64 Tensor Cores — just the number that we need! We can transfer the data from shared memory to the Tensor Cores with 1 memory transfers (34 cycles) and then do those 64 parallel Tensor Core operations (1 cycle). This means the total cost for Tensor Cores matrix multiplication, in this case, is:

200 cycles (global memory) + 34 cycles (shared memory) + 1 cycle (Tensor Core) = 235 cycles.

Thus we reduce the matrix multiplication cost significantly from 504 cycles to 235 cycles via Tensor Cores. In this simplified case, the Tensor Cores reduced the cost of both shared memory access and FFMA operations.

This example is simplified, for example, usually each thread needs to calculate which memory to read and write to as you transfer data from global memory to shared memory. With the new Hooper (H100) architectures we additionally have the Tensor Memory Accelerator (TMA) compute these indices in hardware and thus help each thread to focus on more computation rather than computing indices.

Matrix multiplication with Tensor Cores and Asynchronous copies (RTX 30/RTX 40) and TMA (H100)
The RTX 30 Ampere and RTX 40 Ada series GPUs additionally have support to perform asynchronous transfers between global and shared memory. The H100 Hopper GPU extends this further by introducing the Tensor Memory Accelerator (TMA) unit. the TMA unit combines asynchronous copies and index calculation for read and writes simultaneously — so each thread no longer needs to calculate which is the next element to read and each thread can focus on doing more matrix multiplication calculations. This looks as follows.

The TMA unit fetches memory from global to shared memory (200 cycles). Once the data arrives, the TMA unit fetches the next block of data asynchronously from global memory. While this is happening, the threads load data from shared memory and perform the matrix multiplication via the tensor core. Once the threads are finished they wait for the TMA unit to finish the next data transfer, and the sequence repeats.

As such, due to the asynchronous nature, the second global memory read by the TMA unit is already progressing as the threads process the current shared memory tile. This means, the second read takes only 200 – 34 – 1 = 165 cycles.

Since we do many reads, only the first memory access will be slow and all other memory accesses will be partially overlapped with the TMA unit. Thus on average, we reduce the time by 35 cycles.

165 cycles (wait for async copy to finish) + 34 cycles (shared memory) + 1 cycle (Tensor Core) = 200 cycles.

Which accelerates the matrix multiplication by another 15%.

From these examples, it becomes clear why the next attribute, memory bandwidth, is so crucial for Tensor-Core-equipped GPUs. Since global memory is the by far the largest cycle cost for matrix multiplication with Tensor Cores, we would even have faster GPUs if the global memory latency could be reduced. We can do this by either increasing the clock frequency of the memory (more cycles per second, but also more heat and higher energy requirements) or by increasing the number of elements that can be transferred at any one time (bus width).

Memory Bandwidth
From the previous section, we have seen that Tensor Cores are very fast. So fast, in fact, that they are idle most of the time as they are waiting for memory to arrive from global memory. For example, during GPT-3-sized training, which uses huge matrices — the larger, the better for Tensor Cores — we have a Tensor Core TFLOPS utilization of about 45-65%, meaning that even for the large neural networks about 50% of the time, Tensor Cores are idle.

This means that when comparing two GPUs with Tensor Cores, one of the single best indicators for each GPU’s performance is their memory bandwidth. For example, The A100 GPU has 1,555 GB/s memory bandwidth vs the 900 GB/s of the V100. As such, a basic estimate of speedup of an A100 vs V100 is 1555/900 = 1.73x.

L2 Cache / Shared Memory / L1 Cache / Registers
Since memory transfers to the Tensor Cores are the limiting factor in performance, we are looking for other GPU attributes that enable faster memory transfer to Tensor Cores. L2 cache, shared memory, L1 cache, and amount of registers used are all related. To understand how a memory hierarchy enables faster memory transfers, it helps to understand how matrix multiplication is performed on a GPU.

To perform matrix multiplication, we exploit the memory hierarchy of a GPU that goes from slow global memory, to faster L2 memory, to fast local shared memory, to lightning-fast registers. However, the faster the memory, the smaller it is.

While logically, L2 and L1 memory are the same, L2 cache is larger and thus the average physical distance that need to be traversed to retrieve a cache line is larger. You can see the L1 and L2 caches as organized warehouses where you want to retrieve an item. You know where the item is, but to go there takes on average much longer for the larger warehouse. This is the essential difference between L1 and L2 caches. Large = slow, small = fast.

For matrix multiplication we can use this hierarchical separate into smaller and smaller and thus faster and faster chunks of memory to perform very fast matrix multiplications. For that, we need to chunk the big matrix multiplication into smaller sub-matrix multiplications. These chunks are called memory tiles, or often for short just tiles.

We perform matrix multiplication across these smaller tiles in local shared memory that is fast and close to the streaming multiprocessor (SM) — the equivalent of a CPU core. With Tensor Cores, we go a step further: We take each tile and load a part of these tiles into Tensor Cores which is directly addressed by registers. A matrix memory tile in L2 cache is 3-5x faster than global GPU memory (GPU RAM), shared memory is ~7-10x faster than the global GPU memory, whereas the Tensor Cores’ registers are ~200x faster than the global GPU memory.

Having larger tiles means we can reuse more memory. I wrote about this in detail in my TPU vs GPU blog post. In fact, you can see TPUs as having very, very, large tiles for each Tensor Core. As such, TPUs can reuse much more memory with each transfer from global memory, which makes them a little bit more efficient at matrix multiplications than GPUs.

Each tile size is determined by how much memory we have per streaming multiprocessor (SM) and how much we L2 cache we have across all SMs. We have the following shared memory sizes on the following architectures:

Volta (Titan V): 128kb shared memory / 6 MB L2
Turing (RTX 20s series): 96 kb shared memory / 5.5 MB L2
Ampere (RTX 30s series): 128 kb shared memory / 6 MB L2
Ada (RTX 40s series): 128 kb shared memory / 72 MB L2
We see that Ada has a much larger L2 cache allowing for larger tile sizes, which reduces global memory access. For example, for BERT large during training, the input and weight matrix of any matrix multiplication fit neatly into the L2 cache of Ada (but not other Us). As such, data needs to be loaded from global memory only once and then data is available throught the L2 cache, making matrix multiplication about 1.5 – 2.0x faster for this architecture for Ada. For larger models the speedups are lower during training but certain sweetspots exist which may make certain models much faster. Inference, with a batch size larger than 8 can also benefit immensely from the larger L2 caches.

Estimating Ada / Hopper Deep Learning Performance
This section is for those who want to understand the more technical details of how I derive the performance estimates for Ampere GPUs. If you do not care about these technical aspects, it is safe to skip this section.

Practical Ada / Hopper Speed Estimates
Suppose we have an estimate for one GPU of a GPU-architecture like Hopper, Ada, Ampere, Turing, or Volta. It is easy to extrapolate these results to other GPUs from the same architecture/series. Luckily, NVIDIA already benchmarked the A100 vs V100 vs H100 across a wide range of computer vision and natural language understanding tasks. Unfortunately, NVIDIA made sure that these numbers are not directly comparable by using different batch sizes and the number of GPUs whenever possible to favor results for the H100 GPU. So in a sense, the benchmark numbers are partially honest, partially marketing numbers. In general, you could argue that using larger batch sizes is fair, as the H100/A100 GPU has more memory. Still, to compare GPU architectures, we should evaluate unbiased memory performance with the same batch size.

To get an unbiased estimate, we can scale the data center GPU results in two ways: (1) account for the differences in batch size, (2) account for the differences in using 1 vs 8 GPUs. We are lucky that we can find such an estimate for both biases in the data that NVIDIA provides.

Doubling the batch size increases throughput in terms of images/s (CNNs) by 13.6%. I benchmarked the same problem for transformers on my RTX Titan and found, surprisingly, the very same result: 13.5% — it appears that this is a robust estimate.

As we parallelize networks across more and more GPUs, we lose performance due to some networking overhead. The A100 8x GPU system has better networking (NVLink 3.0) than the V100 8x GPU system (NVLink 2.0) — this is another confounding factor. Looking directly at the data from NVIDIA, we can find that for CNNs, a system with 8x A100 has a 5% lower overhead than a system of 8x V100. This means if going from 1x A100 to 8x A100 gives you a speedup of, say, 7.00x, then going from 1x V100 to 8x V100 only gives you a speedup of 6.67x.  For transformers, the figure is 7%.

Using these figures, we can estimate the speedup for a few specific deep learning architectures from the direct data that NVIDIA provides. The Tesla A100 offers the following speedup over the Tesla V100:

SE-ResNeXt101: 1.43x
Masked-R-CNN: 1.47x
Transformer (12 layer, Machine Translation, WMT14 en-de): 1.70x
Thus, the figures are a bit lower than the theoretical estimate for computer vision. This might be due to smaller tensor dimensions, overhead from operations that are needed to prepare the matrix multiplication like img2col or Fast Fourier Transform (FFT), or operations that cannot saturate the GPU (final layers are often relatively small). It could also be artifacts of the specific architectures (grouped convolution).

The practical transformer estimate is very close to the theoretical estimate. This is probably because algorithms for huge matrices are very straightforward. I will use these practical estimates to calculate the cost efficiency of GPUs.

Possible Biases in Estimates
The estimates above are for H100, A100 , and V100 GPUs. In the past, NVIDIA sneaked unannounced performance degradations into the “gaming” RTX GPUs: (1) Decreased Tensor Core utilization, (2) gaming fans for cooling, (3) disabled peer-to-peer GPU transfers. It might be possible that there are unannounced performance degradations in the RTX 40 series compared to the full Hopper H100.

As of now, one of these degradations was found for Ampere GPUs: Tensor Core performance was decreased so that RTX 30 series GPUs are not as good as Quadro cards for deep learning purposes. This was also done for the RTX 20 series, so it is nothing new, but this time it was also done for the Titan equivalent card, the RTX 3090. The RTX Titan did not have performance degradation enabled.

Currently, no degradation for Ada GPUs are known, but I update this post with news on this and let my followers on twitter know.

Advantages and Problems for RTX40 and RTX 30 Series
The new NVIDIA Ampere RTX 30 series has additional benefits over the NVIDIA Turing RTX 20 series, such as sparse network training and inference. Other features, such as the new data types, should be seen more as an ease-of-use-feature as they provide the same performance boost as Turing does but without any extra programming required.

The Ada RTX 40 series has even further advances like 8-bit Float (FP8) tensor cores. The RTX 40 series also has similar power and temperature issues compared to the RTX 30. The issue of melting power connector cables in the RTX 40 can be easily prevented by connecting the power cable correctly.

Sparse Network Training
Ampere allows for fine-grained structure automatic sparse matrix multiplication at dense speeds. How does this work? Take a weight matrix and slice it into pieces of 4 elements. Now imagine 2 elements of these 4 to be zero. Figure 1 shows how this could look like.

Figure 1: Structure supported by the sparse matrix multiplication feature in Ampere GPUs. The figure is taken from Jeff Pool's GTC 2020 presentation on  Accelerating Sparsity in the NVIDIA Ampere Architecture by the courtesy of NVIDIA.
Figure 1: Structure supported by the sparse matrix multiplication feature in Ampere GPUs. The figure is taken from Jeff Pool’s GTC 2020 presentation on Accelerating Sparsity in the NVIDIA Ampere Architecture by the courtesy of NVIDIA.
When you multiply this sparse weight matrix with some dense inputs, the sparse matrix tensor core feature in Ampere automatically compresses the sparse matrix to a dense representation that is half the size as can be seen in Figure 2. After this compression, the densely compressed matrix tile is fed into the tensor core which computes a matrix multiplication of twice the usual size. This effectively yields a 2x speedup since the bandwidth requirements during matrix multiplication from shared memory are halved.

Figure 2: The sparse matrix is compressed to a dense representation before the matrix multiplication is performed.
Figure 2: The sparse matrix is compressed to a dense representation before the matrix multiplication is performed. The figure is taken from Jeff Pool’s GTC 2020 presentation on Accelerating Sparsity in the NVIDIA Ampere Architecture by the courtesy of NVIDIA.
I was working on sparse network training in my research and I also wrote a blog post about sparse training. One criticism of my work was that “You reduce the FLOPS required for the network, but it does not yield speedups because GPUs cannot do fast sparse matrix multiplication.” Well, with the addition of the sparse matrix multiplication feature for Tensor Cores, my algorithm, or other sparse training algorithms, now actually provide speedups of up to 2x during training.

Figure 3: The sparse training algorithm that I developed has three stages: (1) Determine the importance of each layer. (2) Remove the smallest, unimportant weights. (3) Grow new weights proportional to the importance of each layer. Read more about my work in my sparse training blog post.
Figure 3: The sparse training algorithm that I developed has three stages: (1) Determine the importance of each layer. (2) Remove the smallest, unimportant weights. (3) Grow new weights proportional to the importance of each layer. Read more about my work in my sparse training blog post.
While this feature is still experimental and training sparse networks are not commonplace yet, having this feature on your GPU means you are ready for the future of sparse training.

Low-precision Computation
In my work, I’ve previously shown that new data types can improve stability during low-precision backpropagation.

Figure 4: Low-precision deep learning 8-bit datatypes that I developed. Deep learning training benefits from highly specialized data types. My dynamic tree datatype uses a dynamic bit that indicates the beginning of a binary bisection tree that quantized the range [0, 0.9] while all previous bits are used for the exponent. This allows to dynamically represent numbers that are both large and small with high precision.
Figure 4: Low-precision deep learning 8-bit datatypes that I developed. Deep learning training benefits from highly specialized data types. My dynamic tree datatype uses a dynamic bit that indicates the beginning of a binary bisection tree that quantized the range [0, 0.9] while all previous bits are used for the exponent. This allows to dynamically represent numbers that are both large and small with high precision.
Currently, if you want to have stable backpropagation with 16-bit floating-point numbers (FP16), the big problem is that ordinary FP16 data types only support numbers in the range [-65,504, 65,504]. If your gradient slips past this range, your gradients explode into NaN values. To prevent this during FP16 training, we usually perform loss scaling where you multiply the loss by a small number before backpropagating to prevent this gradient explosion.

The BrainFloat 16 format (BF16) uses more bits for the exponent such that the range of possible numbers is the same as for FP32: [-3*10^38, 3*10^38]. BF16 has less precision, that is significant digits, but gradient precision is not that important for learning. So what BF16 does is that you no longer need to do any loss scaling or worry about the gradient blowing up quickly. As such, we should see an increase in training stability by using the BF16 format as a slight loss of precision.

What this means for you: With BF16 precision, training might be more stable than with FP16 precision while providing the same speedups. With 32-bit TensorFloat (TF32) precision, you get near FP32 stability while giving the speedups close to FP16. The good thing is, to use these data types, you can just replace FP32 with TF32 and FP16 with BF16 — no code changes required!

Overall, though, these new data types can be seen as lazy data types in the sense that you could have gotten all the benefits with the old data types with some additional programming efforts (proper loss scaling, initialization, normalization, using Apex). As such, these data types do not provide speedups but rather improve ease of use of low precision for training.

Fan Designs and GPUs Temperature Issues
While the new fan design of the RTX 30 series performs very well to cool the GPU, different fan designs of non-founders edition GPUs might be more problematic. If your GPU heats up beyond 80C, it will throttle itself and slow down its computational speed / power. This overheating can happen in particular if you stack multiple GPUs next to each other. A solution to this is to use PCIe extenders to create space between GPUs.

Spreading GPUs with PCIe extenders is very effective for cooling, and other fellow PhD students at the University of Washington and I use this setup with great success. It does not look pretty, but it keeps your GPUs cool! This has been running with no problems at all for 4 years now. It can also help if you do not have enough space to fit all GPUs in the PCIe slots. For example, if you can find the space within a desktop computer case, it might be possible to buy standard 3-slot-width RTX 4090 and spread them with PCIe extenders within the case. With this, you might solve both the space issue and cooling issue for a 4x RTX 4090 setup with a single simple solution.

Figure 5: 4x GPUs with PCIe extenders. It looks like a mess, but it is very effective for cooling. I used this rig for 2 years and cooling is excellent despite problematic RTX 2080 Ti Founders Edition GPUs.
Figure 5: 4x GPUs with PCIe extenders. It looks like a mess, but it is very effective for cooling. I used this rig for 4 years and cooling is excellent despite problematic RTX 2080 Ti Founders Edition GPUs.
3-slot Design and Power Issues
The RTX 3090 and RTX 4090 are 3-slot GPUs, so one will not be able to use it in a 4x setup with the default fan design from NVIDIA. This is kind of justified because it runs at over 350W TDP, and it will be difficult to cool in a multi-GPU 2-slot setting. The RTX 3080 is only slightly better at 320W TDP, and cooling a 4x RTX 3080 setup will also be very difficult.

It is also difficult to power a 4x 350W = 1400W or 4x 450W = 1800W system in the 4x RTX 3090 or 4x RTX 4090 case. Power supply units (PSUs) of 1600W are readily available, but having only 200W to power the CPU and motherboard can be too tight. The components’ maximum power is only used if the components are fully utilized, and in deep learning, the CPU is usually only under weak load. With that, a 1600W PSU might work quite well with a 4x RTX 3080 build, but for a 4x RTX 3090 build, it is better to look for high wattage PSUs (+1700W). Some of my followers have had great success with cryptomining PSUs — have a look in the comment section for more info about that. Otherwise, it is important to note that not all outlets support PSUs above 1600W, especially in the US. This is the reason why in the US, there are currently few standard desktop PSUs above 1600W on the market. If you get a server or cryptomining PSUs, beware of the form factor — make sure it fits into your computer case.

Power Limiting: An Elegant Solution to Solve the Power Problem?
It is possible to set a power limit on your GPUs. So you would be able to programmatically set the power limit of an RTX 3090 to 300W instead of their standard 350W. In a 4x GPU system, that is a saving of 200W, which might just be enough to build a 4x RTX 3090 system with a 1600W PSU feasible. It also helps to keep the GPUs cool. So setting a power limit can solve the two major problems of a 4x RTX 3080 or 4x RTX 3090 setups, cooling, and power, at the same time. For a 4x setup, you still need effective blower GPUs (and the standard design may prove adequate for this), but this resolves the PSU problem.

Figure 6: Reducing the power limit has a slight cooling effect. Reducing the RTX 2080 Ti power limit by 50-60 W decreases temperatures slightly and fans run more silent.
Figure 6: Reducing the power limit has a slight cooling effect. Reducing the RTX 2080 Ti power limit by 50-60 W decreases temperatures slightly and fans run more silent.
You might ask, “Doesn’t this slow down the GPU?” Yes, it does, but the question is by how much. I benchmarked the 4x RTX 2080 Ti system shown in Figure 5 under different power limits to test this. I benchmarked the time for 500 mini-batches for BERT Large during inference (excluding the softmax layer). I choose BERT Large inference since, from my experience, this is the deep learning model that stresses the GPU the most. As such, I would expect power limiting to have the most massive slowdown for this model. As such, the slowdowns reported here are probably close to the maximum slowdowns that you can expect. The results are shown in Figure 7.

Figure 7: Measured slowdown for a given power limit on an RTX 2080 Ti. Measurements taken are mean processing times for 500 mini-batches of BERT Large during inference (excluding softmax layer).
Figure 7: Measured slowdown for a given power limit on an RTX 2080 Ti. Measurements taken are mean processing times for 500 mini-batches of BERT Large during inference (excluding softmax layer).
As we can see, setting the power limit does not seriously affect performance. Limiting the power by 50W — more than enough to handle 4x RTX 3090 — decreases performance by only 7%.

RTX 4090s and Melting Power Connectors: How to Prevent Problems
There was a misconception that RTX 4090 power cables melt because they were bent. However, it was found that only 0.1% of users had this problem and the problem occured due to user error. Here a video that shows that the main problem is that cables were not inserted correctly.

So using RTX 4090 cards is perfectly safe if you follow the following install instructions:

If you use an old cable or old GPU make sure the contacts are free of debri / dust.
Use the power connector and stick it into the socket until you hear a *click* — this is the most important part.
Test for good fit by wiggling the power cable left to right. The cable should not move.
Check the contact with the socket visually, there should be no gap between cable and socket.
8-bit Float Support in H100 and RTX 40 series GPUs
The support of the 8-bit Float (FP8) is a huge advantage for the RTX 40 series and H100 GPUs. With 8-bit inputs it allows you to load the data for matrix multiplication twice as fast, you can store twice as much matrix elements in your caches which in the Ada and Hopper architecture are very large, and now with FP8 tensor cores you get 0.66 PFLOPS of compute for a RTX 4090 — this is more FLOPS then the entirety of the worlds fastest supercomputer in year 2007. 4x RTX 4090 with FP8 compute rival the faster supercomputer in the world in year 2010 (deep learning started to work just in 2009).

The main problem with using 8-bit precision is that transformers can get very unstable with so few bits and crash during training or generate non-sense during inference. I have written a paper about the emergence of instabilities in large language models and I also written a more accessible blog post.

The main take-way is this: Using 8-bit instead of 16-bit makes things very unstable, but if you keep a couple of dimensions in high precision everything works just fine.


Main results from my work on 8-bit matrix multiplication for Large Language Models (LLMs). We can see that the best 8-bit baseline fails to deliver good zero-shot performance. The method that I developed, LLM.int8(), can perform Int8 matrix multiplication with the same results as the 16-bit baseline.
But Int8 was already supported by the RTX 30 / A100 / Ampere generation GPUs, why is FP8 in the RTX 40 another big upgrade? The FP8 data type is much more stable than the Int8 data type and its easy to use it in functions like layer norm or non-linear functions, which are difficult to do with Integer data types. This will make it very straightforward to use it in training and inference. I think this will make FP8 training and inference relatively common in a couple of months.

If you want to read more about the advantages of Float vs Integer data types you can read my recent paper about k-bit inference scaling laws. Below you can see one relevant main result for Float vs Integer data types from this paper. We can see that bit-by-bit, the FP4 data type preserve more information than Int4 data type and thus improves the mean LLM zeroshot accuracy across 4 tasks.


4-bit Inference scaling laws for Pythia Large Language Models for different data types. We see that bit-by-bit, 4-bit float data types have better zeroshot accuracy compared to the Int4 data types.
Raw Performance Ranking of GPUs
Below we see a chart of raw relevative performance across all GPUs. We see that there is a gigantic gap in 8-bit performance of H100 GPUs and old cards that are optimized for 16-bit performance.


Shown is raw relative transformer performance of GPUs. For example, an RTX 4090 has about 0.33x performance of a H100 SMX for 8-bit inference. In other words, a H100 SMX is three times faster for 8-bit inference compared to a RTX 4090.
For this data, I did not model 8-bit compute for older GPUs. I did so, because 8-bit Inference and training are much more effective on Ada/Hopper GPUs because of the 8-bit Float data type and Tensor Memory Accelerator (TMA) which saves the overhead of computing read/write indices which is particularly helpful for 8-bit matrix multiplication. Ada/Hopper also have FP8 support, which makes in particular 8-bit training much more effective.

I did not model numbers for 8-bit training because to model that I need to know the latency of L1 and L2 caches on Hopper/Ada GPUs, and they are unknown and I do not have access to such GPUs. On Hopper/Ada, 8-bit training performance can well be 3-4x of 16-bit training performance if the caches are as fast as rumored.

But even with the new FP8 tensor cores there are some additional issues which are difficult to take into account when modeling GPU performance. For example, FP8 tensor cores do not support transposed matrix multiplication which means backpropagation needs either a separate transpose before multiplication or one needs to hold two sets of weights — one transposed and one non-transposed — in memory. I used two sets of weight when I experimented with Int8 training in my LLM.int8() project and this reduced the overall speedups quite significantly. I think one can do better with the right algorithms/software, but this shows that missing features like a transposed matrix multiplication for tensor cores can affect performance.

For old GPUs, Int8 inference performance is close to the 16-bit inference performance for models below 13B parameters. Int8 performance on old GPUs is only relevant if you have relatively large models with 175B parameters or more. If you are interested in 8-bit performance of older GPUs, you can read the Appendix D of my LLM.int8() paper where I benchmark Int8 performance.

GPU Deep Learning Performance per Dollar
Below we see the chart for the performance per US dollar for all GPUs sorted by 8-bit inference performance. How to use the chart to find a suitable GPU for you is as follows:

Determine the amount of GPU memory that you need (rough heuristic: at least 12 GB for image generation; at least 24 GB for work with transformers)
While 8-bit inference and training is experimental, it will become standard within 6 months. You might need to do some extra difficult coding to work with 8-bit in the meantime. Is that OK for you? If not, select for 16-bit performance.
Using the metric determined in (2), find the GPU with the highest relative performance/dollar that has the amount of memory you need.
We can see that the RTX 4070 Ti is most cost-effective for 8-bit and 16-bit inference while the RTX 3080 remains most cost-effective for 16-bit training. While these GPUs are most cost-effective, they are not necessarily recommended as they do not have sufficient memory for many use-cases. However, it might be the ideal cards to get started on your deep learning journey. Some of these GPUs are excellent for Kaggle competition where one can often rely on smaller models. Since to do well in Kaggle competitions the method of how you work is more important than the models size, many of these smaller GPUs are excellent for Kaggle competitions.

The best GPUs for academic and startup servers seem to be A6000 Ada GPUs (not to be confused with A6000 Turing). The H100 SXM GPU is also very cost effective and has high memory and very strong performance. If I would build a small cluster for a company/academic lab, I would use 66-80% A6000 GPUs and 20-33% H100 SXM GPUs. If I get a good deal on L40 GPUs, I would also pick them instead of A6000, so you can always ask for a quote on these.


Shown is relative performance per US Dollar of GPUs normalized by the cost for a desktop computer and the average Amazon and eBay price for each GPU. Additionally, the electricity cost of ownership for 5 years is added with an electricity price of 0.175 USD per kWh and a 15% GPU utilization rate. The electricity cost for a RTX 4090 is about $100 per year. How to read and interpret the chart: a desktop computer with RTX 4070 Ti cards owned for 5 years yields about 2x more 8-bit inference performance per dollar compared to a RTX 3090 GPU.
GPU Recommendations
I have a create a recommendation flow-chart that you can see below (click here for interactive app from Nan Xiao). While this chart will help you in 80% of cases, it might not quite work for you because the options might be too expensive. In that case, try to look at the benchmarks above and pick the most cost effective GPU that still has enough GPU memory for your use-case. You can estimate the GPU memory needed by running your problem in the vast.ai or Lambda Cloud for a while so you know what you need. The vast.ai or Lambda Cloud might also work well if you only need a GPU very sporadically (every couple of days for a few hours) and you do not need to download and process large dataset to get started. However, cloud GPUs are usually not a good option if you use your GPU for many months with a high usage rate each day (12 hours each day). You can use the example in the “When is it better to use the cloud vs a dedicated GPU desktop/server?” section below to determine if cloud GPUs are good for you.


GPU recommendation chart for Ada/Hopper GPUs. Follow the answers to the Yes/No questions to find the GPU that is most suitable for you. While this chart works well in about 80% of cases, you might end up with a GPU that is too expensive. Use the cost/performance charts above to make a selection instead. [interactive app]
Is it better to wait for future GPUs for an upgrade? The future of GPUs.
To understand if it makes sense to skip this generation and buy the next generation of GPUs, it makes sense to talk a bit about what improvements in the future will look like.

In the past it was possible to shrink the size of transistors to improve speed of a processor. This is coming to an end now. For example, while shrinking SRAM increased its speed (smaller distance, faster memory access), this is no longer the case. Current improvements in SRAM do not improve its performance anymore and might even be negative. While logic such as Tensor Cores get smaller, this does not necessarily make GPU faster since the main problem for matrix multiplication is to get memory to the tensor cores which is dictated by SRAM and GPU RAM speed and size. GPU RAM still increases in speed if we stack memory modules into high-bandwidth modules (HBM3+), but these are too expensive to manufacture for consumer applications. The main way to improve raw speed of GPUs is to use more power and more cooling as we have seen in the RTX 30s and 40s series. But this cannot go on for much longer.

Chiplets such as used by AMD CPUs are another straightforward way forward. AMD beat Intel by developing CPU chiplets. Chiplets are small chips that are fused together with a high speed on-chip network. You can think about them as two GPUs that are so physically close together that you can almost consider them a single big GPU. They are cheaper to manufacture, but more difficult to combine into one big chip. So you need know-how and fast connectivity between chiplets. AMD has a lot of experience with chiplet design. AMD’s next generation GPUs are going to be chiplet designs, while NVIDIA currently has no public plans for such designs. This may mean that the next generation of AMD GPUs might be better in terms of cost/performance compared to NVIDIA GPUs.

However, the main performance boost for GPUs is currently specialized logic. For example, the asynchronous copy hardware units on the Ampere generation (RTX 30 / A100 / RTX 40) or the extension, the Tensor Memory Accelerator (TMA), both reduce the overhead of copying memory from the slow global memory to fast shared memory (caches) through specialized hardware and so each thread can do more computation. The TMA also reduces overhead by performing automatic calculations of read/write indices which is particularly important for 8-bit computation where one has double the elements for the same amount of memory compared to 16-bit computation. So specialized hardware logic can accelerate matrix multiplication further.
Low-bit precision is another straightforward way forward for a couple of years. We will see widespread adoption of 8-bit inference and training in the next months. We will see widespread 4-bit inference in the next year. Currently, the technology for 4-bit training does not exists, but research looks promising and I expect the first high performance FP4 Large Language Model (LLM) with competitive predictive performance to be trained in 1-2 years time.

Going to 2-bit precision for training currently looks pretty impossible, but it is a much easier problem than shrinking transistors further. So progress in hardware mostly depends on software and algorithms that make it possible to use specialized features offered by the hardware.

We will probably be able to still improve the combination of algorithms + hardware to the year 2032, but after that will hit the end of GPU improvements (similar to smartphones). The wave of performance improvements after 2032 will come from better networking algorithms and mass hardware. It is uncertain if consumer GPUs will be relevant at this point. It might be that you need an RTX 9090 to run run Super HyperStableDiffusion Ultra Plus 9000 Extra or OpenChatGPT 5.0, but it might also be that some company will offer a high-quality API that is cheaper than the electricity cost for a RTX 9090 and you want to use a laptop + API for image generation and other tasks.

Overall, I think investing into a 8-bit capable GPU will be a very solid investment for the next 9 years. Improvements at 4-bit and 2-bit are likely small and other features like Sort Cores would only become relevant once sparse matrix multiplication can be leveraged well. We will probably see some kind of other advancement in 2-3 years which will make it into the next GPU 4 years from now, but we are running out of steam if we keep relying on matrix multiplication. This makes investments into new GPUs last longer.

Question & Answers & Misconceptions
Do I need PCIe 4.0 or PCIe 5.0?
Generally, no. PCIe 5.0 or 4.0 is great if you have a GPU cluster. It is okay if you have an 8x GPU machine, but otherwise, it does not yield many benefits. It allows better parallelization and a bit faster data transfer. Data transfers are not a bottleneck in any application. In computer vision, in the data transfer pipeline, the data storage can be a bottleneck, but not the PCIe transfer from CPU to GPU. So there is no real reason to get a PCIe 5.0 or 4.0 setup for most people. The benefits will be maybe 1-7% better parallelization in a 4 GPU setup.

Do I need 8x/16x PCIe lanes?
Same as with PCIe 4.0 — generally, no. PCIe lanes are needed for parallelization and fast data transfers, which are seldom a bottleneck. Operating GPUs on 4x lanes is fine, especially if you only have 2 GPUs. For a 4 GPU setup, I would prefer 8x lanes per GPU, but running them at 4x lanes will probably only decrease performance by around 5-10% if you parallelize across all 4 GPUs.

How do I fit 4x RTX 4090 or 3090 if they take up 3 PCIe slots each?
You need to get one of the two-slot variants, or you can try to spread them out with PCIe extenders. Besides space, you should also immediately think about cooling and a suitable PSU.

PCIe extenders might also solve both space and cooling issues, but you need to make sure that you have enough space in your case to spread out the GPUs. Make sure your PCIe extenders are long enough!

How do I cool 4x RTX 3090 or 4x RTX 3080?
See the previous section.

Can I use multiple GPUs of different GPU types?
Yes, you can! But you cannot parallelize efficiently across GPUs of different types since you will often go at the speed of the slowest GPU (data and fully sharded parallelism). So different GPUs work just fine, but parallelization across those GPUs will be inefficient since the fastest GPU will wait for the slowest GPU to catch up to a synchronization point (usually gradient update).

What is NVLink, and is it useful?
Generally, NVLink is not useful. NVLink is a high speed interconnect between GPUs. It is useful if you have a GPU cluster with +128 GPUs. Otherwise, it yields almost no benefits over standard PCIe transfers.

I do not have enough money, even for the cheapest GPUs you recommend. What can I do?
Definitely buy used GPUs. You can buy a small cheap GPU for prototyping and testing and then roll out for full experiments to the cloud like vast.ai or Lambda Cloud. This can be cheap if you train/fine-tune/inference on large models only every now and then and spent more time protoyping on smaller models.

What is the carbon footprint of GPUs? How can I use GPUs without polluting the environment?
I built a carbon calculator for calculating your carbon footprint for academics (carbon from flights to conferences + GPU time). The calculator can also be used to calculate a pure GPU carbon footprint. You will find that GPUs produce much, much more carbon than international flights. As such, you should make sure you have a green source of energy if you do not want to have an astronomical carbon footprint. If no electricity provider in our area provides green energy, the best way is to buy carbon offsets. Many people are skeptical about carbon offsets. Do they work? Are they scams?

I believe skepticism just hurts in this case, because not doing anything would be more harmful than risking the probability of getting scammed. If you worry about scams, just invest in a portfolio of offsets to minimize risk.

I worked on a project that produced carbon offsets about ten years ago. The carbon offsets were generated by burning leaking methane from mines in China. UN officials tracked the process, and they required clean digital data and physical inspections of the project site. In that case, the carbon offsets that were produced were highly reliable. I believe many other projects have similar quality standards.

What do I need to parallelize across two machines?
If you want to be on the safe side, you should get at least +50Gbits/s network cards to gain speedups if you want to parallelize across machines. I recommend having at least an EDR Infiniband setup, meaning a network card with at least 50 GBit/s bandwidth. Two EDR cards with cable are about $500 on eBay.

In some cases, you might be able to get away with 10 Gbit/s Ethernet, but this is usually only the case for special networks (certain convolutional networks) or if you use certain algorithms (Microsoft DeepSpeed).

Is the sparse matrix multiplication features suitable for sparse matrices in general?
It does not seem so. Since the granularity of the sparse matrix needs to have 2 zero-valued elements, every 4 elements, the sparse matrices need to be quite structured. It might be possible to adjust the algorithm slightly, which involves that you pool 4 values into a compressed representation of 2 values, but this also means that precise arbitrary sparse matrix multiplication is not possible with Ampere GPUs.

Do I need an Intel CPU to power a multi-GPU setup?
I do not recommend Intel CPUs unless you heavily use CPUs in Kaggle competitions (heavy linear algebra on the CPU). Even for Kaggle competitions AMD CPUs are still great, though. AMD CPUs are cheaper and better than Intel CPUs in general for deep learning. For a 4x GPU built, my go-to CPU would be a Threadripper. We built dozens of systems at our university with Threadrippers, and they all work great — no complaints yet. For 8x GPU systems, I would usually go with CPUs that your vendor has experience with. CPU and PCIe/system reliability is more important in 8x systems than straight performance or straight cost-effectiveness.

Does computer case design matter for cooling?
No. GPUs are usually perfectly cooled if there is at least a small gap between GPUs. Case design will give you 1-3 C better temperatures, space between GPUs will provide you with 10-30 C improvements. The bottom line, if you have space between GPUs, cooling does not matter. If you have no space between GPUs, you need the right cooler design (blower fan) or another solution (water cooling, PCIe extenders), but in either case, case design and case fans do not matter.

Will AMD GPUs + ROCm ever catch up with NVIDIA GPUs + CUDA?
Not in the next 1-2 years. It is a three-way problem: Tensor Cores, software, and community.

AMD GPUs are great in terms of pure silicon: Great FP16 performance, great memory bandwidth. However, their lack of Tensor Cores or the equivalent makes their deep learning performance poor compared to NVIDIA GPUs. Packed low-precision math does not cut it. Without this hardware feature, AMD GPUs will never be competitive. Rumors show that some data center card with Tensor Core equivalent is planned for 2020, but no new data emerged since then. Just having data center cards with a Tensor Core equivalent would also mean that few would be able to afford such AMD GPUs, which would give NVIDIA a competitive advantage.

Let’s say AMD introduces a Tensor-Core-like-hardware feature in the future. Then many people would say, “But there is no software that works for AMD GPUs! How am I supposed to use them?” This is mostly a misconception. The AMD software via ROCm has come to a long way, and support via PyTorch is excellent. While I have not seen many experience reports for AMD GPUs + PyTorch, all the software features are integrated. It seems, if you pick any network, you will be just fine running it on AMD GPUs. So here AMD has come a long way, and this issue is more or less solved.

However, if you solve software and the lack of Tensor Cores, AMD still has a problem: the lack of community. If you have a problem with NVIDIA GPUs, you can Google the problem and find a solution. That builds a lot of trust in NVIDIA GPUs. You have the infrastructure that makes using NVIDIA GPUs easy (any deep learning framework works, any scientific problem is well supported). You have the hacks and tricks that make usage of NVIDIA GPUs a breeze (e.g., apex). You can find experts on NVIDIA GPUs and programming around every other corner while I knew much less AMD GPU experts.

In the community aspect, AMD is a bit like Julia vs Python. Julia has a lot of potential, and many would say, and rightly so, that it is the superior programming language for scientific computing. Yet, Julia is barely used compared to Python. This is because the Python community is very strong. Numpy, SciPy, Pandas are powerful software packages that a large number of people congregate around. This is very similar to the NVIDIA vs AMD issue.

Thus, it is likely that AMD will not catch up until Tensor Core equivalent is introduced (1/2 to 1 year?) and a strong community is built around ROCm (2 years?). AMD will always snatch a part of the market share in specific subgroups (e.g., cryptocurrency mining, data centers). Still, in deep learning, NVIDIA will likely keep its monopoly for at least a couple more years.

When is it better to use the cloud vs a dedicated GPU desktop/server?
Rule-of-thumb: If you expect to do deep learning for longer than a year, it is cheaper to get a desktop GPU. Otherwise, cloud instances are preferable unless you have extensive cloud computing skills and want the benefits of scaling the number of GPUs up and down at will.

Numbers in the following paragraphs are going to change, but it serves as a scenario that helps you to understand the rough costs. You can use similar math to determine if cloud GPUs are the best solution for you.

For the exact point in time when a cloud GPU is more expensive than a desktop depends highly on the service that you are using, and it is best to do a little math on this yourself. Below I do an example calculation for an AWS V100 spot instance with 1x V100 and compare it to the price of a desktop with a single RTX 3090 (similar performance). The desktop with RTX 3090 costs $2,200 (2-GPU barebone + RTX 3090). Additionally, assuming you are in the US, there is an additional $0.12 per kWh for electricity. This compares to $2.14 per hour for the AWS on-demand instance.

At 15% utilization per year, the desktop uses:

(350 W (GPU) + 100 W (CPU))*0.15 (utilization) * 24 hours * 365 days = 591 kWh per year

So 591 kWh of electricity per year, that is an additional $71.

The break-even point for a desktop vs a cloud instance at 15% utilization (you use the cloud instance 15% of time during the day), would be about 300 days ($2,311 vs $2,270):

$2.14/h * 0.15 (utilization) * 24 hours * 300 days = $2,311

So if you expect to run deep learning models after 300 days, it is better to buy a desktop instead of using AWS on-demand instances.

You can do similar calculations for any cloud service to make the decision if you go for a cloud service or a desktop.

Common utilization rates are the following:

PhD student personal desktop: < 15%
PhD student slurm GPU cluster: > 35%
Company-wide slurm research cluster: > 60%
In general, utilization rates are lower for professions where thinking about cutting edge ideas is more important than developing practical products. Some areas have low utilization rates (interpretability research), while other areas have much higher rates (machine translation, language modeling). In general, the utilization of personal machines is almost always overestimated. Commonly, most personal systems have a utilization rate between 5-10%. This is why I would highly recommend slurm GPU clusters for research groups and companies instead of individual desktop GPU machines.

Version History
2023-01-30: Improved font and recommendation chart. Added 5 years cost of ownership electricity perf/USD chart. Updated Async copy and TMA functionality. Slight update to FP8 training. General improvements.
2023-01-16: Added Hopper and Ada GPUs. Added GPU recommendation chart. Added information about the TMA unit and L2 cache.
2020-09-20: Added discussion of using power limiting to run 4x RTX 3090 systems. Added older GPUs to the performance and cost/performance charts. Added figures for sparse matrix multiplication.
2020-09-07: Added NVIDIA Ampere series GPUs. Included lots of good-to-know GPU details.
2019-04-03: Added RTX Titan and GTX 1660 Ti. Updated TPU section. Added startup hardware discussion.
2018-11-26: Added discussion of overheating issues of RTX cards.
2018-11-05: Added RTX 2070 and updated recommendations. Updated charts with hard performance data. Updated TPU section.
2018-08-21: Added RTX 2080 and RTX 2080 Ti; reworked performance analysis
2017-04-09: Added cost-efficiency analysis; updated recommendation with NVIDIA Titan Xp
2017-03-19: Cleaned up blog post; added GTX 1080 Ti
2016-07-23: Added Titan X Pascal and GTX 1060; updated recommendations
2016-06-25: Reworked multi-GPU section; removed simple neural network memory section as no longer relevant; expanded convolutional memory section; truncated AWS section due to not being efficient anymore; added my opinion about the Xeon Phi; added updates for the GTX 1000 series
2015-08-20: Added section for AWS GPU instances; added GTX 980 Ti to the comparison relation
2015-04-22: GTX 580 no longer recommended; added performance relationships between cards
2015-03-16: Updated GPU recommendations: GTX 970 and GTX 580
2015-02-23: Updated GPU recommendations and memory calculations
2014-09-28: Added emphasis for memory requirement of CNNs
Acknowledgments
I thank Suhail for making me aware of outdated prices on H100 GPUs, Gjorgji Kjosev for pointing out font issues, Anonymous for pointing out that the TMA unit does not exist on Ada GPUs, Scott Gray for pointing out that FP8 tensor cores have no transposed matrix multiplication, and reddit and HackerNews users for pointing out many other improvements.

For past updates of this blog post, I want to thank Mat Kelcey for helping me to debug and test custom code for the GTX 970; I want to thank Sander Dieleman for making me aware of the shortcomings of my GPU memory advice for convolutional nets; I want to thank Hannes Bretschneider for pointing out software dependency problems for the GTX 580; and I want to thank Oliver Griesel for pointing out notebook solutions for AWS instances. I want to thank Brad Nemire for providing me with an RTX Titan for benchmarking purposes. I want to thank Agrin Hilmkil, Ari Holtzman, Gabriel Ilharco, Nam Pho for their excellent feedback on the previous version of this blog post."
12,https://gleam.run/cheatsheets/gleam-for-python-users/,"Gleam for Python users
Hello productive pragmatic Pythonistas!

a soft wavey boundary between two sections of the website
Comments
Variables
Match operator
Variables type annotations
Functions
Exporting functions
Function type annotations
Referencing functions
Labelled arguments
Operators
Constants
Blocks
Data types
Strings
Tuples
Lists
Dicts
Flow control
Case
Try
Type aliases
Custom types
Records
Unions
Opaque custom types
Modules
Imports
Named imports
Unqualified imports
Comments
Python
In Python, comments are written with a # prefix.

# Hello, Joe!
A docstring (matching “””) that occurs as the first statement in a module, function, class, or method definition will become the __doc__ attribute of that object.

def a_function():
    """"""Return some important data.""""""
    pass
Gleam
In Gleam, comments are written with a // prefix.

// Hello, Joe!
Comments starting with /// are used to document the following statement. Comments starting with //// are used to document the current module.

//// This module is very important.

/// The answer to life, the universe, and everything.
const answer: Int = 42
Variables
You can reassign variables in both languages.

Python
size = 50
size = size + 100
size = 1
Python has no specific variable keyword. You choose a name and that’s it!

Gleam
Gleam has the let keyword before its variable names.

let size = 50
let size = size + 100
let size = 1
Match operator
Python
Python supports basic, one directional destructuring (also called unpacking). Tuple of values can be unpacked and inner values can be assigned to left-hand variable names.

(a, b) = (1, 2)
# a == 1
# b == 2

# works also for for-loops
for key, value in enumerate(a_dict):
    print(key, value)
Gleam
In Gleam, let and = can be used for pattern matching, but you’ll get compile errors if there’s a type mismatch, and a runtime error if there’s a value mismatch. For assertions, the equivalent let assert keyword is preferred.

let #(x, _) = #(1, 2)
let assert [] = [1] // runtime error
let assert [y] = ""Hello"" // compile error, type mismatch
Variables type annotations
Python
Python is a dynamically typed language. Types are only checked at runtime and a variable can have different types in its lifetime.

Type hints (Python 3+) are optional annotations that document the code with type information. These annotations are accessible at runtime via the __annotations__ module-level variable.

These hints will mainly be used to inform static analysis tools like IDEs, linters…

some_list: list[int] = [1, 2, 3]
Gleam
In Gleam type annotations can optionally be given when binding variables.

let some_list: List(Int) = [1, 2, 3]
Gleam will check the type annotation to ensure that it matches the type of the assigned value. It does not need annotations to type check your code, but you may find it useful to annotate variables to hint to the compiler that you want a specific type to be inferred.

Functions
Python
In Python, you can define functions with the def keyword. In that case, the return keyword is mandatory.

def sum(x, y):
    return x + y
Anonymous functions returning a single expression can also be defined with the lambda keyword and be assigned into variables.

mul = lambda x, y: x * y
mul(1, 2)
Gleam
Gleam’s functions are declared using a syntax similar to Rust or JavaScript. Gleam’s anonymous functions have a similar syntax and don’t need a . when called.

pub fn sum(x, y) {
  x + y
}

let mul = fn(x, y) { x * y }
mul(1, 2)
Exporting functions
Python
In Python, top level functions are exported by default. There is no notion of private module-level functions.

Gleam
In Gleam, functions are private by default and need the pub keyword to be public.

// this is public
pub fn sum(x, y) {
  x + y
}

// this is private
fn mul(x, y) {
  x * y
}
Function type annotations
Python
Type hints can be used to optionally annotate function arguments and return types.

Discrepancies between type hints and actual values at runtime do not prevent interpretation of the code.

Static code analysers (IDE tooling, type checkers like mypy) will be required to detect those errors.

def sum(x: int, y: int) -> int:
    return x + y

def mul(x: int, y: int) -> bool:
    # no errors from the interpreter.
    return x * y
Gleam
Functions can optionally have their argument and return types annotated in Gleam. These type annotations will always be checked by the compiler and throw a compilation error if not valid. The compiler will still type check your program using type inference if annotations are omitted.

pub fn add(x: Int, y: Int) -> Int {
  x + y
}

pub fn mul(x: Int, y: Int) -> Bool { // compile error, type mismatch
  x * y
}
Referencing functions
Python
As long as functions are in scope they can be assigned to a new variable. There is no special syntax to assign a module function to a variable.

Gleam
Gleam has a single namespace for value and functions within a module, so there is no need for a special syntax to assign a module function to a variable.

fn identity(x) {
  x
}

fn main() {
  let func = identity
  func(100)
}
Labelled arguments
Both Python and Gleam have ways to give arguments names and in any order.

Python
Keyword arguments are evaluated once at function definition time, and there is no evidence showing a noticeable performance penalty when using named arguments.

When calling a function, arguments can be passed

positionally, in the same order of the function declaration
by name, in any order
def replace(inside: str, each: str, with_string: str):
    pass

# equivalent calls
replace('hello world', 'world', 'you')
replace(each='world', inside='hello world',  with_string='you')
Gleam
In Gleam arguments can be given a label as well as an internal name. Contrary to Python, the name used at the call-site does not have to match the name used for the variable inside the function.

pub fn replace(inside string, each pattern, with replacement) {
  go(string, pattern, replacement)
}
replace(each: "","", with: "" "", inside: ""A,B,C"")
There is no performance cost to Gleam’s labelled arguments as they are optimised to regular function calls at compile time, and all the arguments are fully type checked.

Operators
Operator	Python	Gleam	Notes
Equal	==	==	In Gleam both values must be of the same type
Strictly equal to	==	==	Comparison in Gleam is always strict. (see note for Python)
Reference equality	is	 	True only if the two objects have the same reference
Not equal	!=	!=	In Gleam both values must be of the same type
Greater than	>	>	In Gleam both values must be ints
Greater than	>	>.	In Gleam both values must be floats
Greater or equal	>=	>=	In Gleam both values must be ints
Greater or equal	>=	>=.	In Gleam both values must be floats
Less than	<	<	In Gleam both values must be ints
Less than	<	<.	In Gleam both values must be floats
Less or equal	<=	<=	In Gleam both values must be ints
Less or equal	<=	<=.	In Gleam both values must be floats
Boolean and	and	&&	In Gleam both values must be bools
Logical and	and	 	Not available in Gleam
Boolean or	or	||	In Gleam both values must be bools
Logical or	or	 	Not available in Gleam
Add	+	+	In Gleam both values must be ints
Add	+	+.	In Gleam both values must be floats
Subtract	-	-	In Gleam both values must be ints
Subtract	-	-.	In Gleam both values must be floats
Multiply	*	*	In Gleam both values must be ints
Multiply	*	*.	In Gleam both values must be floats
Divide	/	/	In Gleam both values must be ints
Divide	/	/.	In Gleam both values must be floats
Remainder	%	%	In Gleam both values must be ints, in Gleam negative values behave differently: Use int.modulo to mimick Python’s behavior.
Concatenate	+	<>	In Gleam both values must be strings
Pipe	 	|>	Gleam’s pipe can pipe into anonymous functions. This operator does not exist in python
Some notes for Python:

== is by default comparing by value:

scalars will have their value compared
the only type cast will be for 0 and 1 that will be coerced to False and True respectively
variables that point to the same object will be equal with ==
two objects with the same members values won’t be equal:

no structural equality, unless the __eq__ operator is redefined.
Python operators are short-circuiting as in Gleam.
Python operators can be overloaded and be applied to any types with potential custom behaviors
Constants
Python
In Python, top-level declarations are in the global/module scope is the highest possible scope. Any variables and functions defined will be accessible from anywhere in the code.

There is no notion of constant variables in Python.

# in the global scope
THE_ANSWER = 42
Gleam
In Gleam constants can be created using the const keyword.

const the_answer = 42

pub fn main() {
  the_answer
}
Blocks
Python
Python blocks are always associated with a function / conditional / class declarations… There is no way to create multi-line expressions blocks like in Gleam.

Blocks are declared via indentation.

def a_func():
    # A block here
    pass
Gleam
In Gleam braces { } are used to group expressions.

pub fn main() {
  let x = {
    some_function(1)
    2
  }
  let y = x * {x + 10} // braces are used to change arithmetic operations order
  y
}
Data types
Strings
In Python, strings are stored as unicode code-points sequence. Strings can be encoded or decoded to/from a specific encoding.

In Gleam all strings are UTF-8 encoded binaries.

Python
""Hellø, world!""
Gleam
""Hellø, world!""
Tuples
Tuples are very useful in Gleam as they’re the only collection data type that allows mixed types in the collection.

Python
Python tuples are immutable, fixed-size lists that can contain mixed value types. Unpacking can be used to bind a name to a specific value of the tuple.

my_tuple = (""username"", ""password"", 10)
_, password, _ = my_tuple
Gleam
let my_tuple = #(""username"", ""password"", 10)
let #(_, password, _) = my_tuple
Lists
Lists in Python are allowed to have values of mixed types, but not in Gleam.

Python
Python can emulate the cons operator of Gleam using the * operator and unpacking:

list = [2, 3, 4]
[head, *tail] = list
# head == 2
# tail == [3, 4]
Gleam
Gleam has a cons operator that works for lists destructuring and pattern matching. In Gleam lists are immutable so adding and removing elements from the start of a list is highly efficient.

let list = [2, 3, 4]
let list = [1, ..list]
let [1, second_element, ..] = list
[1.0, ..list] // compile error, type mismatch
Dictionaries
In Python, dictionaries can have keys of any type as long as:

the key type is hashable, such as integers, strings, tuples (due to their immutable values), functions… and custom mutable objects implementing the __hash__ method.
the key is unique in the dictionary. and values of any type.
In Gleam, dicts can have keys and values of any type, but all keys must be of the same type in a given dict and all values must be of the same type in a given dict.

There is no dict literal syntax in Gleam, and you cannot pattern match on a dict. Dicts are generally not used much in Gleam, custom types are more common.

Python
{""key1"": ""value1"", ""key2"": ""value2""}
{""key1"":  ""1"", ""key2"": 2}
Gleam
import gleam/dict

dict.from_list([#(""key1"", ""value1""), #(""key2"", ""value2"")])
dict.from_list([#(""key1"", ""value1""), #(""key2"", 2)]) // Type error!
Flow control
Case
Case is one of the most used control flows in Gleam. It can be seen as a switch statement on steroids. It provides a terse way to match a value type to an expression. Gleam’s case expression is fairly similar to Python’s match statement.

Python
Matching on primitive types:

def http_error(status):
    match status:
        case 400:
            return ""Bad request""
        case 404:
            return ""Not found""
        case 418:
            return ""I'm a teapot""
Matching on tuples with variable capturing:

match point:
    case (0, 0):
        print(""Origin"")
    case (0, y):
        print(f""Y={y}"")
    case (x, 0):
        print(f""X={x}"")
    case (x, y):
        print(f""""X={x}, Y={y}"""")
    case _:
        raise ValueError(""Not a point"")
Matching on type constructors:

match point:
    case Point(x=0, y=0):
        print(""Origin is the point's location."")
    case Point(x=0, y=y):
        print(f""Y={y} and the point is on the y-axis."")
    case Point(x=x, y=0):
        print(f""X={x} and the point is on the x-axis."")
    case Point():
        print(""The point is located somewhere else on the plane."")
    case _:
        print(""Not a point"")
The match expression supports guards, similar to Gleam:

match point:
    case Point(x, y) if x == y:
        print(f""The point is located on the diagonal Y=X at {x}."")
    case Point(x, y):
        print(f""Point is not on the diagonal."")
Gleam
The case operator is a top level construct in Gleam:

case some_number {
  0 -> ""Zero""
  1 -> ""One""
  2 -> ""Two""
  n -> ""Some other number"" // This matches anything
}
The case operator especially coupled with destructuring to provide native pattern matching:

case xs {
  [] -> ""This list is empty""
  [a] -> ""This list has 1 element""
  [a, b] -> ""This list has 2 elements""
  _other -> ""This list has more than 2 elements""
}
The case operator supports guards:

case xs {
  [a, b, c] if a >. b && a <=. c -> ""ok""
  _other -> ""ko""
}
and disjoint union matching:

case number {
  2 | 4 | 6 | 8 -> ""This is an even number""
  1 | 3 | 5 | 7 -> ""This is an odd number""
  _ -> ""I'm not sure""
}
Try
Error management is approached differently in Python and Gleam.

Python
Python uses the notion of exceptions to interrupt the current code flow and pop up the error to the caller.

An exception is raised using the keyword raise.

def a_function_that_fails():
    raise Exception(""an error"")
The callee block will be able to capture any exception raised in the block using a try/except set of blocks:

try:
    print(""executed"")
    a_function_that_fails()
    print(""not_executed"")
except Exception as e:
    print(""doing something with the exception"", e)

Gleam
In contrast in Gleam, errors are just containers with an associated value.

A common container to model an operation result is Result(ReturnType, ErrorType).

A Result is either:

an Error(ErrorValue)
or an Ok(Data) record
Handling errors actually means to match the return value against those two scenarios, using a case for instance:

case int.parse(""123"") {
  Error(e) -> io.println(""That wasn't an Int"")
  Ok(i) -> io.println(""We parsed the Int"")
}
In order to simplify this construct, we can use the use expression with the try function from the gleam/result module.

bind a value to the providing name if Ok(Something) is matched
interrupt the flow and return Error(Something)
let a_number = ""1""
let an_error = ""ouch""
let another_number = ""3""

use int_a_number <- try(parse_int(a_number))
use attempt_int <- try(parse_int(an_error)) // Error will be returned
use int_another_number <- try(parse_int(another_number)) // never gets executed

Ok(int_a_number + attempt_int + int_another_number) // never gets executed
Type aliases
Type aliases allow for easy referencing of arbitrary complex types. Even though their type systems does not serve the same function, both Python and Gleam provide this feature.

Python
A simple variable can store the result of a compound set of types.

type Headers = list[tuple[str, str]]

# can now be used to annotate a variable
headers: Headers = [(""Content-Type"", ""application/json"")]
Gleam
The type keyword can be used to create aliases:

pub type Headers =
  List(#(String, String))

let headers: Headers = [#(""Content-Type"", ""application/json"")]
Custom types
Records
Custom type allows you to define a collection data type with a fixed number of named fields, and the values in those fields can be of differing types.

Python
Python uses classes to define user-defined, record-like types. Properties are defined as class members and initial values are generally set in the constructor.

By default the constructor does not provide base initializers in the constructor so some boilerplate is needed:

class Person:
    name: str
    age: int

    def __init__(name: str, age: int) -> None:
        self.name = name
        self.age = age

person = Person(name=""Jake"", age=20)
# or with positional arguments Person(""Jake"", 20)
name = person.name
More recent alternatives are to use dataclasses or to leverage the NamedTuple base type to generate a constructor with initializers.

By default a class created with the dataclass decorator is mutable (although you can pass options to the dataclass decorator to change the behavior):

from dataclasses import dataclasses

@dataclass
class Person:
    name: str
    age: int

person = Person(name=""Jake"", age=20)
name = person.name
person.name = ""John""  # The name is now ""John""
NamedTuples on the other hand are immutable:

from typing import NamedTuple

class Person(NamedTuple):
    name: str
    age: int

person = Person(name=""Jake"", age=20)
name = person.name

# cannot reassign a value
person.name = ""John""  # error
Gleam
Gleam’s custom types can be used in much the same way that structs are used in Elixir. At runtime, they have a tuple representation and are compatible with Erlang records.

type Person {
  Person(name: String, age: Int)
}

let person = Person(name: ""Jake"", age: 35)
let name = person.name
An important difference to note is there is no OOP in Gleam. Methods can not be added to types.

Unions
In Python unions can be declared with the | operator.

In Gleam functions must always take and receive one type. To have a union of two different types they must be wrapped in a new custom type.

Python
def int_or_float(x: int | float) -> str:
    if isinstance(x, int):
        return f""It's an integer: {x}""
    else:
        return f""It's a float: {x}""
Gleam
type IntOrFloat {
  AnInt(Int)
  AFloat(Float)
}

fn int_or_float(x) {
  case x {
    AnInt(1) -> ""It's an integer: 1""
    AFloat(1.0) -> ""It's a float: 1.0""
  }
}
Opaque custom types
In Python, constructors cannot be marked as private. Opaque types can be imperfectly emulated using a class method and some magic property that only updates via the class factory method.

In Gleam, custom types can be defined as being opaque, which causes the constructors for the custom type not to be exported from the module. Without any constructors to import other modules can only interact with opaque types using the intended API.

Python
class OnlyCreatable:

    __create_key = object()

    @classmethod
    def create(cls, value):
        return OnlyCreatable(cls.__create_key, value)

    def __init__(self, create_key, value):
        assert(create_key == OnlyCreatable.__create_key), \
            ""OnlyCreatable objects must be created using OnlyCreatable.create""
        self.value = value
Gleam
pub opaque type Identifier {
  Identifier(Int)
}

pub fn get_id() {
  Identifier(100)
}
Modules
Python
There is no special syntax to define modules as files are modules in Python

Gleam
Gleam’s file is a module and named by the file name (and its directory path). Since there is no special syntax to create a module, there can be only one module in a file.

// in file foo.gleam
pub fn identity(x) {
  x
}
// in file main.gleam
import foo // if foo was in a folder called `lib` the import would be `lib/foo`
pub fn main() {
  foo.identity(1)
}
Imports
Python
# inside module src/nasa/moon_base.py
# imports module src/nasa/rocket_ship.py
from nasa import rocket_ship

def explore_space():
    rocket_ship.launch()
Gleam
Imports are relative to the root src folder.

Modules in the same directory will need to reference the entire path from src for the target module, even if the target module is in the same folder.

// inside module src/nasa/moon_base.gleam
// imports module src/nasa/rocket_ship.gleam
import nasa/rocket_ship

pub fn explore_space() {
  rocket_ship.launch()
}
Named imports
Python
import unix.cat as kitty
Gleam
import unix/cat as kitty
Unqualified imports
Python
from animal.cat import Cat, stroke

def main():
    kitty = Cat(name=""Nubi"")
    stroke(kitty)
Gleam
import animal/cat.{Cat, stroke}

pub fn main() {
  let kitty = Cat(name: ""Nubi"")
  stroke(kitty)
}"

13,https://towardsdatascience.com/gpt-from-scratch-with-mlx-acf2defda30e,"GPT from Scratch with MLX
Define and train GPT-2 on your MacBook
Pranav Jadhav
Towards Data Science
Pranav Jadhav

·
Follow

Published in
Towards Data Science

31 min read
·
2 days ago

Table of Contents
Preparing the data
Coding GPT-2
Input Embeddings
Positional Embeddings
Self Attention
Keys, Queries, and Values
Multi-Head Attention
MLP
Block
Layernorms and Skip Connections
Forward Pass
Sampling
Initialization
Training Loop
References
Preparing the data
Install mlx and run the following imports.

import mlx.core as mx
import mlx.nn as nn
import mlx.optimizers as optim
import mlx.utils as utils
import numpy as np
import math
The first step to training an LLM is collecting a large corpus of text data and then tokenizing it. Tokenization is the process of mapping text to integers, which can be fed into the LLM. Our training corpus for this model will be the works of Shakespeare concatenated into one file. This is roughly 1 million characters and looks like this:

First Citizen:
Before we proceed any further, hear me speak.

All:
Speak, speak.

First Citizen:
You are all resolved rather to die than to famish?

All:
Resolved. resolved.

First Citizen:
First, you know Caius Marcius is chief enemy to the people.
...
First, we read the file as a single long string into the text variable. Then we use the set() function to get all the unique characters in the text which will be our vocabulary. By printing vocab you can see all the characters in our vocabulary as one string, and we have a total of 65 characters which till be our tokens.

# Creating the vocabulary
with open('input.txt', 'r', encoding='utf-8') as f:
    text = f.read()
vocab = sorted(list(set(text)))
vocab_size = len(vocab)

print(''.join(vocab))
# !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz
print(vocab_size)
# 65
Production models will use tokenization algorithms like byte-pair encoding to generate a larger vocabulary of sub-word chunks. Since our focus today is on the architecture, we will continue with character-level tokenization. Next, we will map our vocabulary to integers known as token IDs. Then we can encode our text into tokens and decode them back to a string.

# Create mapping from vocab to integers
itos = {i:c for i,c in enumerate(vocab)} # int to string
stoi = {c:i for i,c in enumerate(vocab)} # string to int
encode = lambda x: [stoi[c] for c in x]
decode = lambda x: ''.join([itos[i] for i in x])

print(encode(""hello world""))
# [46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]
print(decode(encode(""hello world"")))
# hello world
We use theenumerate() function to iterate over all characters and their index in the vocabulary and create a dictionary itos which maps integers to characters and stoi which maps strings to integers. Then we use these mappings to create our encode and decode functions. Now we can encode the entire text and split training and validation data.

data = encode(text)
split = int(0.9 * len(data))
train_data = data[:split]
val_data = data[split:]
Currently, our training data is just a very long string of tokens. However, we are trying to train our model to predict the next token some given previous tokens. Therefore our dataset should be comprised of examples where the input is some string of tokens and the label is the correct next token. We need to define a model parameter called context length which is the maximum number of tokens used to predict the next token. Our training examples will be the length of our context length.

Let’s look at the first ctx_len+1 tokens.

ctx_len = 8
print(train_data[:ctx_len + 1])
# [18, 47, 56, 57, 58,  1, 15, 47, 58]
# x: [18, 47, 56, 57, 58,  1, 15, 47] | y: 58
This is one training example where the input is “18, 47, 56, 57, 58, 1, 15, 47” and the desired output is “58”. This is 8 tokens of context. However, we also want to train the model to predict the next token given only 7, 6, 5 … 0 tokens as context which is needed during generation. Therefore we also consider the 8 sub examples packed into this example:

ctx_len = 8
print(train_data[:ctx_len + 1])
# [18, 47, 56, 57, 58,  1, 15, 47, 58]
# 8 sub examples
# [18] --> 47
# [18, 47] --> 56
# [18, 47, 56] --> 57
# [18, 47, 56, 57] --> 58
# [18, 47, 56, 57, 58] --> 1
# [18, 47, 56, 57, 58, 1] --> 15
# [18, 47, 56, 57, 58, 1, 15] --> 47
# [18, 47, 56, 57, 58, 1, 15, 47] --> 58
Notice that the labels are simply the inputs shifted left.

print(""inputs: "", train_data[:ctx_len])
print(""labels: "", train_data[1:ctx_len+1]) # labels = inputs indexed 1 higher
# inputs: [18, 47, 56, 57, 58,  1, 15, 47]
# labels: [47, 56, 57, 58,  1, 15, 47, 58]
At index 0 the input is 18 and the label is 47. At index 1 the input is everything before and including index 1 which is [18, 47] and the label is 56, etc. Now that we understand that the labels are simply the input sequence indexed one higher we can build our datasets.

# Creating training and validation datasets
ctx_len = 8
X_train = mx.array([train_data[i:i+ctx_len] for i in range(0, len(train_data) - ctx_len, ctx_len)])
y_train = mx.array([train_data[i+1:i+ctx_len+1] for i in range(0, len(train_data) - ctx_len, ctx_len)])
X_val = mx.array([val_data[i:i+ctx_len] for i in range(0, len(val_data) - ctx_len, ctx_len)])
y_val = mx.array([val_data[i+1:i+ctx_len+1] for i in range(0, len(val_data) - ctx_len, ctx_len)])
We loop through the data and take chunks of size ctx_len as the inputs (X) and then take the same chunks but at 1 higher index as the labels (y). Then we take these Python lists and create mlx array objects from them. The model internals will be written with mlx so we want our inputs to be mlx arrays.

One more thing. During training we don’t want to feed the model one example at a time, we want to feed it multiple examples in parallel for efficiency. This group of examples is called our batch, and the number of examples in a group is our batch size. Thus we define a function to generate batches for training.

def get_batches(X, y, b_size, shuffle=True):
    if shuffle:
        ix = np.arange(X.shape[0])
        np.random.shuffle(ix)
        ix = mx.array(ix)
        X = X[ix]
        y = y[ix]
    for i in range(0, X.shape[0], b_size):
        input = X[i:i+b_size]
        label = y[i:i+b_size]
        yield input, label
If shuffle=True, we shuffle the data by indexing it with a randomly shuffled index. Then we loop through our dataset and return batch-size chunks from input and label datasets. These chunks are known as mini-batches and are just stacked examples that we process in parallel. These mini-batches will be our input to the model during training.

Here’s an example of a minibatch of 4 examples with context length 8.


A single minibatch (image by author)
This minibatch packs 32 next-token prediction problems. The model will predict the next token for each token in the input and the labels will be used to calculate the loss. Notice that the labels contain the next token for each index of the inputs.

You’ll want to keep this picture in your mind because the shapes of these tensors will get hairy. For now, just remember that we will input a tensor of shape (batch_size, ctx_len) to the model.

Coding GPT-2
Let’s look at the GPT-2 architecture to get an overview of what we are trying to implement.


GPT-2 Architecture (image by author)
Don’t worry if this looks confusing. We will implement it step by step from bottom to top. Let’s start by implementing the input embeddings.

Input Embeddings
The purpose of the input embedding layer is to map token IDs to vectors. Each token will be mapped to a vector which will be its representation as it is forwarded through the model. The vectors for each token will accumulate and exchange information as they pass through the model and eventually be used to predict the next token. These vectors are called embeddings.

The simplest way to map token IDs to vectors is through a lookup table. We create a matrix of size (vocab_size, n_emb) where each row is the embedding vector for the corresponding token. This matrix is known as the embedding weights.


Embedding Layer (image by author)
The diagram shows an example embedding layer of size (65, 6). This means there are 65 tokens in the vocabulary and each one will be represented by a length 6 embedding vector. The inputted sequence will be used to index the embedding weights to get the vector corresponding to each token. Remember the minibatches we input into the model? Originally the minibatch is size (batch_size, ctx_len). After passing through the embedding layer it is size (batch_size, ctx_len, n_emb). Instead of each token being a single integer, each token is now a vector of length n_emb.

Let’s define the embedding layer in code now.

n_emb = 6 # You can add these hyperparams at the top of your file
class GPT(nn.Module):
    def __init__(self):
        super().__init__()
        self.wte = nn.Embedding(vocab_size, n_emb)
We will define a class to organize our implementation. We subclass nn.Module to take advantage of mlx’s features. Then in the init function, we call the superclass constructor and initialize our token embedding layer called wte .

Positional Embeddings
Next up is the positional embeddings. The purpose of positional embeddings is to encode information about the position of each token in the sequence. This can be added to our input embeddings to get a complete representation of each token that contains information about the token’s position in the sequence.

class GPT(nn.Module):
    def __init__(self):
        super().__init__()
        self.wte = nn.Embedding(vocab_size, n_emb) # token embeddings
        self.wpe = nn.Embedding(ctx_len, n_emb) # position embeddings
The position embeddings work the same as token embeddings, except instead of having a row for each token we have a row for each possible position index. This means our embedding weights will be of shape (ctx_len, n_emb). Now we implement the __call__ function in our GPT class. This function will contain the forward pass of the model.

# Tensor shapes commented
def __call__(self, x):
    B, T = x.shape # (B = batch_size, T = ctx_len)
    tok_emb = self.wte(x) # (B, T, n_emb)
    pos_emb = self.wpe(mx.arange(T)) # (T, n_emb)
    x = tok_emb + pos_emb # (B, T, n_emb)
First, we break out the dimensions of our input into variables B and T for easy handling. In sequence modeling contexts B and T are usually used as shorthand for “batch” and “time” dimensions. In this case, the “time” dimension of our sequence is the context length.

Next, we calculate token and position embeddings. Notice that for the position embeddings, our input is mx.arange(T) . This will output an array of consecutive integers from 0 to T-1 which is exactly what we want because those are the positions we want to embed. After passing that through the embedding layer we will have a tensor of shape (T, n_emb) because the embedding layer plucks out the n_emb length vector for each of the T positions. Note that even though pos_emb is not the same shape as tok_emb we can add the two because mlx will broadcast, or replicate pos_emb across the batch dimension to allow elementwise addition. Finally, we perform the addition to get the new representations of the tokens with positional information.

Self-Attention
So far the representation vectors for each token have been calculated independently. They have not had the opportunity to exchange any information. This is intuitively bad in language modeling because the meaning and usage of words depend on the surrounding context. Self-attention is how we incorporate information from previous tokens into a given token.

First, let’s consider a naive approach. What if we simply represented each token as the average of its representation vector and the vectors of all the tokens before it? This achieves our goal of packing information from previous tokens into the representation for a given token. Here’s what it would look like.


image by author
But self-attention doesn’t involve writing a for-loop. The key insight is we can achieve this previous token averaging with matrix multiplication!


image by author
By multiplying our input sequence on the left by a special matrix we get the desired result. This matrix is known as the attention weights. Notice that each row of the attention weight matrix specificies “how much” of each other token goes into the representation for any given token. For example in row two, we have [0.5, 0.5, 0, 0]. This means that row two of the result will be 0.5*token1 + 0.5*token2 + 0*token3 + 0*token4 , or the average of token1 and token2. Note that the attention weights are a lower-triangular matrix (zeros in upper right entries). This ensures that future tokens will not be included in the representation of a given token. This ensures that tokens can only communicate with the previous tokens because during generation the model will only have access to previous tokens.

Let’s look at how we can construct the attention weight matrix.


image by author
Notice that if we create an array of zeros with -inf in the upper right entries and then perform row-wise softmax we get the desired attention weights. A good exercise is to step through the softmax calculation for a row to see how this works. The takeaway is that we can take some array of size (ctx_len, ctx_len) and softmax each row to get attention weights that sum to one.

Now we can leave the realm of naive self-attention. Instead of simply averaging previous tokens, we use arbitrary weighted sums over previous tokens. Notice what happens when we do row-wise softmax of an arbitrary matrix.


image by author
We still get weights that sum to one on each row. During training, we can learn the numbers in the matrix on the left which will specify how much each token goes into the representation for another token. This is how tokens pay “attention” to each other. But we still haven’t understood where this matrix on the left came from. These pre-softmax attention weights are calculated from the tokens themselves, but indirectly through three linear projections.

Keys, Queries, and Values

image by author
Each token in our sequence emits 3 new vectors. These vectors are called keys, queries, and values. We use the dot product of the query vector of one token and the key vector of another token to quantify the “affinity” those two tokens have. We want to calculate the pairwise affinities of each token with every other token, therefore we multiply the query vector (4x3) with the key vector transposed (3x4) to get the raw attention weights (4x4). Due to the way matrix multiplication works the (i,j) entry in the raw attention weights will be the query of token i dot the key of token j or the “affinity” between the two. Thus we have calculated interactions between every token. However, we don’t want past tokens interacting with future tokens so we apply a mask of -inf to the upper right entries to ensure they will zero out after softmax. Then we perform row-wise softmax to get the final attention weights. Instead of multiplying these weights directly with the input, we multiply them with the value projection. This results in the new representations.

Now that we understand attention conceptually, let’s implement it.

class Attention(nn.Module):
    def __init__(self, head_size):
        super().__init__()
        self.head_size = head_size
        self.k_proj = nn.Linear(n_emb, head_size, bias=False)
        self.q_proj = nn.Linear(n_emb, head_size, bias=False)
        self.v_proj = nn.Linear(n_emb, head_size, bias=False)

We start by defining the key, query, and value projection layers. Note that instead of going from n_emb to n_emb, we project from n_emb to head_size. This doesn’t change anything, it just means the new representations calculated by attention will be dimension head_size.

class Attention(nn.Module):
    def __init__(self, head_size):
        super().__init__()
        self.head_size = head_size
        self.k_proj = nn.Linear(n_emb, head_size, bias=False)
        self.q_proj = nn.Linear(n_emb, head_size, bias=False)
        self.v_proj = nn.Linear(n_emb, head_size, bias=False)
    def __call__(self, x): # shapes commented
        B, T, C = x.shape # (batch_size, ctx_len, n_emb)
        K = self.k_proj(x) # (B, T, head_size)
        Q = self.q_proj(x) # (B, T, head_size)
        V = self.v_proj(x) # (B, T, head_size)
The forward pass begins by calculating the key, query, and value projections. We also break out the input shape into the variables B, T, and C for future convenience.

class Attention(nn.Module):
    def __init__(self, head_size):
        super().__init__()
        self.head_size = head_size
        self.k_proj = nn.Linear(n_emb, head_size, bias=False)
        self.q_proj = nn.Linear(n_emb, head_size, bias=False)
        self.v_proj = nn.Linear(n_emb, head_size, bias=False)
    def __call__(self, x):
        B, T, C = x.shape # (batch_size, ctx_len, n_emb)
        K = self.k_proj(x) # (B, T, head_size)
        Q = self.q_proj(x) # (B, T, head_size)
        V = self.v_proj(x) # (B, T, head_size)
        attn_weights = (Q @ K.transpose([0, 2, 1])) / math.sqrt(self.head_size)
        # attn_weights.shape = (B, T, T)
Next, we calculate the attention weights. We only want to transpose the last two dimensions of the key tensor, because the batch dimension is just there so we can forward multiple training examples in parallel. The mlx transpose function expects the new order of the dimensions as input, so we pass it [0, 2, 1] to transpose the last two dimensions. One more thing: we scale the attention weights by the inverse square root of head_size. This is known as scaled attention and the purpose is to ensure that when Q and K are unit variance, attn_weights will be unit variance. If the variance of attn_weights is high, then the softmax will map these small and large values to 0 or 1which results in less complex representations.

The next step is to apply the mask to ensure we are doing causal language modeling i.e. ensuring tokens cannot attend to future tokens.

class Attention(nn.Module):
    def __init__(self, head_size):
        super().__init__()
        self.head_size = head_size
        self.k_proj = nn.Linear(n_emb, head_size, bias=False)
        self.q_proj = nn.Linear(n_emb, head_size, bias=False)
        self.v_proj = nn.Linear(n_emb, head_size, bias=False)
        indices = mx.arange(ctx_len)
        mask = indices[:, None] < indices[None] # broadcasting trick
        self._causal_mask = mask * -1e9
    def __call__(self, x):
        B, T, C = x.shape # (batch_size, ctx_len, n_emb)
        K = self.k_proj(x) # (B, T, head_size)
        Q = self.q_proj(x) # (B, T, head_size)
        V = self.v_proj(x) # (B, T, head_size)
        attn_weights = (Q @ K.transpose([0, 2, 1])) / math.sqrt(self.head_size)
        # attn_weights.shape = (B, T, T)
We create the mask with a clever broadcasting trick. Let’s say our ctx_len=4 like in the diagrams above. First, we use mx.arange(4) to set the indices variable to [0, 1, 2, 3].


image by author
Then we can index like so indices[:, None] to generate a column vector with the values of indices. Similarly, we can get a row vector using indices[None]. Then when we do the < comparison, mlx broadcasts the vectors because they have mismatching shapes so they can’t be compared elementwise. Broadcasting means mlx will replicate the vectors along the lacking dimension. This results in an elementwise comparison of two (4, 4) matrices which makes sense. Side note: I recommend familiarizing yourself with the details of broadcasting by reading this, it comes up all the time when dealing with tensors.

After the elementwise comparison, we are left with the following tensor:

[[False,  True,  True,  True],
 [False, False,  True,  True],
 [False, False, False,  True],
 [False, False, False, False]]
Multiplying this tensor by -1e9, we get:

[[-0e+00, -1e+09, -1e+09, -1e+09],
 [-0e+00, -0e+00, -1e+09, -1e+09],
 [-0e+00, -0e+00, -0e+00, -1e+09],
 [-0e+00, -0e+00, -0e+00, -0e+00]]
Now we have an additive mask. We can add this matrix to our attention weights to make all the upper right entries very large negative numbers. This will cause them to be zeroed out after the softmax operation. Also, note that we add “_” as a prefix to the attribute name _causal_mask which marks it as a private variable. This signals to mlx that it is not a parameter and should not be updated during training.

class Attention(nn.Module):
    def __init__(self, head_size):
        super().__init__()
        self.head_size = head_size
        self.k_proj = nn.Linear(n_emb, head_size, bias=False)
        self.q_proj = nn.Linear(n_emb, head_size, bias=False)
        self.v_proj = nn.Linear(n_emb, head_size, bias=False)
        indices = mx.arange(ctx_len)
        mask = indices[:, None] < indices[None] # broadcasting trick
        self._causal_mask = mask * -1e9
    def __call__(self, x):
        B, T, C = x.shape # (batch_size, ctx_len, n_emb)
        K = self.k_proj(x) # (B, T, head_size)
        Q = self.q_proj(x) # (B, T, head_size)
        V = self.v_proj(x) # (B, T, head_size)
        attn_weights = (Q @ K.transpose([0, 2, 1])) / math.sqrt(self.head_size)
        # attn_weights.shape = (B, T, T)
        attn_weights = attn_weights + self._causal_mask
        attn_weights = mx.softmax(attn_weights, axis=-1)
        o = (attn_weights @ V) # (B, T, head_size)
Now we can softmax row-wise to get the final attention weights and multiply these weights by the values to get our output. Note we pass axis=-1 to softmax which specifies that we want to softmax across the last dimension which are the rows.

The final step is output linear projection and dropout.

dropout = 0.1 # add this with hyperparams at top of file
class Attention(nn.Module):
    def __init__(self, head_size):
        super().__init__()
        self.head_size = head_size
        self.k_proj = nn.Linear(n_emb, head_size, bias=False)
        self.q_proj = nn.Linear(n_emb, head_size, bias=False)
        self.v_proj = nn.Linear(n_emb, head_size, bias=False)
        indices = mx.arange(ctx_len)
        mask = indices[:, None] < indices[None] # broadcasting trick
        self._causal_mask = mask * -1e9
        self.c_proj = nn.Linear(head_size, n_emb) # output projection
        self.resid_dropout = nn.Dropout(dropout)
    def __call__(self, x):
        B, T, C = x.shape # (batch_size, ctx_len, n_emb)
        K = self.k_proj(x) # (B, T, head_size)
        Q = self.q_proj(x) # (B, T, head_size)
        V = self.v_proj(x) # (B, T, head_size)
        attn_weights = (Q @ K.transpose([0, 2, 1])) / math.sqrt(self.head_size)
        # attn_weights.shape = (B, T, T)
        attn_weights = attn_weights + self._causal_mask
        attn_weights = mx.softmax(attn_weights, axis=-1)
        o = (attn_weights @ V) # (B, T, head_size)
        o = self.c_proj(self.resid_dropout(o))
        return o
We add two new layers, c_proj and resid_dropout which are the output projection and residual dropout. The output projection is to return the vectors to their original dimension n_emb. The dropout is added for regularization and training stability which is important as we start layering the transformer blocks to get a deep network. And that’s it for implementing one attention head!

Multi-Head Attention
Instead of having just one attention head LLMs often use multiple attention heads in parallel and concatenate their outputs to create the final representation. For example, let’s say we had one attention head with head_size=64 so the vector it produced for each token was 64 dimensional. We could achieve the same thing with 4 parallel attention heads each with head_size=16 by concatenating their outputs to produce a 16x4 = 64 dimensional output. Multi-head attention allows the model to learn more complex representations because each head learns different projections and attention weights.

n_heads = 4
class MultiHeadAttention(nn.Module): # naive implementation
    def __init__(self):
        super().__init__()
        self.heads = [Attention(head_size // n_heads) for _ in range(n_heads)]
    def __call__(self, x):
        return mx.concatenate([head(x) for head in self.heads], axis=-1)
The straightforward implementation is to create a list of n_heads attention heads where each one has size equal to our final head size divided by n_heads. Then we concatenate the output of each head over the last axis. However, this implementation is inefficient and does not take advantage of the speed of tensors. Let’s implement multi-head attention with the power of tensors.

head_size = 64 # put at top of file
class MultiHeadAttention(nn.Module):
    def __init__(self):
        super().__init__()
        self.k_proj = nn.Linear(n_emb, head_size, bias=False)
        self.q_proj = nn.Linear(n_emb, head_size, bias=False)
        self.v_proj = nn.Linear(n_emb, head_size, bias=False)
        indices = mx.arange(ctx_len)
        mask = indices[:, None] < indices[None] # broadcasting trick
        self._causal_mask = mask * -1e9
        self.c_proj = nn.Linear(head_size, n_emb) # output projection
        self.resid_dropout = nn.Dropout(dropout)
    def __call__(self, x):
        B, T, C = x.shape # (batch_size, ctx_len, n_emb)
        K = self.k_proj(x) # (B, T, head_size)
        Q = self.q_proj(x) # (B, T, head_size)
        V = self.v_proj(x) # (B, T, head_size)
We start with our single-head attention implementation. The __init__() function has not changed. The forward pass begins as normal with the creation of the key, query, and value projections.

head_size = 64 # put at top of file
n_heads = 8 # put at top of file
class MultiHeadAttention(nn.Module):
    def __init__(self):
        super().__init__()
        self.k_proj = nn.Linear(n_emb, head_size, bias=False)
        self.q_proj = nn.Linear(n_emb, head_size, bias=False)
        self.v_proj = nn.Linear(n_emb, head_size, bias=False)
        indices = mx.arange(ctx_len)
        mask = indices[:, None] < indices[None] # broadcasting trick
        self._causal_mask = mask * -1e9
        self.c_proj = nn.Linear(head_size, n_emb) # output projection
        self.resid_dropout = nn.Dropout(dropout)
    def __call__(self, x):
        B, T, C = x.shape # (batch_size, ctx_len, n_emb)
        K = self.k_proj(x) # (B, T, head_size)
        Q = self.q_proj(x) # (B, T, head_size)
        V = self.v_proj(x) # (B, T, head_size)
        mha_shape = (B, T, n_heads, head_size//n_heads)
        K = mx.as_strided(K, (mha_shape)) # (B, T, n_heads, head_size//n_heads)
        Q = mx.as_strided(Q, (mha_shape)) # (B, T, n_heads, head_size//n_heads)
        V = mx.as_strided(V, (mha_shape)) # (B, T, n_heads, head_size//n_heads)
The next thing we need to do is introduce a new dimension for the number of heads n_heads . In the naive implementation, we had separate attention objects each with their own key, query, and value tensors but now we have them all in one tensor, therefore we need a dimension for the heads. We define the new shape we want in mha_shape . Then we use mx.as_strided() to reshape each tensor to have the head dimension. This function is equivalent to view from pytorch and tells mlx to treat this array as a different shape. But we still have a problem. Notice that we if try to multiply Q @ K_t (where K_t is K transposed over it’s last 2 dims) to compute attention weights as we did before, we will be multiplying the following shapes:

(B, T, n_heads, head_size//n_heads) @ (B, T, head_size//n_heads, n_heads)
Result shape: (B, T, n_heads, n_heads)
This would result in a tensor of shape (B, T, n_heads, n_heads) which is incorrect. With one head our attention weights were shape (B, T, T) which makes sense because it gives us the interaction between each pair of tokens. So now our shape should be the same but with a heads dimension: (B, n_heads, T, T) . We achieve this by transposing the dimensions of keys, queries, and values after we reshape them to make n_heads dimension 1 instead of 2.

head_size = 64 # put at top of file
n_heads = 8 # put at top of file
class MultiHeadAttention(nn.Module):
    def __init__(self):
        super().__init__()
        self.k_proj = nn.Linear(n_emb, head_size, bias=False)
        self.q_proj = nn.Linear(n_emb, head_size, bias=False)
        self.v_proj = nn.Linear(n_emb, head_size, bias=False)
        indices = mx.arange(ctx_len)
        mask = indices[:, None] < indices[None] # broadcasting trick
        self._causal_mask = mask * -1e9
        self.c_proj = nn.Linear(head_size, n_emb) # output projection
        self.attn_dropout = nn.Dropout(dropout)
        self.resid_dropout = nn.Dropout(dropout)
    def __call__(self, x):
        B, T, C = x.shape # (batch_size, ctx_len, n_emb)
        K = self.k_proj(x) # (B, T, head_size)
        Q = self.q_proj(x) # (B, T, head_size)
        V = self.v_proj(x) # (B, T, head_size)
        mha_shape = (B, T, n_heads, head_size//n_heads)
        K = mx.as_strided(K, (mha_shape)).transpose([0, 2, 1, 3]) # (B, n_heads, T, head_size//n_heads)
        Q = mx.as_strided(Q, (mha_shape)).transpose([0, 2, 1, 3]) # (B, n_heads, T, head_size//n_heads)
        V = mx.as_strided(V, (mha_shape)).transpose([0, 2, 1, 3]) # (B, n_heads, T, head_size//n_heads)
        attn_weights = (Q @ K.transpose([0, 1, 3, 2])) / math.sqrt(Q.shape[-1]) # (B, n_heads, T, T)
        attn_weights = attn_weights + self._causal_mask[:T, :T]
        attn_weights = mx.softmax(attn_weights, axis=-1)
        attn_weights = self.attn_dropout(attn_weights)
        o = (attn_weights @ V) # (B, n_heads, T, head_size//n_heads)

Now we can calculate the correction attention weights. Notice that we scale the attention weights by the size of an individual attention head rather than head_size which would be the size after concatenation. We also apply dropout to the attention weights.

Finally, we perform the concatenation and apply the output projection and dropout.

head_size = 64 # put at top of file
n_heads = 8 # put at top of file
class MultiHeadAttention(nn.Module):
    def __init__(self):
        super().__init__()
        self.k_proj = nn.Linear(n_emb, head_size, bias=False)
        self.q_proj = nn.Linear(n_emb, head_size, bias=False)
        self.v_proj = nn.Linear(n_emb, head_size, bias=False)
        indices = mx.arange(ctx_len)
        mask = indices[:, None] < indices[None] # broadcasting trick
        self._causal_mask = mask * -1e9
        self.c_proj = nn.Linear(head_size, n_emb) # output projection
        self.attn_dropout = nn.Dropout(dropout)
        self.resid_dropout = nn.Dropout(dropout)
    def __call__(self, x):
        B, T, C = x.shape # (batch_size, ctx_len, n_emb)
        K = self.k_proj(x) # (B, T, head_size)
        Q = self.q_proj(x) # (B, T, head_size)
        V = self.v_proj(x) # (B, T, head_size)
        mha_shape = (B, T, n_heads, head_size//n_heads)
        K = mx.as_strided(K, (mha_shape)).transpose([0, 2, 1, 3]) # (B, n_heads, T, head_size//n_heads)
        Q = mx.as_strided(Q, (mha_shape)).transpose([0, 2, 1, 3]) # (B, n_heads, T, head_size//n_heads)
        V = mx.as_strided(V, (mha_shape)).transpose([0, 2, 1, 3]) # (B, n_heads, T, head_size//n_heads)
        attn_weights = (Q @ K.transpose([0, 1, 3, 2])) / math.sqrt(Q.shape[-1]) # (B, n_heads, T, T)
        attn_weights = attn_weights + self._causal_mask[:T, :T]
        attn_weights = mx.softmax(attn_weights, axis=-1)
        attn_weights = self.attn_dropout(attn_weights)
        o = (attn_weights @ V) # (B, n_heads, T, head_size//n_heads)
        o = o.transpose([0, 2, 1, 3]).reshape((B, T, head_size)) # concat heads
        o = self.c_proj(self.resid_dropout(o))
        return o
Since we have everything in one tensor, we can do some shape manipulation to do the concatenation. First, we move n_heads back to the second to last dimension with the transpose function. Then we reshape back to the original size to undo the splitting into heads we performed earlier. This is the same as concatenating the final vectors from each head. And that’s it for multi-head attention! We’ve gotten through the most intense part of our implementation.

MLP
The next part of the architecture is the multilayer perception or MLP. This is a fancy way of saying 2 stacked linear layers. There’s not much to be said here, it is a standard neural network.

class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.c_fc = nn.Linear(n_emb, 4 * n_emb)
        self.gelu = nn.GELU()
        self.c_proj = nn.Linear(4 * n_emb, n_emb)
        self.dropout = nn.Dropout(dropout)
    def __call__(self, x):
        x = self.gelu(self.c_fc(x))
        x = self.c_proj(x)
        x = self.dropout(x)
        return x
We take the input and project it to a higher dimension with c_fc . Then we apply gelu nonlinearity and project it back down to the embedding dimension with c_proj . Finally, we apply dropout and return. The purpose of the MLP is to allow for some computation after the vectors have communicated during attention. We will stack these communication layers (attention) and computation layers (mlp) into a block.

Block
A GPT block consists of attention followed by an MLP. These blocks will be repeated to make the architecture deep.

class Block(nn.Module):
    def __init__(self):
        super().__init__()
        self.mlp = MLP()
        self.mha = MultiHeadAttention()
    def __call__(self, x):
        x = self.mha(x)
        x = self.mlp(x)
        return x
Now, we need to add two more features to improve training stability. Let’s take a look at the architecture diagram again.

Layernorms and Skip Connections

image by author
We still need to implement the components highlighted in red. The arrows are skip connections. Instead of the input being transformed directly, the effect of the attention and MLP layers is additive. Their result is added to the input instead of directly replacing it. This is good for the training stability of deep networks since in the backward pass, the operands of an addition operation will receive the same gradient as their sum. Gradients can thus flow backwards freely which prevents issues like vanishing/exploding gradients that plague deep networks. Layernorm also helps with training stability by ensuring activations are normally distributed. Here is the final implementation.

class Block(nn.Module):
    def __init__(self):
        super().__init__()
        self.mlp = MLP()
        self.mha = MultiHeadAttention()
        self.ln_1 = nn.LayerNorm(dims=n_emb)
        self.ln_2 = nn.LayerNorm(dims=n_emb)
    def __call__(self, x):
        x = x + self.mha(self.ln_1(x))
        x = x + self.mlp(self.ln_2(x))
        return x
Layernorm is applied before multi-head attention and MLP. The skip connections are added with x = x + ... making the operations additive.

Forward Pass
With the Block defined, we can finish the full GPT-2 forward pass.

n_layers = 3 # put at top of file
class GPT(nn.Module):
    def __init__(self):
        super().__init__()
        self.wte = nn.Embedding(vocab_size, n_emb) # token embeddings
        self.wpe = nn.Embedding(ctx_len, n_emb) # position embeddings
        self.blocks = nn.Sequential(
            *[Block() for _ in range(n_layers)],
        ) # transformer blocks
        self.ln_f = nn.LayerNorm(dims=n_emb) # final layernorm
        self.lm_head = nn.Linear(n_emb, vocab_size) # output projection
    # Tensor shapes commented
    def __call__(self, x):
        B, T = x.shape # (B = batch_size, T = ctx_len)
        tok_emb = self.wte(x) # (B, T, n_emb)
        pos_emb = self.wpe(mx.arange(T)) # (T, n_emb)
        x = tok_emb + pos_emb # (B, T, n_emb)
        x = self.blocks(x) # (B, T, n_emb)
        x = self.ln_f(x) # (B, T, b_emb)
        logits = self.lm_head(x) # (B, T, vocab_size)
        return logits
We create a container for the blocks using nn.Sequential which takes any input and passes it sequentially through the contained layers. Then we can apply all the blocks with self.blocks(x) . Finally, we apply a layer norm and then the lm_head. The lm_head or language modeling head is just a linear layer that maps from the embedding dimension to the vocab size. The model will output a vector containing some value for each word in our vocabulary, or the logits. We can softmax the logits to get a probability distribution over the vocabulary which we can sample from to get the next token. We will also use the logits to calculate the loss during training. There are just two more things we need to implement before we begin training.

Sampling
We need to write a generate function to sample from the model once training is complete. The idea is that we start with some sequence of our choice, then we predict the next token and append this to our sequence. Then we feed the new sequence in and predict the next token again. This continues until we decide to stop.

# method of GPT class
def generate(self, max_new_tokens):
  ctx = mx.zeros((1, 1), dtype=mx.int32)
We prompt the model with a single token, zero. Zero is the newline character so it is a natural place to start the generation since we just want to see how Shakespeare-like our model can get. Note that we initialize the shape to (1, 1) to simulate a single batch with a sequence length of one.

# method of GPT class
def generate(self, max_new_tokens):
  ctx = mx.zeros((1, 1), dtype=mx.int32)
  for _ in range(max_new_tokens):
    logits = self(ctx[:, -ctx_len:]) # pass in last ctx_len characters
    logits = logits[:, -1, :] # get logits for the next token
    next_tok = mx.random.categorical(logits, num_samples=1)
    ctx = mx.concatenate((ctx, next_tok), axis=1)
return ctx
Then we get the logits for the next token by passing in the last ctx_len characters to the model. However, our model output is of shape (B, T, vocab_size) since it predicts the next token logits for each token in the input. We use all of that during training, but now we only want the logits for the last token because we can use this to sample a new token. Therefore we index the logits to get the last element in the first dimension which is the sequence dimension. Then we sample the next token using the mx.random.categorical() function which takes the logits and the number of samples we want as input. This function will softmax the logits to turn them into a probability distribution and then randomly sample a token according to the probabilities. Finally, we concatenate the new token to the context and repeat the process max_new_tokens number of times.

Initialization
The last thing to do is handle weight initialization which is important for training dynamics.

# method of GPT
def _init_parameters(self):
    normal_init = nn.init.normal(mean=0.0, std=0.02)
    residual_init = nn.init.normal(mean=0.0, std=(0.02 / math.sqrt(2 * n_layers)))
First, we define two different nn.init.normal functions. The first one is for initializing all linear and embedding layers. The second one is for initializing linear layers that are specifically residual projections i.e. the last linear layer inside multi-head attention and MLP. The reason for this special initialization is that it checks accumulation along the residual path as model depth increases according to the GPT-2 paper [2].

In mlx we can change the parameters of the model using the mx.update() function. Checking the docs, it expects a complete or partial dictionary of the new model parameters. We can see what this dictionary looks like by printing out self.parameters() inside the GPT class.

{'wte': {'weight': array([[-0.025084, -0.0197523, -0.0341617, ..., -0.0979123, -0.0830218, -0.0784692],
       [-0.00777913, -0.117002, -0.0310708, ..., 0.0128591, 0.122941, 0.000414443],
       [0.0240044, -0.0859084, 0.0253116, ..., 0.108967, 0.0767123, 0.0221565],
       ...,
       [0.050729, -0.04578, 0.0685943, ..., -0.0496998, -0.00350879, -0.00631825],
       [0.00518804, 0.0499818, 0.0330045, ..., 0.0300661, 0.0431054, 0.000958906],
       [-0.0323007, 0.0132046, 0.0208218, ..., -0.0785159, 0.00436121, -0.00726994]], dtype=float32)}, 'wpe': {'weight': array([[0.000797923, -0.0396898, -0.029047, ..., -0.0132273, 0.00684483, -0.0067624],
       [-0.0247021, -0.0274349, 0.0310587, ..., -0.100099, 0.0301566, -0.0178732],
       [0.0929172, -0.0468649, 0.0101506, ..., -0.0341086, -0.0516283, 0.0447596],
       ...,
       [-0.0508172, 0.0892201, -0.00183612, ..., -0.00341944, 0.023437, 0.0296461],
       [0.0105829, 0.0688093, 0.146744, ..., -0.0836337, 0.0206679, 0.0184166],
       [-0.00578717, -0.0606196, -0.0917056, ..., -0.0641549, -0.0490424, 0.0998114]], dtype=float32)}, 'blocks': {'layers': [{'mlp': {'c_fc': {'weight': array([[0.0169199, 0.00264431, 0.0316978, ..., -0.0596867, -0.0153549, 0.0176386],
       ...
It’s a nested dictionary containing each model weight as an mx.array. So to initialize the parameters of our model we need to build up a dictionary like this with our new params and pass them to self.update() . We can achieve this as follows:

# method of GPT
def _init_parameters(self):
    normal_init = nn.init.normal(mean=0.0, std=0.02)
    residual_init = nn.init.normal(mean=0.0, std=(0.02 / math.sqrt(2 * n_layers)))
    new_params = []
    for name, module in self.named_modules():
        if isinstance(module, nn.layers.linear.Linear):
            new_params.append((name + '.weight', normal_init(module.weight)))
        elif isinstance(module, nn.layers.embedding.Embedding):
            new_params.append((name + '.weight', normal_init(module.weight)
We maintain a list of tuples called new_params which will contain tuples of (parameter_name, new_value). Next, we loop through each nn.Module object in our model with self.named_modules() which returns tuples of (name, module). If we print out the module names within the loop we see that they look like this:

lm_head
blocks
blocks.layers.4
blocks.layers.3
blocks.layers.3.ln_2
blocks.layers.3.ln_1
blocks.layers.3.mha
blocks.layers.3.mha.resid_dropout
blocks.layers.3.mha.c_proj
blocks.layers.3.mha.attn_dropout
blocks.layers.3.mha.c_attn
...
blocks.layers.0.mlp.dropout
blocks.layers.0.mlp.c_proj
blocks.layers.0.mlp.gelu
blocks.layers.0.mlp.c_fc
wpe
wte
We use the isinstance() function to find the linear and embedding layers and then add them to our list. For example, say we are looping and reach “blocks.layers.0.mlp.c_fc” which is the first linear layer in the MLP. This would trigger the first if statement, and the tuple (""block.layers.0.mlp.c_fc.weight"", [<normally initialized weight here>]) would be added to our list. We have to add “.weight” to the name because we specifically want to initialize the weight in this way, not the bias. Now we need to handle the residual projection initialization.

# method of GPT
def _init_parameters(self):
    normal_init = nn.init.normal(mean=0.0, std=0.02)
    residual_init = nn.init.normal(mean=0.0, std=(0.02 / math.sqrt(2 * n_layers)))
    new_params = []
    for name, module in self.named_modules():
        if isinstance(module, nn.layers.linear.Linear):
            if 'c_proj' in name: # residual projection
                new_params.append((name + '.weight', residual_init(module.weight)))
            else:
                new_params.append((name + '.weight', normal_init(module.weight)))
        elif isinstance(module, nn.layers.embedding.Embedding):
            new_params.append((name + '.weight', normal_init(module.weight)))
After checking if the module is a linear layer, we check if “c_proj” is in the name because that’s how we named the residual projections. Then we can apply the special initialization. Finally, we need to initialize the biases to be zero.

# method of GPT
def _init_parameters(self):
    normal_init = nn.init.normal(mean=0.0, std=0.02)
    residual_init = nn.init.normal(mean=0.0, std=(0.02 / math.sqrt(2 * n_layers)))
    new_params = []
    for name, module in self.named_modules():
        if isinstance(module, nn.layers.linear.Linear):
            if 'c_proj' in name:
                new_params.append((name + '.weight', residual_init(module.weight)))
            else:
                new_params.append((name + '.weight', normal_init(module.weight)))
            if 'bias' in module:
                new_params.append((name + '.bias', mx.zeros(module.bias.shape)))
        elif isinstance(module, nn.layers.embedding.Embedding):
            new_params.append((name + '.weight', normal_init(module.weight)))
    self = self.update(utils.tree_unflatten(new_params))
We add another if statement under our linear branch to check if the nn.Module object has a bias attribute. If it does, we add it to the list initialized to zeros. Finally, we need to transform our list of tuples into a nested dictionary. Luckily mlx has some functions implemented for dealing with parameter dictionaries, and we can use util.tree_unflatten() to convert this list of tuples to a nested parameter dictionary. This is passed into the update method to initialize the parameters. Now we can call _init_parameters() in the constructor.

class GPT(nn.Module):
    def __init__(self):
        super().__init__()
        self.wte = nn.Embedding(vocab_size, n_emb) # token embeddings
        self.wpe = nn.Embedding(ctx_len, n_emb) # position embeddings
        self.blocks = nn.Sequential(
            *[Block() for _ in range(n_layers)],
        ) # transformer blocks
        self.ln_f = nn.LayerNorm(dims=n_emb) # final layernorm
        self.lm_head = nn.Linear(n_emb, vocab_size) # output projection
        self._init_parameters() # <-- initialize params
        # print total number of params on initialization
        total_params = sum([p.size for n,p in utils.tree_flatten(self.parameters())])
        print(f""Total params: {(total_params / 1e6):.3f}M"")
    # Tensor shapes commented
    def __call__(self, x):
        B, T = x.shape # (B = batch_size, T = ctx_len)
        tok_emb = self.wte(x) # (B, T, n_emb)
        pos_emb = self.wpe(mx.arange(T)) # (T, n_emb)
        x = tok_emb + pos_emb # (B, T, n_emb)
        x = self.blocks(x) # (B, T, n_emb)
        x = self.ln_f(x) # (B, T, b_emb)
        logits = self.lm_head(x) # (B, T, vocab_size)
        return logits
    def generate(self, max_new_tokens):
        ctx = mx.zeros((1, 1), dtype=mx.int32)
        for _ in range(max_new_tokens):
          logits = self(ctx[:, -ctx_len:]) # pass in last ctx_len characters
          logits = logits[:, -1, :] # get logits for the next token
          next_tok = mx.random.categorical(logits, num_samples=1)
          ctx = mx.concatenate((ctx, next_tok), axis=1)
        return ctx
    def _init_parameters(self):
        normal_init = nn.init.normal(mean=0.0, std=0.02)
        residual_init = nn.init.normal(mean=0.0, std=(0.02 / math.sqrt(2 * n_layers)))
        new_params = []
        for name, module in self.named_modules():
            if isinstance(module, nn.layers.linear.Linear):
                if 'c_proj' in name:
                    new_params.append((name + '.weight', residual_init(module.weight)))
                else:
                    new_params.append((name + '.weight', normal_init(module.weight)))
                if 'bias' in module:
                    new_params.append((name + '.bias', mx.zeros(module.bias.shape)))
            elif isinstance(module, nn.layers.embedding.Embedding):
                new_params.append((name + '.weight', normal_init(module.weight)))
        self = self.update(utils.tree_unflatten(new_params))

We also add 2 lines of code in the constructor to print the total number of params. Finally, we are ready to build the training loop.

Training Loop
To train the model we need a loss function. Since we are predicting classes (next token) we use cross-entropy loss.

def loss_fn(model, x, y):
    logits = model(x)
    B, T, C = logits.shape # (batch_size, seq_len, vocab_size)
    logits = logits.reshape(B*T, C)
    y = y.reshape(B*T)
    loss = nn.losses.cross_entropy(logits, y, reduction='mean')
    return loss
First, we get the logits from the model. Then we reshape logits to make a list of vocab_size length arrays. We also reshape y, the correct token ids, to have the same length. Then we use the built-in cross-entropy loss function to calculate the loss for each example and average them to get a single value.

model = GPT()
mx.eval(model.parameters()) # Create the model params (mlx is lazy evaluation)
loss_and_grad = nn.value_and_grad(model, loss_fn)
lr = 0.1
optimizer = optim.AdamW(learning_rate=lr)
Next, we instantiate the model, but since mlx is lazy evaluation it won’t allocate and create the parameters. We need to call mx.eval on the parameters to ensure they get created. Then we can use nn.value_and_grad() to get a function that returns the loss and gradient of model parameters w.r.t the loss. This is all we need to optimize. Finally, we initialize an AdamW optimizer.

A quick note on nn.value_and_grad(). If you are used to PyTorch you might expect us to use loss.backward() which goes through the computation graph and updates the .grad attribute of each tensor in our model. However, mlx automatic differentiation works on functions instead of computation graphs [3]. Therefore, mlx has built-ins that take in a function and return the gradient function such as nn.value_and_grad() .

Now we define the training loop.

num_epochs=20
batch_size=32
for epoch in range(num_epochs):
    model.train(True)
    running_loss = 0
    batch_cnt = 0
    for input, label in get_batches(X_train, y_train, batch_size):
        batch_cnt += 1
        loss, grads = loss_and_grad(model, input, label)
        optimizer.update(model, grads)
        running_loss += loss.item()
        # compute new parameters and optimizer state
        mx.eval(model.parameters(), optimizer.state)
    avg_train_loss = running_loss / batch_cnt
    model.train(False) # set eval mode
    running_loss = 0
    batch_cnt = 0
    for input, label in get_batches(X_val, y_val, batch_size):
        batch_cnt += 1
        loss = loss_fn(model, input, label)
        running_loss += loss.item()
    avg_val_loss = running_loss / batch_cnt
    print(f""Epoch {epoch:2} | train = {avg_train_loss:.4f} | val = {avg_val_loss:.4f}"")
The outer loop runs through the epochs. We first set the model to training mode because some modules have different behaviors during training and testing such as dropout. Then we use our get_batches function from earlier to loop through batches of the training data. We get the loss over the batch and the gradient using loss_and_grad . Then we pass the model and gradients to the optimizer to update the model parameters. Finally we call mx.eval (remember mlx does lazy evaluation) to ensure the parameters and optimizer state get updated. Then we calculate the average train loss over the data to print later. This is one pass through the training data. Similarly, we calculate the validation loss and then print the average train and val loss over the epoch.

completion = decode(model.generate(1000)[0].tolist())
print(completion)
with open('completions.txt', 'w') as f:
    f.write(completion)
Finally, we add some code to generate from our model. Since the generation output is still in the (B, T) shape we have to index it at 0 to make it 1D and then convert it from an mlx array to a Python list. Then we can pass it to our decode function from earlier, and write it to a file.

These are the parameters we will use for training (you can play around with this):

ctx_len = 128
n_emb = 128
dropout = 0.1
head_size = 128
n_heads = 4
n_layers = 3
num_epochs = 20
batch_size = 64
lr = 1e-3
Now we can run the file to start training. With the settings above training took around 10 minutes on my m2 MacBook. I achieved the following training loss last epoch.

Epoch 19 | train = 1.6961 | val = 1.8143
Let’s look at some output.

GLOUCESTER:
But accomes mo move it.

KING EDWARD:
Where our that proclaim that I curse, or I sprithe.

CORIOLANUS:
Not want:
His bops to thy father
At with hath folk; by son and fproathead:
The good nor may prosperson like it not,
What, the beggares
More hath, when that made a,
Your vainst Citizen:
Let here are go in queen me and knife
To my deserved me you promise: not a fettimes,
That one the will not.

CORIOLANUS:
And been of queens,
Thou to do we best!

JULIET:
Not, brother recourable this doth our accuse
Into fight!
Not bad for just 10 minutes of training with a tiny model that is predicting characters! It clearly has the form of Shakespeare, although it is nonsense. The only difference between our model and the real GPT-2 now is scale! Now I encourage you to experiment — try out different settings, maybe tinker with the architecture, and see how low of a loss you can achieve.

References
[1] Karpathy A (2015).Tiny Shakespeare [Data set]. https://github.com/karpathy/char-rnn (MIT license)

[2] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, Language Models are Unsupervised Multitask Learners (2019), OpenAI

[3] Automatic Differentiation — mlx docs"
14,https://blog.reedsy.com/short-story/a3gstd/,"My eyes felt like galaxies—holding the swirling glow of countless memories—as I took in our childhood home. Its siding looked like remnants of driftwood after a bonfire. I swore I smelled the smoky char of pine creep into my nostrils. It’s wild how the past stays with you like that. It can feel more visceral and real than the tangible things right in front of you.

“Jesus, it feels like just yesterday.” I placed a trembling hand over my heart, struggling to steady my breath.

My brother, Perry, pulled me into a tight embrace, his strength grounding me like an anchor.

“The house hasn’t changed much,” he said, his voice steady and comforting. “But we have.” His certainty made me question, Have I really changed?

Between the two of us, Perry was as solid and stoic as a mountain range. Good thing, because I was like the wind—flighty and unpredictable. Over the years, Perry had learned to handle even my harshest hurricanes.

Being his older sister—even if only by four minutes—I always wished I’d been his protector rather than the other way around. But that demon burning deep in my belly also flashed a crooked smile, knowing that Perry would never abandon me, especially since I got sober.

I hadn’t had a drink in exactly seven hundred and thirty days, and although it remained unsaid, I knew Perry was terrified of leaving me to my own devices in fear I would relapse.

Our sibling bond was iron-clad. After we lost our parents in the fire (my mother didn’t properly butt out her 2:00 am cigarette and well, the rest is history), all Perry and I had was each other. But let’s call a spade a spade; we were also as fucked up and as co-dependent as it gets. Who mutually decides to visit the catalyst of your alcohol addiction on the anniversary of your sobriety?

The house’s dilapidated front door creaked as Perry gently pushed it open. The rusted metal hinges were holding it up by a thread.

“After you.” Perry gestured me in, squinting from the sunlight. He was a gentleman, even in such obscurity.

As he held the door open, the shallow scar on his right cheek taunted me like some kind of schoolyard bully. His wound often pulled me in like that. Some days, I was sure I would dive right into it and drown. Other days, I prayed to God and the Devil himself to just let me fucking drown, already.

That mark became permanently etched on Perry’s face on the day I quit drinking, exactly seven hundred and thirty days ago. That was the day Perry screamed bloody murder at me from the passenger seat, “Jackie! Stop the fucking car!” But my bloodstream was far too poisoned with Bacardi Limon to listen. All I remember next was my vehicle being wrapped around a tree. I could have died that day, but what truly disturbed me in the middle of the night was the fact that I almost killed Perry.

A lot can happen in seven hundred and thirty days. But I assure you, forgiving yourself isn’t one of them.

“Well? You coming in?” Perry was still holding the door ajar.

I shook it off and gave my brother a knowing glance. I swear, even though we were fraternal, we had twin telepathy. I exhaled and walked in.

“Watch your step,” I warned, my forehead tense.

I imagined the rickety floorboards collapsing, crashing us into what had once been our dad’s “man cave”. That’s where he was passed out, the night of the fire.

“Kids, stay here. Do not move,” our mother demanded after getting us out of the house safely. I remember the black soot on her face and the spiderweb veins in her eyes. She shook us firmly by the forearms. “I’m getting your father.”

Perry and I held each other, shaking. The heat from the inferno felt like standing in a volcano. We never saw our parents again.

Two decades later, there we were—Perry and I—-making our way through the wreckage of our home. It was midday, yet the inside of the house screamed a tone of pale blue midnight. My shoulders were up to my ears, as though we were walking through a haunted house attraction.

I coughed into my forearm. The ocean of dust was thick like butter. As I cleared my lungs, Perry called out from behind me.

“Jacks, look at this! The fireplace,” Perry's voice was filled with awe.

“Unbelievable. It’s still here,” I whispered, a lump forming in my throat.

It was as though a Fairy Godmother breezed by and brought the decaying living room to life with her magical paintbrush. Kind of like in “Titanic”, when they showed the sunken ship underwater, versus in its prestige as it sailed across the Atlantic.

We made our way over to the fireplace and sat cross-legged on the floor.

“This was our favorite spot, remember?” I avoided his gaze, overwhelmed by the flood of memories.

“Yeah,” Perry murmured, his eyes softening with nostalgia.

For a moment, the taste of crispy, fire roasted marshmallows superseded the saturated stench of mildew.

“Remember our s’mores nights?” I asked.

“Duh. What about all our fireplace movie nights?” Perry proceeded to do his best, nasally, childhood impersonation of me, “But mom! I want to watch Beauty and the Beast!! Perry always gets to pick the movie!!”

I punched him in the arm, “First of all, I never sounded like that. And second. So what? I knew what I wanted.”

The corners of Perry’s mouth lifted. He had such a sincere sparkle about him, as though a storm cloud of confetti followed him overhead wherever he went, “You really did, kiddo.”

My chest went heavy. How could Perry love me after everything I had done? After all the relationships I’d ruined? All the jobs I’d lost? All of his relationships I’d ruined? How could he still choose me, when so often I had chosen a forty of Jack Daniels over him?

How could Perry still love me after I almost fucking killed him?

Perry’s gaze widened, “Hey! Remember when Mom would bring out those hot drinks she always made?” He paused, almost as if he was searching for the right term. “Apple… something? Apple ssshhh…”

I snapped my fingers, “Apple-Schnapple!”

“Yes!”

“I mean, looking back it was basically just hot apple cider, but damn it was good.” And it really was.

Our laughs echoed throughout the abandoned asylum we once called home.

Perry leaned back, holding himself up with his hands. “I loved our fireplace hangs. Especially our movie nights down here. But nothing beats our movies up in the projector room.”

I tilted my head, “We never had a projector room.”

Perry playfully “shoo’d” me away.

“No. Perry. I would remember us having a projector room. Our movie nights together were our favorite thing. You even just said it yourself.”

The house suddenly became pin-drop silent as Perry leaned in. ""Memories are quite something, aren’t they?"" The slight shift in his tone made my skin crawl. Perry was always wistful, but this felt different, almost… clinical, ""We often remember things in ways that are… easier for us to digest.""

I was fidgety. “Gees Perry. You sound like Dr. Lasko.”

He seemed to enjoy my little joke.

Dr. Lasko had been my therapist since the accident, and I would go out on a limb and say that he would not have approved of this self-inflicted exposure therapy I was subjecting myself to by visiting the house.

Perry seemingly snapped out of his little therapist moment and went right back to being his sparkly confetti self. As I watched his amicable face scan the mantle above the fireplace, I felt a sickening uneasiness. Imagine you had actually fucking killed him.

“Man, I can still picture all our family photos up there,” Perry’s childlike wonder destroyed me.

My face went flush. I could feel the water rising in my eyes like the tides. How pathetic and infuriating was it that after everything I’d done, I was still somehow the victim.

“Hey.” He took my hand.

“Oh Perry,”  I threw myself into him. “I’m so sorry.”

My brother held me with his usual care that I didn’t deserve.

“Jacks, it’s ok. I’m still here. We’re both still here.”

As my chin rested on my brother’s shoulder, I looked ahead of me at the remains of the house. Something felt off, and it wasn’t just the overall unsettling environment. My brow furrowed. “Wasn’t the doorway to the kitchen on the other side of the living room?”

I felt Perry shrug, “I don’t think so.”

I was staring into the out of place doorway like I was trying to pull a recollection towards me. And that’s when I saw them in the kitchen: the translucent memory of mom and dad. Mom was getting our Apple-Schnapples ready. She was pacing, unlike Dad, who was sitting motionless at the table. His face was planted on its mahogany surface. His glass of Apple-Schnapple was empty, and so was the bottle of Jim Beam beside it.

Mom floated into the living room, our warm beverages in hand and a cigarette in her mouth, “Kids, your father’s not feeling well. Let’s have our Apple-Schnapples in here.”

Oh my God. The bruise on her face.

Perry jarred me back to reality with the gut punch of what he had to say next, “You’re remembering the truth about mom and dad, aren’t you?”

I pushed myself off and away from him. “How did you…”

My brother looked down, delicately tracing the floor with his finger, “We always put them on a pedestal after they died.”

I felt a shiver run down my spine, “What are you talking about?”

As Perry continued to speak, his words grew even more detached. ""Do you remember that first drink Dad ever gave you?""

My eyes darted around the room as my jugular pulsed in my neck. As much as I tried to escape what Perry had just said, I did remember.

I could hear my dad’s slurring words of encouragement, “Come on, Jackie. Just one drink. It’ll be our special time, just you and me.”

The bitterness of that first sip of beer made me squirm, but sharing a “special time” with my dad—and the desperate yearning that maybe he did love me, afterall—was the overwhelm of the full moon swallowing me whole. I was only a child, and much like how my mom turned a blind eye to my father’s drinking, she did the exact same when it came to her daughter.

I’d used the death of my parents as the excuse for my alcoholism for so long, because admitting that they helped create the monster I would eventually become was like a knife to the heart. And knowing I had been too weak to conquer the addiction from my own volition just made the weapon twist in my chest.

The room was spinning. My face was blistering hot like the night of the fire. Or was that the warm heat from the fireplace when we were kids? The childhood fireplace memories ran through my mind, frame by frame, until…..they eventually vaporized to nothing. I crouched over, thinking I might vomit.

“We never had a fireplace.” Perry was nodding, very matter-of-factly.

My fingernails dug into my thighs when I looked at the fireplace and: it was gone. Only a blank wall of faded, forest green wallpaper remained. Our house was once on fire, yes, but that was it. There was never a happy childhood fireplace. Ever.

My hands were cold and clammy. I fell back onto the wall behind me. “Perry. Where are we?”

He stood up and glided towards the staircase. One hand on the banister, his footsteps creaked, one by one, as he made his way to the second floor.

My mouth was bone dry, “Perry!”

He stopped and turned towards me, “Come to the projector room. We love watching movies together, don’t we? There’s a movie I’d like to show you.”

As my brother disappeared from sight, I did what any classic horror movie trope would tell you to do: I went upstairs.

I found Perry standing at the end of the ominous hallway. Large, empty picture frames lined the oxblood walls leading up to him. Through the doorway where Perry stood, particles in the air danced in the projector’s cone-shaped light. That telltale winding of a film reel was the only sound in the deafening quiet of this house that I no longer recognized.

Half of Perry’s face—the one with the scar—was perfectly illuminated, as though he was wearing the mask from “The Phantom of the Opera”. “I think you’re ready to see how this movie ends, Jackie. This is the most progress you’ve made since we’ve been coming here.”

I gripped my thumbs in the palms of my hands, “Perry, you’re freaking the fuck out of me!”

I thought my knees might buckle as my brother’s face glitched, like a flash of static snow on a television set. As his face settled back to normal to a deadpan gaze, he disappeared in the innocuous room. I followed, running on nothing but fumes.

Clutching the doorway, my mouth fell agape. Perry was gone. I darted to the middle of the room.

As I frantically searched for my brother, I shielded my eyes with the back of my hand from the projector’s light. And that’s when, from behind me, I heard five words that made my blood run cold, “Jackie! Stop the fucking car!”

I was convulsing yet paralyzed. Moving as slowly as cold molasses, I rotated on the spot towards my worst nightmare, shown on 35 mm. On the projector screen was Perry and me in my car, exactly seven hundred and thirty days ago, the day I almost kill—

Oh my God.

My head pounded as fragmented memories surged. The reality of what happened began to crystallize, unrelenting.

My joints ached and my stomach churned. Clamping a hand over my mouth to stifle a scream, I stumbled down the hallway as it began caving in on itself. The picture frames were sucked into the walls. The floorboards cracked into distorted peaks and valleys. Wooden beams swung down from the ceiling like pendulums. I tried to spit out the chalky grit of disintegrating drywall that made the hallway look like a winter squall.

Panting heavily, I stopped dead in my tracks at the stained glass window. My body trembled with an all-too-familiar dread. Each time I faced this, I wondered if this fall would be the one that would finally end it all.

Maybe it’d be better if it did.

Holding my breath, I threw myself through the glass, my hands clawing the air for dear life. Free falling two stories feels like an eternity when you’re watching the memory of your childhood home fall apart before your very eyes. But when the weight of my body finally made contact with the earth I—

I gasped. The cold air of the hospital room shocked my lungs. I sat up, ripping the suction cups from my face and body. My gown was clinging to me, soaked in sweat. Medical equipment beeped all around me like a metronome.

Dr. Lasko, my therapist since the accident, sat across the stark white room, sighing as he rubbed his forehead. He, too, was connected to a plethora of suction cups and wires. He looked a little worn out in the fluorescent overhead lighting. Ever since I was hospitalized and later incarcerated, Dr. Lasko had been helping me delve into my memories, namely the ones that were too excruciating for me to face. And as such, Dr. Lasko had been appearing in the simulations as my brother Perry, the love of my life who died in the car crash, seven hundred and thirty days prior.

Disoriented, I blinked rapidly, the vividness of the memory contrasting sharply with the sterile, geometric ceiling tiles above me.

“I don’t ever want to do that again!” I was venomous.

“Jackie,” Dr. Lasko started.

“Don’t start,” I pulled up four fingers for air quotes, “‘Jackie, don’t give up. This was the closest you’ve ever come to facing the truth.”

As the initial burst of adrenal and cortisol left my body, I fell back on my pillow. I was depleted. Quiet rivers flowed down my cheeks.

Removing his own suction cups, Dr. Lasko approached my bedside and took a seat. He treaded lightly. “Jackie, I understand how challenging this is for you, but you did an incredible job today. If we continue making progress like this, there's a real possibility you'll gain your freedom sooner.”

I looked at the well-meaning doctor, but all I could see was Perry. Multicolored confetti fell softly around him like that first November snow. His face was the sun. His eyes reflected the whole world back to me.

Perry.

With a weak grip, I took Dr. Lasko’s hand. My vocal cords were like sandpaper. “I’ll leave this place one day, doc.” A single tear dripped from my chin onto my collarbone. “But I’m not sure if I’ll ever be free.”

Dr. Lasko didn’t say a word, but I felt him squeeze my hand, just a little bit tighter.

I licked the cracks on my lips as my eyes closed shut, imagining the oaky comfort of bourbon on my tongue. I felt myself drift, and good thing, because I needed the rest. Dr. Lasko and I would be delving into my memories again the following day.

No matter how masochistic it felt, I vowed to keep showing up for the simulations. Even if I never forgave myself for what I did, at least in my memories, I got to see Perry."
15,http://www.chakoteya.net/DoctorWho/40-1.html,"Space Babies

Original Airdate: 11 May 2024

[Tardis]
(Ruby has walked into the unlocked Tardis at the end of The Church on Ruby Road.)
RUBY: Who are you?
DOCTOR: I'm the Doctor. You don't have to stand over there. Come and have a look. It's called the Tardis.
(Snaps his fingers and the lighting changes.)
RUBY: Ooo! Nice! But hold on. I can't call you Doctor. No, I want to know your name.
DOCTOR: Yeah, that's er... that's tricky, because I was adopted, and the planet that took me in, they were kind of... they were kind of posh. They'd use titles like the Doctor, or the Bishop, or the Rani, or the Conquistador. Say Doctor for a thousand years and it becomes my name.
RUBY: Okay. The planet. Parking that. Thousand years, double parked. So you're a doctor, but you're... the police?
DOCTOR: Police box. No. No, no, no, no, that's a disguise.
RUBY: Oh.
DOCTOR: Inside, it's a Time and Space machine, but outside, it's like a chameleon, 'cos once I landed in 1963 and they used to have police boxes on street corners.
RUBY: 1963?
DOCTOR: Yep.
RUBY: Okay. Ooo, jukebox. I like that.
DOCTOR: Mmm.
RUBY: Okay, so, back to the planet.
DOCTOR: My world was called Gallifrey.
RUBY: Gallifrey? And where's that?
DOCTOR: Gone! Ruby, it's gone. It's gone. They died. There was a genocide, and they died. So the one that was adopted was the only one left. I am the last of the Time Lords. And I am so, so glad to be alive. This thing flies. Do you want to see?
(The gravity goes off, the Tardis dematerialises, gravity back on. Never done that before.)
DOCTOR: Let's have a random landing.
RUBY: Whoa!
DOCTOR: Hoo-hoo! Ooo... 150 million years in the past.
RUBY: No!
DOCTOR: Really.
RUBY: No, you've got to be k... You are kidding. Don't be so ridiculous. Are there dinosaurs out there?
DOCTOR: I don't know. Go and have a look.
RUBY: Wait! No. Is it safe? What if I change history by stepping on a butterfly or summat?
DOCTOR: Well, that's not going to happen, is it? Who steps on butterflies? You'd literally have to be like, ""Wait. Come 'ere, butterfly! ""Come 'ere, 'ave it!""

[Prehistoric Earth]

RUBY: Oh, my God. That... that's so beautiful.
DOCTOR: And Tardis stands for Time And Relative Dimension In Space, huh? So we've moved location as well. This will be North America. One day, this is Wyoming. A little town called Green River.
(A boot steps on a butterfly.)
DOCTOR: Oh!
(Ruby is no longer a human.)
RUBATHON: What's wrong? Did I do something wrong? Because I am Rubathon Blue of the 57th Hemisphere Hatchlings, and I do not do wrong things, Dok-tah.
DOCTOR: But...
RUBATHON: If you have made an incorrect accusation, I will have to kill you.
DOCTOR: No, no, no. Just wait, wait a minute. Just...
RUBATHON: What are you doing?
DOCTOR: Nothing, just...
(He scoops up the butterfly, breathes on it, and it flies off. The human is back.)
RUBY: Am I missing summat?
DOCTOR: Nothing. Let's try that again, okay?
RUBY: Thank you.
DOCTOR: Yeah. Yeah, yeah, yeah.

[Tardis]

DOCTOR: Okay. Controls are new. Completely forgot... the butterfly compensation switch. Good. Right. Yes. Let's go forward. Give me a number. Give me a year.
RUBY: Er, two.
DOCTOR: Two.
RUBY: One.
DOCTOR: One.
RUBY: Five.
DOCTOR: Five.
RUBY: Oh.
DOCTOR: Oh.
RUBY: Ah, six!
DOCTOR: Six! Ah! Five numbers! I like it!
(The Tardis travels the Vortex.)

[Space station]

RUBY: But we're indoors. We got through walls. Ah-ha. Is that like a matter transporter, like in Star Trek?
DOCTOR: We've got to visit them one day.
RUBY: Hey, but you said the Tardis was like a chameleon, but it still looks like a police box.
DOCTOR: Oh, it's, er... it's broken. Most of the universe is knackered, babes. Okay. Come, come, come, come.
RUBY: Oh, it stinks
DOCTOR: Something is wrong with this place. It is a space station reaching overload. Whoa! Whoa!
RUBY: No, you've made it worse.
(Something snarls nearby. They both jump.)
DOCTOR: No, that is worse.
RUBY: Is that a monster?
DOCTOR: No. No, don't be silly, Ruby. There's no such thing as monsters, there's just... just creatures you haven't met yet. Hi there.
(The creature roars.)
RUBY: Run?
DOCTOR: Run! Run! Run!
(They and the monster are visible on monitors as they run down passages.)
DOCTOR: Come on! In here, in here, in here, in here.
RUBY: But...now we're trapped! Now we're trapped! Push the button! Doctor!
DOCTOR: Okay.
(The tiny one-person lift takes them up. The Doctor's hand is over Ruby's eyes.)
DOCTOR: Oh, yeah, yeah.
(The lift abruptly arrives.)

[Birth Zone 6]

DOCTOR: The question is, why did I run?
RUBY: 'Cos it was scary.
DOCTOR: It was new. I love meeting new things, so why did it give me the shivers? I couldn't run fast enough. I was like whoosh!
RUBY: Well, it'd help if we knew where we were.
DOCTOR: Yet again, push the button.
(The lights come on so they can see all the glassware, containing...)
DOCTOR: Oh. Oh, we're on a baby farm. Ha-ha! A parthenogenesis machine. What is it with you and babies?
RUBY: I was going to say the same thing to you.
DOCTOR: We've gone from baby to baby. I'm not saying things are connected, and yet... things connect.
RUBY: Well, I'm the one looking for my parents, and you've got a Time and Space machine. So this place grows babies. What for? Food?
DOCTOR: Food? What? What?! Food? They're not tomatoes!
RUBY: Well, excuse me. There's a big hungry thing downstairs.
DOCTOR: Baby farms boost the population. Sometimes a world goes sterile or... I don't know, goes mad and bans kissing.
RUBY: So these babies are human, yeah?
DOCTOR: Yep, grown for a colony world.
RUBY: And a colony world is not Earth?
DOCTOR: Hey. Okay, one last time, push the button.
(And a shield retracts to reveal that they are in orbit.)
RUBY: We made it. The human race, we survived. We went to the stars. And ten minutes ago, Doctor, just ten minutes ago, you said genocide. Your people are gone.
DOCTOR: Yeah.
RUBY: How do you keep going?
DOCTOR: For days like this, Ruby Sunday. I don't have a people. I don't have a home. But I don't have a job, either. I don't have a boss, or taxes or rent or bills to pay. I don't have a purpose or a cause, or a mission, but I have... ..freedom. And so I keep moving on, to see the next thing, and the next, and the next. And sometimes... it looks even better through your eyes.
RUBY: So where's this, then?
DOCTOR: Oh, er...
(Calls it up on a screen.)
DOCTOR: Huh. Planet Pacifico del Rio.
RUBY: Oh, that's in English. They speak English here? English exists?
DOCTOR: Er, no. No, no, no. Humans all speak one language by this point. A bit like Cantonese. This is what it really looks like, but the Tardis translates. It's got a perception filter, so it helps you fit into every time and place.
RUBY: Right, and my mum, she's long gone now.
DOCTOR: Can I see your phone?
RUBY: Yes.
DOCTOR: So, my sonic screwdriver can make the distance between you and Earth 19,000 years or... one phone call.
RUBY: What?
DOCTOR: Carla. Phone her.
RUBY: But...
DOCTOR: Your mum, Ruby. Call your mum.

[Ruby's home / Birth Zone 6]

CARLA: Well? What is it now?
RUBY: Mum?
CARLA: Yes, Mum, obviously. You've just ran out the door ten seconds ago. Why are you phoning me? You went like the wind. Where are you going?
RUBY: Yeah. Yes, I will... I'll, er... I'll catch up with you in a minute. Bye. Love you. Love you. Merry Christmas!

[Birth Zone 6]

RUBY: That was my mum, on Christmas Eve. On my birthday, ten minutes ago. That's the best signal ever. How much does that cost?
DOCTOR: I want to know what the hell is wrong with this place. Do you see? It's calm up here, but underneath it is seething, just like downstairs with that creature. There's got to be a crew or a captain...
(Enter a child in a motorised push-chair.)
ERIC: This is Eric, reporting from Birth Zone 6. I keep getting these temperature fluctuations. I've opened up safety valves 10 to 16. Tried cross-matching with the CO2 exchange, but until we get that pressure down, I can't...
DOCTOR: Hi.
RUBY: You all right?
ERIC: But... you. Oh. We've been waiting for an awfully long time. Mummy! Daddy!
DOCTOR: Oh, no.
RUBY: No, no. No, darling, we're not...
ERIC: Boys-oh-boys, I've got to tell everybody Mummy and Daddy are here.
(Leaves the room.)
RUBY: A baby farm. Run by babies.
DOCTOR: Ha-ha! Space babies!
(They follow Eric along a corridor with crayon drawings on the lower part of the wall.)

[Control room]

ERIC: They're here. They came at last. Mummy and Daddy are here.
(All the crew are in electric pushchairs.)
BABIES: Mummy and Daddy! They came back!
DOCTOR: Hello, space babies.
BABIES: Hello, Daddy. Hi, Daddy. Hello, Daddy!
DOCTOR: Oh.
POPPY: Everyone, back to work. Show Mummy and Daddy what a good job we've been doing. Make them proud.
(The controls are jury-rigged with string and wooden pointers so the babies can activate them.)
MARCEL?: My job is to keep the pipes clean. I'm proud of the pipes.
ADJANI?: And I keep the oxygen nice and cool. We need oxygen to breathe.
SANDRA?: And I pull this string and that string. I'm not sure what they do, but I pull them very hard.
ERIC: And I made this for you. It's a little flower.
RUBY: Thank you.
POPPY: I'm Captain Poppy and I kept the station running for Mummy and Daddy, because we knew you'd come back for us one day. We waited.
DOCTOR: Right. You're not supposed to be running this place. This isn't Baby World. You got left behind when the adults... ..vamoosed?
POPPY: We took over. We were very brave.
RUBY: Right. That's great. That's, oh, that's good. That's amazing. You've done a really great job.
DOCTOR: I'm sorry, Poppy, I'm so sorry, but we are not your mummy and daddy. I wish we were, but we're not.
ERIC: They left us. Where did they go?
RUBY: I don't know, darling, but... I'm Ruby and this... this is the Doctor. And we're your friends. Yeah, got you. I've got you, I've got you, I've got you, I've got you.
(She picks up Eric from his pushchair.)
BABIES: And me! And me! And me! And me!
DOCTOR: Oh, gosh.
BABIES: And me! And me!
DOCTOR: Captain Poppy, when was the last time that you had a hug?
POPPY: Never.
DOCTOR: Oh. Oh, baby, it's okay. Come here, it's okay. It's okay, it's okay. Shh-shh-shh. Aww, never had a hug.
RUBY: Come on, you can all have a hug.
(Later, with everyone back in their pushchairs.)
POPPY: Did I get things wrong, Doctor
DOCTOR: Well, according to this, the crew went home. They abandoned ship and they left you guys behind. I don't know why, but they left the birth machine running, so you lot grew up, but you stayed the same size. Baby size. Space babies.
POPPY: But are we wrong?
DOCTOR: What do you mean?
POPPY: We're not meant to be like this. Did we grow up wrong?
DOCTOR: Oh, Poppy. Oh, Popsicle. Look at me. Look at me. Nobody grows up wrong. You are what you are, and that is magnificent.
POPPY: But Mummy and Daddy left us.
DOCTOR: That's okay. Mine did, too.
POPPY: What happened?
DOCTOR: Well, I was found.
POPPY: Hooray!
DOCTOR: Yeah. Little baby me was left alone in the middle of outer space, and guess who took me in.
POPPY: I don't know.
DOCTOR: The Time Lords.
POPPY: Ooo.
DOCTOR: Can you say it like me?
POPPY: The Time Lords.
DOCTOR: That's it, P-P-P-P-Pop. But the point is, is that it doesn't matter where I come from, because I am absolutely lovely, aren't I?
(Poppy yawns.)
DOCTOR: That wasn't rhetorical, Pops.
POPPY: Yes, you are.
DOCTOR: And do you want to know my secret? There's no one like me in the whole wide universe. No one like me exists, and that's true of everyone.
It's not a problem, Captain Pops. It's a superpower. High five. Yeah.
POPPY: Yeah!
(Ruby is dandling Eric, with the other babies in a semi-circle.)
RUBY: So you're Eric. And you're Tasha. And Ruben. And then there's Saltine and Boo.
ERIC: I love you, Ruby.
RUBY: Aw, I love you too, Eric. But how do you manage all on your own?
ERIC: We've got Nanny. Say hello, Nanny.
NAN-E: Good afternoon, children, and welcome to our new visitors.
DOCTOR: Oh. Nanomatrix Electroform. Nan-E. Right. Hi, Nan-E. I'm the Doctor, and this is Ruby.
NAN-E: We have visitors, children.
ERIC: Nanny!
NAN-E: Noses must be blown. Activate nose-blow.
DOCTOR: Er...
NAN-E: One, two, three and... blow.
(Mechanical hands on the pushchairs put handkerchiefs to the babies' noses. They blow into them, then the dirty handkies are dropped into a disposal tube.)
NAN-E: Well done, children And now, children, back to work. Nappies are changed at 1800 hours.
RUBY: Oh, can't wait to see that.
DOCTOR: Right. So it's you lot? It's Nan-E And downstairs, is that your pet dog?
(Everyone screams and cries.)
ERIC: That's not a doggo.
RUBY: What is it then, Eric?
ERIC: The Bogeyman.
RUBY: Shush, shush, shush. Shush, shush, shush.
DOCTOR: No. Gosh.
ERIC: We don't like the Bogeyman.
RUBY: No, no, no. Shush, shush. I did not mean to scare you. There is no such thing as the Bogeyman. That thing was more sort of like a er...
DOCTOR: Bogeyman!
RUBY: No, stop it! No, stop it! Nan-E, tell them there's no such thing as the Bogeyman.
NAN-E: Nan-E is scared of the Bogeyman.
DOCTOR: Then what is the Bogeyman doing down there, and why... why is it so scary?
(Puts it on monitor. The babies wail.)
RUBY: Doctor, turn it off.
DOCTOR: Okay.
RUBY: No, listen to me. Listen to me.
DOCTOR: I'm sorry, I'm sorry. I'm sorry, babies. Space babies. I'm sorry.
POPPY: Oh, Ruby...
(The Doctor finds a headset and puts it on, then works a computer.)
DOCTOR: Right. Nan-E. These babies are trying their best - space babies - but this station is in trouble. You have got a build-up of pressure in Hull 3-B. Something is ramping up down where the Bogeyman lives. And if that continues... baby boom.
NAN-E: Portal 3-5-7.
DOCTOR: Okay, what's that?
NAN-E: Access Portal 3-5-7.
DOCTOR: That's on this floor. What is it?
NAN-E: Access Portal 3-5-7.
DOCTOR: Yeah, it is just a storage unit. What would I need to go there for?
NAN-E: Oh, for God's sakes, 3-5-7. Come on!
RUBY: Where do you think you're going?
DOCTOR: Portal 3-5-7!
RUBY: Right. Great. Ok. Coming!

[Corridor]

RUBY: So, is this what you do, Doctor? I mean, in life? You help? That's like your... purpose?
DOCTOR: No, no, I'm just, er... helping babies - space babies. Ha! Listening to my hearts. Two hearts. Plural.
RUBY: Okay. Two hearts. But what if helping the babies takes six weeks? Or ten years? Because my mum's still waiting for me.
DOCTOR: Back home, on your birthday. Yeah, it's strange, your life. You were abandoned, like this lot. If things connect, then you are connecting like crazy. You don't know anything about your birth mother or your father? They didn't leave a note or a scrap of paper...?
RUBY: Nothing. I was... I was just left.
DOCTOR: By the church.
RUBY: In the snow.
DOCTOR: On Ruby Road.
(The Doctor sees a figure point at him by the church.)
RUBY: Doctor...
DOCTOR: What?
RUBY: It's snowing. Okay, what just happened? I said snow, and we've got... ..snowflakes.
DOCTOR: It's like a memory just came through, from the day that you were born.
RUBY: But how? Is this the sort of thing that happens with time travel?
DOCTOR: I have been to the ends of time and back, and I have never seen anything like this before.
RUBY: Then what does it mean?
DOCTOR: I don't know.
(The snow has stopped.)
DOCTOR: Oh, I thought my birth was crazy...
RUBY: Oh, yeah.
DOCTOR: Oh... I wonder who she is. Your mother. The memory changed. She was pointing at me.
(A door opens.)
JOCELYN: I said Portal 3-5-7. Don't just stand there yapping, you pair of idiots. Get inside!
RUBY: Who's she?
DOCTOR: Nan-E. Ha!
RUBY: Oh.

[Portal 357]

RICO [on screen]: This is Captain Rico Trieste, signing off duty from Babystation Beta, Pacifico date 56-56-22. For the record, I'm signing off under protest and wish to condemn this course of action.
LUCIA [on screen]: Chief Engineer Lucia Colasanto signing off, 56-56-22. And I'd like to say for the record, the company's actions are appalling. I will be launching an appeal against this as soon as we're home.
(Jocelyn fixes a gas leak with a blow from a wrench.)
GINA [on screen]: This is Comms Officer Gina Scalzi signing off, 56-56-22.
(Played by Susan Twist. She keeps turning up, does this woman.)
DOCTOR: So the crew went home, and left the babies behind? Space babies.
JOCELYN: It's the recession. The government closed the Babystation to save money, but the law says it's illegal to stop the birth machine.
(Another leak, another thump with the wrench.)
JOCELYN: But how did you arrive? Have you got a way out of here?
DOCTOR: I've got a ship, yeah, it's er... What is your name - sorry, Nan-E?
JOCELYN: Jocelyn, Jocelyn Sancerre. I was the on-site accountant. I don't know how this place works.
(The Doctor plugs his sonic into the computer.)
DOCTOR: Jocelyn, hold on, hold on, hold on. This... this can help. If you leave this to sync up, that should recalibrate the whole shebang.
JOCELYN: Thank you. Wanna swap?
RUBY: Hang on. So the planet down below refused to stop the babies being born... but once they're born, they don't look after them?
JOCELYN: It's a very strange planet.
RUBY: It's not that strange.
DOCTOR: But you stayed behind.
JOCELYN: I couldn't leave them. And I tried with this place. But I'm not an engineer. The machine went out of sync, I patched it back, but then the education software ran out of control. It's a mess. And I've been all on my own, watching the kids, for six years.
DOCTOR: But I don't understand. They are gorgeous. Why would you hide?
JOCELYN: Cos I don't want to see them die. And I don't want them to see me die. 'Cos that's how bad it is. This is a closed station. There's only so much air. There's only so much food. The last thing I'll do is give them the air out of Portal 3-5-7. But then... ..then you came along.
RUBY: Can't you fly somewhere else?
JOCELYN: What do you mean, fly?
DOCTOR: Er, space station. Stationary, no engines. This great big thing can't move. It's just stuck in orbit, which is a shame, because this is a nice little system.
JOCELYN: The fifth planet out, Mondo Caroon, that's a DuBarryDuPlessy world.
DOCTOR: Oh, that's good. DuBarryDuPlessy is a starwide organisation. It means they can take in lots of refugees.
RUBY: Oh. Well, can't we call them for help?
JOCELYN: They don't go and fetch refugees. That's the fate of every refugee in the universe. You physically have to turn up on someone else's shore. And we can't move.
DOCTOR: But now you have a ship. Plenty of room. It's called the Tardis. The trouble is, between us and the ship is the Bogeyman.
JOCELYN: What is that thing?
BOTH: You don't know?
JOCELYN: It's nothing to do with me. It's not part of the manifest. It's not like anything I've ever seen.
DOCTOR: No, nor me. But it reminds me of something. What is it? And what is its skin made of? And why... was I so scared?
JOCELYN: Because it's terrifying.
DOCTOR: Yeah, but I've met a million ugly bugs. I'm an ugly bug. That thing made me run, and I just wonder why.
RUBY: Okay. Thing is, this place is completely mad, but it sort of makes sense. Because you've got babies, you've got a nanny, and you've got the Bogeyman. You've literally got a monster living down below. It's a children's story come to life.
DOCTOR: And every story has its hero.
(They spot someone on the screen.)
RUBY: That's Eric. Is that Eric?
JOCELYN: Eric, get out of there.

[Space station]

NAN-E: Eric, please vacate this area.

[Portal 357]

DOCTOR: Oh, is that how it works?
JOCELYN: Nan-E filter. Eric, get out now.

[Space station]

NAN-E: Eric will leave immediately.
ERIC: No, Nanny! I'm being brave.

[Portal 357]

JOCELYN: Eric, for God's sake, run!

[Space station]

NAN-E: Eric, invoking the deity, accelerate perambulation.
ERIC: I'm doing what Ruby said.

[Portal 357]

RUBY: What?

[Space station]

ERIC: I love Ruby, and she said there's no such thing as the Bogeyman. So I'm going to find the naughty doggo and tell him off.
(He meets the Bogeyman.)
ERIC: But I'm so scared.

[Portal 357]

RUBY: Oh, my God, it's my fault!

[Birth Zone 6]

RUBY: Eric, I'm coming! I'm coming! I'm coming! I'm coming! I'm coming! I'm coming!

[Space station]

(Ruby and the Doctor take the little lift down, and find Eric's pushchair fallen over.)
RUBY: Oh.
DOCTOR: Nan-E, where's the Bogeyman?

[Portal 357]

JOCELYN: It's about 400 metres north-west of you. But still no sign of Eric. I can't get a proper fix. I told you, these systems are a crock of...

[Space station]

NAN-E: ..waste products.
DOCTOR: Mind your language, Nan-E.
RUNY: Okay, Doctor, if we make a ton of noise, then the Bogeyman will come for us and leave Eric alone, yes?
DOCTOR: Yes.
RUBY: Okay, right.
DOCTOR: Yes. Yes, yes.
(They pick up things to hit the pipework with and move off.)
RUBY: Bogeyman! Bogeyman!
BOTH: Bogeyman! Bogeyman!

[Portal 357]

JOCELYN: It's moving. It's heard you.

[Space station]

RUBY: Okay, nice plan, but what now?
DOCTOR: I think... if I was very, very little and I knew the Bogeyman was coming... I would need to change my nappy. 'Cos I can detect...
(In a locker.)
DOCTOR: Space baby! Oh, Eric.
RUBY: We've got you, we've got you.
DOCTOR: Oh, you poor thing. It's okay.
RUBY: I know, I know. I know.

[Portal 357]

JOCELYN: Not west, I meant east.

[Space station]

DOCTOR: Go, go. It's all right, it's all right. It's all right, it's all right, it's all right. All right, all right. It's all right.
RUBY: It's okay, it's okay.
DOCTOR: All right.
(The Bogeyman moves off. They come out of hiding, and there it is. They run.)

[Portal 357]

JOCELYN: Don't you touch them, you...

[Space station]

NAN-E: ..illegitimate person.
DOCTOR: Go! Go. It's a dead end.
(The Bogeyman is there.)
DOCTOR: Whoa! It's okay, it's okay. You're okay.
(The Bogeyman is attacked by flames. It runs away.)
POPPY: Babies to the rescue!
DOCTOR: Ha! Space babies!
RUBY: Babies with a flame-thrower!
DOCTOR: Babies, babies, babies, you did brilliant! You did so great! Space babies, you need to go, okay? Get.. get out of here.
(He whistles up Eric's pushchair.)
RUBY: Okay, let's get you in here, come on. Let's get you in there. Nan-E, tell them what to do.
NAN-E: Children will return to the upper levels or have no expletive dinner.
BABIES: Goodbye.
DOCTOR: Okay, er, you... you go with them. I've got to stay here. Not just for the Tardis, but I've got to find out what that thing is.
RUBY: If that's you telling me to leave you on your own, then... Oh, Doctor. Well, come on.
(They head back through the stinky area.)
DOCTOR: Ooo! Whew! Whew! So how did this begin, Jocelyn?

[Portal 357]

JOCELYN: First I knew, six years ago, it was like a rattling in the pipes. Then the howling began. By the time I got the cameras working, there it was. The Bogeyman. I don't know how it even exists.

[Space station]

RUBY: And that was six years ago?
DOCTOR: Shh-shh-shh.
RUBY: Oh. That's the same time the babies were born.
DOCTOR: It's leaving... some sort of spoor. Man, that's a good word. Spoor.
NAN-E: What the bleep-bleep is that?
RUBY: Oh, Jocelyn, turn the filter off.

[Portal 357]

JOCELYN: What is that stuff?
DOCTOR (on screen): If I could get this to your machine, it could analyse it.
JOCELYN: The machine's got a vent in the basement. Follow the corridor. Left, straight ahead, left again.

[Space station]

DOCTOR: Into the belly of the beast. Yeah, this stuff is slippy, Rubes. Be careful.
(She slips then gets dribbled on from a pipe outlet.)
RUBY: Oh. Ah. Oh, my God. Oh, this is disgusting. Don't call me Rubes!
DOCTOR: Are we almost there, Joce? This gunk stuff is sealing the whole place off. Oh, but never mind, because... Ah! We are right under the parthenogenesis machine. Now, let's make sense of this thing. Ah, according to the machine... Oh.
RUBY: What?
DOCTOR: It has been right in front of us. We've been saying it all along. It's all one machine. One up above, and one down below. The one up above grew the babies. The one down below...
RUBY: It grew the Bogeyman.
DOCTOR: Yes!
RUBY: I said this. I told you so. Six years ago, the machine is mother and father to the babies, and mother and father to the Bogeyman.
DOCTOR: And why? Because Jocelyn said that the educational software ran out of control, and then you said...
RUBY: It's like a story. The teaching software, it told a story.
DOCTOR: It invented the Bogeyman.
RUBY: For the babies.
DOCTOR: For the space babies.
RUBY: The machine is literal, like a computer. It literally said, ""Babies need fiction, they need stories, they need monsters.""
DOCTOR: Yes. That is why I'm so scared. It's all deliberate, it's infrasound. The Bogeyman is roaring at 17 hertz, that's the exact pitch designed to make you scared. It's scary because it's meant to be. The machine made it tall and big and noisy, and it built it out of... Oh.
RUBY: What.
DOCTOR: Oh, Ruby.
RUBY: What?
DOCTOR: Oh, man.
RUBY: Tell me what it is.
DOCTOR: I can't.
RUBY: Doctor!
DOCTOR: Ruby, I have travelled the universe and back and seen many, many things. Nothing... is as bad as this. A Bogeyman is made out of what?
RUBY: I don't know.
DOCTOR: The machine is literal, and the name is Bogeyman.
RUBY: So?
DOCTOR: Oh, babes. Space babes. We saw it. The nose-blowing. The machine was literal, and so it grew the Bogeyman out of bogeys.
RUBY: What?
DOCTOR: All of this is bogeys.
RUBY: No.
DOCTOR: Yes.
RUBY: No.
DOCTOR: Yes.
RUBY: No.
DOCTOR: No wonder it was shedding its skin. Doesn't everyone?
RUBY: No, no, no, no.
DOCTOR: It's snot.
RUBY: It's not.
DOCTOR: Oh, Ruby, it is a living sneeze!
RUBY: But it's in my...
DOCTOR: I know.
RUBY: Oh, my God! This is the worst thing that has ever happened to anyone! Don't laugh!
DOCTOR: Sorry. Oh, isn't the universe mad?
RUBY: Oh yeah, it just made a monster out of snot.
DOCTOR: Oh, Ruby Sunday, Monday, Tuesday, that is... so funny.
(The monster is in front of them.)
RUBY: Bogeyman.
DOCTOR: Run. Run! Go!
(A barrier blocks their path.)
DOCTOR: No, no, no, no!

[Portal 357]

JOCELYN: Don't worry, it's me. Turn right. It's your device. It's calibrated. It's brilliant! I've got control at last. Now trust me. Turn right!
DOCTOR [on screen]: This isn't the way to the lift!
JOCELYN: Keep going.
(She unlocks doors remotely.)

[Space station]

RUBY: Ah!
DOCTOR: Go, go, go, go!
RUBY: I'm coming, I'm coming!

[Portal 357]

JOCELYN: It's catching up!

[Space station]

RUBY: Coming!
(A door slides closed between them and the Bogeyman.)
DOCTOR: Whoa!
RUBY: Yeah, thanks for using us as bait. Just next time ask!

[Portal 357]

DOCTOR [on screen]: Oh, wait until we tell you what that thing is made of!
JOCELYN: You can tell me later. Once I've got rid of it. I will protect my children and blast that thing into space!

[Airlock door]

DOCTOR: It's an airlock.
(The Bogeyman is hanging on for dear life.)
DOCTOR: It is one of the children, Jocelyn! I... She's got the sonic. Jocelyn, Jocelyn!
COMPUTER: Oxygen field at 10%.
DOCTOR: Okay, okay, okay, okay. We haven't got time. Stop Jocelyn, yeah?
RUBY: Wait...
DOCTOR: Left, second right, next left, you'll get to the lift.
RUBY: What about you?
DOCTOR: Left, second right, next left!
RUBY: Right, okay.
(She runs off.)

[Control room]

COMPUTER: Oxygen field at 9%.
POPPY: You're hurting him.
ERIC: Stop it, Nanny. Stop it!
COMPUTER: Oxygen field at 8%.

[Airlock door]

DOCTOR [memory]: I am the last of the Time Lords.
RUBY [memory]: How do you keep going?
DOCTOR [memory]: For days like this. I'm the only one of me in the whole, wide universe. No one else like me exists, and that is true of everyone.
DOCTOR: The only one of its kind.
COMPUTER: Oxygen field at 7%.
(The Doctor opens the airlock door and holds it open with his body.)
COMPUTER: Oxygen field at 6%.
(Then he goes inside, hanging on, with the Bogeyman just beyond reach.)
COMPUTER: Oxygen field at 5%. Oxygen field at 4%.
(Then he lets go, and lands on the hull between the open outer door and the big red button.)
COMPUTER: Oxygen field at 3%.
DOCTOR: Push...the button.
COMPUTER: Oxygen field at 2%. Venting reverse. Venting reverse.

[Portal 357]

(Ruby runs in and grabs the sonic.)
JOCELYN: No!
RUBY: That's what you do, Jocelyn. You save them all.
COMPUTER: Oxygen field at 1%.
(The outer airlock door is closed, the air stops rushing out. The Doctor and the Bogeyman drop to the floor.)
RUBY: You save them all. Come here. It's okay, it's okay.
(Jocelyn cries in Ruby's arms.)

[Control room]

DOCTOR: Attention! Calling Captain Poppy. Calling all crew. Especially you, Eric. Plus Ruby and Jocelyn Sancerre.
ERIC: Nanny was really naughty.
JOCELYN: I know, and I'm so sorry. All of you. I was just... on my own for such a very long time.
ERIC: We still love you, Nanny.
BABIES: Yay! We do!
DOCTOR: But-but-but-but-but-but... your favourite monster is fine. Look. Look, look, look, look.
BABIES: Yay!
(On a monitor, the Bogeyman howls like a wolf, and the babies copy it.)
DOCTOR: But listen, listen, babies, space babies, your world is over here.
BABIES: Wow!
DOCTOR: The world of Mondo Caroon. But... but you can't get there. Got no engines! Except, turns out, that build-up of pressure in Hull 3-B is from you.
BABIES: Huh?
DOCTOR: Huh? 'Cos the system went wrong, and that's where it stacked up all your nappies. No wonder it was stinking down there. For six years, a great big pile of sh...
JOCELYN: Nan-E filter.
DOCTOR: ..shizzle. A zillion metric tonnes of methane, babies. Space babies. But I am going to let it rip!
(The waste gets vented in a massive grey cloud, and the space station gets propelled out of orbit.)
DOCTOR: Oh, set sail for your new home. Baby World!
RUBY: Come here now. Are you happy now, Eric?
ERIC: I'm very, very happy. I love you, Ruby.

[Outside the Tardis]

RUBY: So that was a normal day for you, then?
DOCTOR: No, no. That was extra-special nuts. And you, Ruby Sunday, get this. Your very own Tardis key.
RUBY: What for?
DOCTOR: I have the whole universe at my fingertips, and I'm all on my own. So I'd love it if you came with me.
RUBY: To what, just travel?
DOCTOR: No job. No boss. Just fun.
RUBY: We did almost die.
DOCTOR: Yes. But we lived so much, too.
RUBY: Yes, we did. Yes, we did. Yes, yes, we did. Yep, we did, we did. We did. Yes.
DOCTOR: Yes?
RUBY: Yes.
DOCTOR: Yes?
RUBY: Yes. Oh, my God.
DOCTOR: Yes? Yes! Yes! Yes! Ruby Sunday said yes!
RUBY: Come on in. Follow me.
DOCTOR: Oh, come on.

[Tardis]

RUBY: Right, mate, let me tell you where we're gonna go.
DOCTOR: Except...
RUBY: Oh, terms and conditions.
DOCTOR: There is one thing that I can never do, Ruby. And that's take you to that church on Ruby Road that Christmas. Absolutely never.
RUBY: But you've got a time machine.
DOCTOR: If you change one thing, a single snowflake, that could change your birth mother's story and then you would never meet me, none of this would ever happen, and we would fall into the deepest, darkest paradox. Ruby, trust me. I think that snow was a warning. I can't. And I won't.
RUBY: Well, that's a pity. 'Cos I disagree. And if you let me finish... we are going to go see my mum. At Christmas. Right now. Come on.

[Ruby's home]

CARLA: (on phone) And Ruby phoned, she said, ""I love you,"" and ran off! No word from her since. What sort of Christmas is this? It all started with this man. He called himself the Doctor. Hold on. What's that noise?
(The Tardis materialises, making a hole in the kitchen ceiling. Not normal Tardis behaviour, that.)
CHERRY: What the blinking flip?
(Tardis door opens.)
RUBY: Hiya, Mummy.
CARLA: But... what are you doing? And what the hell is that? What's it done to my kitchen?

[Tardis]

RUBY [OC]: Hey! Come and say hello!
DOCTOR: Yes. Coming. Tell your mum not to slap me.
(He sonicks up a quick DNA scan of Ruby Sunday.)
DOCTOR: Yes, now, the people from my world, they use titles like the Bishop, the Pedant, the Sagi-shi. My name was...
(He doesn't wait for the results of the scan.)"
16,https://stardewvalleywiki.com/Version_History,"Version History
(Redirected from Version history)
Jump to navigationJump to search
Stardew Valley was released on 26 February 2016 for Windows PC after almost four years of closed development. Linux and macOS compatibility was released in a free update on 29 July 2016.

For a detailed breakdown of changes specific to console versions of the game see Console Version History.
For a detailed breakdown of changes specific to the mobile version of the game see Mobile Version History.

1.6.8
Stardew Valley 1.6.8 was released on 28 April 2024.

Bug fixes
Fixes a game-breaking crash introduced in 1.6.7 where the game would crash if NPC's reach the island resort changing room.
1.6.7
Stardew Valley 1.6.7 was released on 27 April 2024.

Bug fixes
Fixes a bug preventing players from giving Pierre the Missing Stock List
Fixed some machines and desert festival logic not working for Linux/MacOS players using the compatibility branch.
1.6.6
Stardew Valley 1.6.6 was released on 26 April 2024.

Balance and Gameplay changes
Bee houses now work with flowers in garden pots.
“Minecart depot” alternative layout now only appears in the upper or lava mine areas.
You can now turn the gold clock on or off
Added coal -> mahogany seed trade at raccoons
50% mastery XP now only applies to farming, as originally intended.
Translation changes
Reverted Chinese text to 1.6.3
Reverted Chinese default font to the original
Added an option to use the smooth font introduced in 1.6.4
Added a dialogue font size slider for Chinese
Added complete Russian translated movie & desert festival sprites
Added option for Russian to use the “old” font
Bug fixes
Fixed a case where Mr. raccoon could disappear during the raccoon cutscene in multiplayer
Fixed being able to trash Pierre’s Missing Stocklist, or give it away at the Feast of the Winter Star.
Fixed being able to give a wilted bouquet to your spouse.
Fixed issue causing only 1 omni geode to be produced from the skull cavern chest.
Fixed crash when a save contains some invalid animal home data.
Fixed a crash when talking to Penny in French.
Fixed a malformed Willy farm event in German.
Fixed an incorrectly blocking map tile in the Forest Farm map.
Fixed some machines and some desert festival logic not working for Linux/macOS players using the compatibility branch.
Fixed a case where an area was inaccessible in the volcano dungeon
Fixed sewer event in German
Fixed cases where you’d see Mayor Lewis’ giftbox note again when collecting 15 parsnips from a different chest.
Fixed emily makeover event from being hard to skip
Fixed Calico Egg Rating prize edge cases for farmhands
Fixed prismatic hat effects not working in French.
Fixed being able to “advance” the ready check menu by pressing Y, which lead to many issues.
Fixed island outfits not being loaded for farmhands
Minor optimizations.
Fixes for modded players
Fixed error showing non-flavored roe items.
Fixed error if a mod sets an NPC’s ‘friends and family’ data to null.
Fixed error loading a save containing enchanted weapons whose ID changed or data was removed.
Fixed soft lock when watching weather TV channel if tomorrow’s weather is custom.
Fixed temporary maps that aren’t part of a location not using seasonal tilesheets.
Fixed summit cutscene showing custom fish with wrong sprites.
Changes for mod authors
In Data/Objects, added two new fields: CanBeGivenAsGift and CanBeTrashed. Setting CanBeGivenAsGift: false adds the not_giftable context tag automatically.
1.6.5
Stardew Valley 1.6.5 was released on 20 April 2024.

Bug fixes
Fixed crash when loading dark mine levels in some cases.
Broken villager NPCs (usually due to a removed mod) are now deleted on load. They'll respawn when they're re-added to the data.
1.6.4
Stardew Valley 1.6.4 was released on 18 April 2024. A hotfix was applied on 19 April 2024.

New content & features
Added 20 new 'alternate' mine layouts, which can start appearing after reaching the bottom of the mines.
Added 20 new volcano mines layouts, which can appear after you've unlocked the shortcut between the caldera and the volcano entrance.
Added fish frenzies.
Added a special cutscene after you've helped your new neighbors grow their family to the max.
Added 4 new fairy styles.
You can now place 8 additional non-fish items into fish tanks.
Raccoon shop now includes a trade for mystery boxes and golden mystery boxes.
Vinegar can now be poured onto trees to stop them from ever growing moss.
Shaving enchantment now causes tree-specific drops (fern → fiddlehead fern, mushroom → red mushroom, mahogany → hardwood).
Visual improvements
Qi club coins now show up in the shop menu.
Various map edits and fixes.
Removed Maru's glasses from all beach portraits for consistency.
Translation changes
Many edits and improvements to the Chinese version.
Edits to the Russian, Portuguese, Hungarian, Turkish, and Korean translations.
Changed the Chinese fonts to make them more appealing and readable.
Changed Hungarian number spacing character from a comma to a period.
Some improvements to Russian font.
Balance changes
When you load a save, you'll now unlock missed Steam achievements if needed (e.g. achievements gained while playing offline).
Added mini-shipping bin as an expensive Qi gem shop purchase.
Added more variety, and improvements, to Skull Cave treasure chests.
Life elixirs now only restores health, and energy tonics now only restore energy.
Wild seed spots are now rarer, but yield more seeds.
Doubled the Iridium Golem's chance to drop iridium.
The monsters in the volcano entry level are now randomized each day.
Using a treasure totem indoors no longer works.
Going down a level in the mines now makes you invulnerable for 1 second.
Increased raccoon mystery box reward from 4 to 5.
Farming experience now contributes to mastery experience at a 50% rate.
Ice orb will no longer freeze a spider while it is in the air jumping.
Removed red snapper from winter fishing quest pool, and sardines from the summer quest pool.
You will no longer get calico rating points from going down the normal mines.
Golden mystery boxes now have a small chance to yield an auto petter.
Slightly reduced chance to find mystery boxes.
Mine barrels and coal carts will now ""refresh"" at the start of each year.
The order of mystery box rewards is now randomized per-player instead of per-save.
Quality of life changes
Added a 1.2-second delay after dropping an item, before it's picked back up by the same player.
Napalm rings are now non-destructive on the farm and inside the slime hutch (i.e. the explosion will only damage monsters).
Added ctrl + right-click as an alternate toolbar drop hotkey.
Receiving a ""special notification"" (e.g. first geode message) will no longer cancel out player actions such as eating or warping.
Flopping fish no longer fall back into fish ponds.
Crab pots now have a short time period after harvesting from them during which they can't be removed (750ms).
The Junimo bundle menu now only highlights items that can actually be placed in the selected bundle.
Other changes
Minor optimizations.
Added two inappropriate names to the list to exclude from the name generator.
You can no longer give gifts to NPC's during green rain year 1 (prevents incongruous attitudes). This also prevents a portrait issue with Demetrius.
Gameplay fixes
Fishing splash zones and ore pan points are now removed overnight.
The prize ticket machine now consumes your ticket when you get the reward, rather than when you first press the button.
For the purposes of perfection, the level 100 stardrop now has a more robust way to check if it's been acquired. This solves a problem where the flag for eating the lvl 100 stardrop could be missing.
Fixed Monster Compendium not actually doubling all loot.
Fixed geodes no longer dropping on the farm in normal circumstances.
Fixed rare crash when spawning items overnight.
Fixed crash when a certain mine level is chosen (_dark_dark crash).
Fixed player colliding with stuff during event cutscenes.
Fixed pre-1.6 forged weapons having reduced stats.
Fixed not being able to put a torch on the sprinkler you get as a CC reward.
Fixed disappearing pets bug.
Fixed various bugs/crashes related to building a cabin.
Fixed wild seeds in garden pots putting their final crop in the top-left corner of the map instead of the pot.
Fixed ""lost and found"" chests (from night market and elsewhere) not working anymore in 1.6.
Fixed a duplication bug with the ""lost and found"" chests.
Fixed slime hutch floors reverting to the default after reloading a game.
Fixed malformed casino shop red fireworks ID.
Fixed pet adoption catch-all case only working with the animal catalogue, and only applying in year 2 specifically, rather than for any year 2 or beyond.
Fixed 'use legacy randomization' option applied inconsistently when loading a save, which caused weird issues like broken green rain days.
Fixed the case where you could permanently miss the ancient seed recipe if you grabbed the ancient seed packet but not the associated recipe, and then closed the museum rewards menu. Missing recipes will also be restored upon loading a file.
Fixed issue with duplicated animals & building interiors from pre ~1.3 saves.
Fixed end tables not being rotateable.
Fixed an issue where empty barns/coops could not be removed by Robin on old saves.
Fixed slime egg incubators letting you reclaim the egg before it hatches.
Fixed farmhouse placement bounds to match the object placement restrictions around it.
Multiplayer fixes
Synced some NPC Gift data in other languages.
Fixed fireworks (and possibly other sprites) drawing behind stuff in multiplayer.
Fixed ""Build a Silo"" quest not being completed for farmhands.
Fixed Raccoon quest not being removed for all players.
Fixed double trinket issue when a farmhand disconnects/reconnects.
Fixed double and/or missing derby participants sprite issue in multiplayer.
Display text and localization fixes
Many edits and fixes in translation text.
Fixed a text parsing error for Asian languages which caused them to incorrectly wrap in some cases.
Fixed spacing issues in Asian languages.
Fixed several localization issues (day time money box extra space in Chinese, missing Elliott sentence opener, Fizz name untranslated, birthday string de-hardcoded, etc).
Fixed Russian line break issue.
Fixed an issue with the perfection tracker display in Chinese, Korean, and Japanese.
Fixed events not consistently handling gender-dependent text.
Cosmetic fixes
Adjusted basic object layer depth anchor point so it's more centered rather than on the bottom.
Placeable grass now offsets vertically a little if it would clip through a front-layer tile.
Fixed seasonal world map not working for non-English players.
Fixed flying monsters not being draw above things on the farm & volcano.
Fixed reward icon in completed quest menu being visually offset.
Fixed george's TV clipping with farmer during green rain.
Fixed movies not showing layered sprites (e.g. the eyeball in Mysterium).
Fixed Qi club coins drawn below the lightmap.
Fixed Qi coin overlay drawn during minigames.
Fixed crane game audio not stopping when the game is forcefully closed.
Fixed some tooltip padding issues.
Fixed movie theater display issues on the world map.
Fixed repeated dialogue selection sound if a control was plugged in but you are using a mouse.
Fixed quest reward icon input offset issue.
Fixed Joja Warehouse roof drawing above the weather (and possibly other similar cases).
Fixed various map tile issues.
Fixed a visual issue with a shadow in the farm house.
Fixed able to swap two chests of the same type.
Fixed machines sometimes wobbling when they're not processing anything.
Changes for modded players
Improved debug commands:
Added debug commands to change pet types/breeds, to allow migrating older saves (setPreferredPet and changePet). This replaces the former toggleCatPerson command.
The filterLoadMenu <search> command now keeps the original save slot numbers.
Combined removeNpc and killNpc commands. The unified command removes all instances of an NPC, to help fix duplicate NPCs.
Changes for mod authors
C# mods can now hide specific animals from the animal social menu (e.g. animals owned by NPCs) via the FarmAnimal.hideFromAnimalSocialMenu field.
In Data/Buildings, you can now add furniture via IndoorItems.
In Data/FarmAnimals, added fields to set the sleep sprite and whether they eat golden crackers.
In Data/Locations, added fields to set effect of fish-specific bait and whether fish can be caught using a training rod.
In Data/Machines, added option to only let the machine complete overnight.
In Data/Powers, added CustomFields field.
Added missing Id fields in Data/FarmAnimals and Data/Pets.
Added a warning when a farm animal's spritesheet size is invalid, which would previously lead to the animal silently freezing.
You can now use tokenizable strings in event scripts and dialogue text.
De-hardcoded forest waterfall fish into Data/Locations.
Fixes for modded players
Fixed some wallpapers and flooring not appearing in mod item lists.
Fixed pre-1.6 farm maps having broken warps to the bus stop.
Fixed errors and crashes due to mods adding items without a name.
Fixed softlock when a warp leads to an invalid location.
Fixed crash when playing events that use the replaceWithClones command.
Fixed crash if an NPC has invalid temporary dialogue.
Fixed world map showing a glitched texture for custom farm types which don't override it.
Fixed another case of NPC duplication involving the movie theater.
Fixed crash when loading a save if a farm Junimo wasn't properly removed before saving.
Fixed crash if the lost & found contains null items.
Fixed disconnect for Steam players when using mods that send messages before a farmhand has been approved.
Fixed crash on save if shipping bin has empty slots.
Fixed riding the bus to the desert instantly warping you back if mods added warps on the desert road.
Fix multiplayer crash if some players don't have the same NPC textures.
Fixes for mod authors
Fixed farmhands' cellars not applying cellar data from Data/Locations.
Fixed crash if a farm animal has no entry in Data/FarmAnimals.
Fixed festivals ignoring year suffixes for <setup>_additionalCharacters and name fields.
Fixed issues with farm animals which have a custom home building.
Fixed hardcoded assumptions about farm animal spritesheets which could cause counter-intuitive behavior with custom animals.
Further changes were later released without changing the version number:
19 April 2024 ""hotfix #1"" (Steam announcement)
Fixes issue when dropping a big chest into a small chest
Quarry mine level can no longer be an alternate layout
Prevents the player from running through barriers in certain transit cutscenes
Mine levels 45 and 85 are now also refreshed at the beginning of the year
1.6.3
Stardew Valley 1.6.3 was released on 27 March 2024.

New content & features
You can now access Clint's shop and geode service while your tool is being upgraded.
Added Krobus portrait.
Added minecart shortcut to year 1 Spirit's Eve festival.
You can now paint the new cabins (at third upgrade level).
Balance changes
Increased raccoon bug meat and bat wing request amount.
Harvesting moss now grants 1 foraging exp per moss.
The Infinite Power achievement is now more generous with how it activates, allowing pre-1.6 saves to trigger the achievement.
You now have to collect (not just donate) the four prismatic shards for the ""Four Precious Stones"" Qi quest.
The Junimo Kart Qi quest now rewards 20 Qi gems (up from 10).
Adult mossy green rain trees boost the moss growth rate of nearby trees.
Moss now grows more slowly on sunny summer days, and recovers more slowly after harvest.
Reduced meteorite prismatic shard chance (100% -> 25%).
Ancient seed packets can no longer be sold at the traveling cart.
Quality of life changes
Waterfall and some other ambient positional sounds now fade out more smoothly.
If you don't have a pet in year two, you can now start adopting them from Marnie's shop.
Bug fixes
Added the new cabin styles to the multiplayer starter layouts.
Starter cabins now appear in the actual order intended.
If a journal quest title is too wide, the clock icon and the rest of the text will be shifted down to make room.
You can now press the Escape or menu button to ""go back"" on a shipping menu category page.
Made berry bush state consistent overnight and on load (both at 20% plus a small new random bonus chance).
Minor graphical fixes.
Fixed glitch where farming speed is changed depending on the player's direction.
Fixed several translation errors and missing translations.
Fixed large character sprites having a visual glitch (e.g. bear, island frog, or Clint when hammering).
Fixed a visual error in the shop menu if an item's name was longer than the width of the menu.
Fixed a crash related to the new festival.
Fixed a bug that caused excess trash to be fished.
Fixed bug preventing Emily's 10-heart mail and event from triggering.
Fixed graphics issues with the shipping menu.
Fixed several more cases of out-of-bounds spawning.
Changes for mod authors
Fixed items able to have a null Name or DisplayName in 1.6. They now default to the qualified item ID and ""Unnamed Item (<id>)"" respectively.
The ${...}$ gender-switch syntax now works in events.
The character.EventActor flag is now synced in multiplayer.
Added paint masks for the new cabins style.
The debug whereis command now lists event actors too.
Fixed pet license name delimiter conflicting with unique string ID format.
Fixed NPC duplication bug with movie theater.
Fixed fruit trees above growth stage 4 no longer producing fruit.
Fixed current location's building interiors not being actively synced.
Fixed error processing a machine with no Data/Machines entry.
Fixed broken fruit tree sprite if it's synced from another player and has a custom texture in Data/FruitTrees.
1.6.2
Stardew Valley 1.6.2 was released on 21 March 2024.

Balance changes
Added scaling edibility to juice, jellies, wine, and pickles.
Void egg is now poisonous.
Bug fixes
Fixed an issue with Willy's rod.
Fixed for invisible tea sprites.
Fixed an error in the Luau event in Portuguese.
Fixed visual error with pre-1.6 basic wines.
Fixed a soft-lock if you charged a tool and then strafed to a warp spot, and released the tool while warping.
Fixed inedible ingredients being poisonous after turning into jelly or pickles.
Fixed Volcano dwarf bombs not matching regular dwarf bombs.
Fixed adventure guild open hours on map.
Fixed various translation issues.
1.6.1
Stardew Valley 1.6.1 was released on 19 March 2024.

Bug fixes
Improved cases where the quest title could go off the edge of the quest log.
Fixed botched German egg quest text (for new players going forward).
Fixed animal page crash when clicking on the scroll bar when there are a low number of animals.
Fixed Junimo harvester in save crash.
Fixed Meadowlands farm quest persisting if the farm icon was selected, but then another farm was started.
Fixed the telephone Robin house upgrade question being malformed.
Fixed eyebrow accessory looking weird when fishing.
Fixed errors due to clipboard being set to a null string.
Fixed crash when farmhand reconnects to town during green rain (and potentially other related crashes).
Fixed several map tile issues and places where forage could spawn out of bounds.
1.6
Stardew Valley 1.6.0 was released on 19 March 2024.

New content & features
Added new festivals and events:
The Desert Festival is a three-day event in spring which can be accessed after the bus is repaired.
Two mini fishing festivals: Trout Derby and SquidFest.
A new environmental ""event"" in summer.
Added a mastery system, accessed via a new area, which grants powerful perks and items…
Added a new farm type: Meadowlands Farm. It has a chewy blue grass that animals love. You also start with a coop and 2 chickens.
Added many new NPC dialogues. That includes...
custom gift reactions;
dynamic dialogues which react to things that happened;
custom flower dance acceptance dialogue;
restored missing dialogue (like Emily and Shane's flower dance custom accept-dance dialogues when married to the player, Shane's dialogue when browsing the Saloon vending machine, Lewis congratulating female farmers after their wedding, a line in Maru's 14-heart event, three dialogues from Sam related to Kent, two randomized dialogues when an NPC buys an item you sold to a shop, two speech bubbles from Marnie and Robin when you enter their shop building, and a fortune teller dialogue for your potential spouse);
and more.
You can now get multiple pets (after getting max hearts with your starter pet).
Added a world map for Ginger Island, visible when visiting the island.
The world map now shows your actual position within the world in real-time (instead of showing you at a fixed point for each location). In multiplayer, you'll see other players' positions in real-time too.
Pets that love you will sometimes give you gifts.
NPCs now have winter outfits.
Festivals now have map and dialogue changes every second year (except the night market and desert festival).
Added a golden Joja parrot, which you can pay to find all remaining golden walnuts on Ginger Island.
Added perfection waivers, a new Joja way to bypass perfection challenges.
Added a prize machine in Lewis' house. You can collect prize tickets as a reward for completing quests and special orders, and from repeated egg/ice festival wins.
A bookseller now comes to town twice a season.
Added mystery boxes.
Added a Big Tree, with a quest line which ultimately gives you some new neighbors.
Added four new crops (carrots, summer squash, broccoli, and powdermelon) which can't be purchased at the store, and two new giant crops.
Added four new home renovations: dining room, attic, expanded corner room, and cubby.
Added new items:
Big Chest, which has almost double the size of a regular chest. It can also be placed ""onto"" a regular chest to upgrade it.
Dehydrator, which turns fruit into dried fruit and mushroom into dried mushrooms.
Mushroom Log, which produces mushrooms and interacts with nearby trees.
Bait Maker, which can produce fish-specific baits.
Heavy Furnace, which can process more bars at a time, and yield bonus bars.
Fish Smoker, which produces smoked fish, doubling the value of the fish. You get one by default when starting a new riverlands farm.
Text Signs, which can be written on.
Anvil, which allows you to re-roll trinkets.
Mini-Forge, which acts as a Dwarvish Forge.
Statue Of Blessings, which grants a random blessing each day.
Statue Of The Dwarf King, which allows you to select one of two mining buffs for the day.
Tent Kits, which allow you to build a tent which can be slept in for one night.
Treasure Totems, which spawn a ring of diggable spots.
Mystic Tree Seeds, which grow a unique tree which can be tapped.
Mystic Syrup, a valuable tapper product.
Deluxe Bait, gets fish biting faster than regular bait.
Challenge Bait, which allows for up to 3 fish to be caught at once, but loses 1 each time a fish leaves the bobber bar.
Deluxe Worm Bin, which upgrades the regular worm bin to produce deluxe bait.
19 unique Books Of Power, which grant special perks.
Skill Books, which grant experience in a skill.
Book Of Stars, which grants experience in all skills.
Moss, a new resource type which grows on old trees.
Mixed Flower Seeds.
Sonar Bobber, which shows the fish on your line before you catch it.
Raisins, which have a special use.
Sea Jelly, River Jelly, and Cave Jelly, a new item that you can fish.
7 Trinkets, which grant powers related to combat.
Red, Purple, and Green Fireworks.
Stardrop Tea, which makes an excellent gift for anyone.
25 new hats.
280 new furnitures.
New unique furniture catalogues, which contain themed furniture sets.
41 new floor styles.
24 new wallpaper styles.
Golden Animal Crackers.
Mannequins, which can be dressed.
Spouse Portraits which can be purchased after reaching 14 hearts.
Butterfly Powder, which allows you to remove pets...
Blue Grass Starter.
Moss Soup.
Secret items.
Added Goby fish.
Added some new remix bundles.
You can now place hats on cats and dogs.
You can now upgrade the copper pan into steel, gold, and iridium pans.
You can now enchant pans with Archaeologist, Generous, Fisher, and Reaching.
Added a ""Special Items & Powers"" tab to replace the wallet. The wallet area now tracks a selection of progress markers.
Added an animals tab that shows all your pets & animals.
You can now build pet bowls in Robin's Shop, with three variants.
The farmhouse and pet bowl can now be moved through Robin's menu.
The farm computer can now be used anywhere to see a summary of that location, instead of only the farm.
The mini-jukebox can now be used on the Ginger Island farm.
Added a new interaction with your horse.
Added a new side-tunnel to the Quarry Mine.
The Community Center fish tank now becomes an actual fish tank when you complete it.
Added more secrets & easter eggs.
Added two new cat and dog breeds.
Added turtle pets.
Added 8 new achievements.
Added 4 new cabin variants.
Added a few more accessory options in character creation.
Added a new bobber machine in Willy's shop, with 39 bobber styles to choose from. New styles unlock by catching new kinds of fish.
Added a cameo appearance to Maru's 14-heart event.
Emily has a new rare ""socialize"" daily quest if you've completed the introduction quest.
You can now add anchors, treasure chests, and pearls to fish tanks.
Pierre now sells a few random items at the winter star booth (at a markup!).
Added a jingling sound when running with the cinderclown shoes on.
Baby toss now has a chance to crit.
Added a skull cavern statue that can be used to toggle hard mode in the skull cave (after completing Qi's challenge).
Added additional chests to Skull Cavern levels 200 and 300.
Added unique skull cavern chest appearance for level 100, 200, and 300 chests.
Added a high note (C5) to flute block.
Added Meowmere from Terraria crossover
Added iridium golem to wilderness farm.
See changes for mod authors.
Visual improvements around the valley
Added waterfalls.
Added more holiday decorations in winter.
Added more pathstones to various maps.
Added jack-o-lanterns after the Stardew Valley Fair in fall.
Added seasonal world map variants.
Added a new rare ambient critter.
Added some rare summer butterfly variants.
Added an uncommon little brown bird variant.
Redrew the world map to better match the in-game locations and be more detailed.
Boat journey textures are now seasonal and reflect the latest valley map.
The bus stop now has a wider map, though the distance to traverse it is the same. (This is to prevent black bars from appearing at the sides of the screen.)
Jelly, pickles, wines, and juices are now colored based on the ingredient item.
Many town trees are now actual tree objects, though you can't cut them down.
Slight adjustment to the way items pop out when dug from the ground.
Updated volcano gold ore node sprite.
Some trees have a chance to lose their leaves in the fall.
Riverbanks and lakeshores in the mountain, town, and forest areas are now less jagged in some places.
Graphical improvements to building interiors.
Improved the art of George & Evelyn's roof.
If you destroy a mines chest, it now shows some graphics debris.
Added special backplates to fortune teller TV show if you get a perfectly good or perfectly bad luck day.
Lighting changes:
It now gets dark an hour earlier in winter.
Night tiles (e.g. town lamps) now activate an hour earlier in all seasons.
Indoor daytime lighting now smoothly transitions to night lighting over the course of two hours.
Night lighting in non-farmhouse indoor locations is now slightly darker.
Farmhouse lighting on rainy days is now slightly moody, and lights stay on all day.
TVs and trees of the Winter Star now give off light at night.
Added light sources to window light glows so there are no more ""dark but lit"" windows.
Made some improvements to the intro bus drive cutscene.
At night, hats are now drawn at the night-time color in the game menu portraits.
Removed lighting quality option. It's now permanently set to ultra quality.
The submerged fishing bobber is now recolored automatically to match the water.
Multiplayer changes
You can now have up to 8 players on PC.
Many improvements for multiplayer performance and stability. That includes:
Steam players will now use Steam multiplayer authentication, potentially improving connection issues significantly.
Large multiplayer packets are now compressed, reducing bandwidth usage and latency.
Internal optimizations to data syncing.
You now need the same build number (in addition to version) to join a multiplayer server. This prevents crashes due to game changes between builds.
Accepting a Qi challenge that increases mine difficulty now only kicks other players out of the affected mine type, not all mines.
Purple shorts no longer show a chat message when placed into the Luau soup.
Jump down mineshaft sound now plays for all players in the level rather than just the jumper.
Balance changes
Added a box with three tent kits to the Ginger Island jungle.
Weapons found in the wild now have a chance to come with a basic ""innate enchantment"". You can re-roll ""innate enchantments"" (if the weapon could have one) at the forge using a dragon tooth.
Slime hutches are now significantly smaller (11x6 → 7x4).
Farm animals now gain a little happiness if you close the animal door behind them at night.
Grass now survives in the winter, though it won't spread. However, cutting grass during winter is much less effective.
The mushroom cave now comes with a free Dehydrator.
Changed recipe skill requirements for:
charcoal kiln (foraging 4→2);
cookout kit (foraging 9→3);
survival burger (foraging level 2→8);
tapper (foraging 3→4);
and worm bin (fishing level 8→4).
Price changes:
Most home renovations now cost money, which is refunded if you undo the renovation. (The game knows whether you bought it though, so refunding a pre-1.6 renovation won't give you free money.)
Reduced fairy dust sell price (500g → 300g).
Reduced tea sapling sell price (500g → 250g).
Reduced life elixir sell price (500g → 250g).
Building cabins no longer requires materials, only the 100g price.
Raised price of second house upgrade (50,000g → 65,000g), but reduced number of hardwood needed (150 → 100).
Reduced worm bin's hardwood needed (25 → 15).
Increased cost of warp totem: farm in casino (500 → 1000)
Raised price of bombs in dwarf shop.
Raised some hat prices from Hat Mouse.
Shop changes:
Put limits on some casino stock.
You can now buy all brazier recipes in Robin's shop at once instead of in sequence.
Item drop changes:
Chopping down a fruit tree now yields the appropriate fruit sapling. If the tree is mature (i.e. the fruit quality is > basic), it will yield a sapling with the same quality as its fruit. The higher the quality, the faster the sapling will mature.
Chopping down a tea bush now gives back a tea sapling.
There's now a small chance to find cosmetic items and other goodies while doing random tasks.
Snake vertebrae are now easier to get.
Train cars which carry wood can now drop hardwood.
Santa's train car can now drop gifts.
Reduced prismatic shard drop rate from iridium nodes (4% → 3.5%).
Rare yellow slimes now drop money.
Brown slimes now drop wood (doesn't affect the copper slimes in the quarry mine).
Botanist perk now applies to items dropped from trees (e.g. coconuts).
Reduced chance of fishing void mayonnaise at the witch swamp.
Gift taste changes:
Adjusted gift tastes for several NPCs.
Treasure chests are now a universally liked gift... except by Linus.
Skill XP changes:
Mushroom Logs and Mushroom Boxes now grant 5 foraging XP on harvest.
Harvesting berry bushes now grants 1 foraging XP per berry.
Harvesting forage crops from wild seeds now give much less foraging experience, but grant some farming experience.
Monsters on the farm now give combat experience, but it's 1/3 of normal value. This excludes slime hutch slimes.
Adjusted combat:
Extended the area of effect of downward facing melee attacks (and slightly extended the side attack of daggers).
Topaz ring now gives +1 defense, rather than the unused precision stat.
Raised insect head's damage (10-20 → 20-30).
Raised kudgel's critical attack power (+4 → +50).
Bombs now affect terrain features (like trees and crops) within the round explosion radius rather than a square area.
Slightly increased rate at which skeletons throw bones or shoot spells.
Adjusted Junimo Kart:
Added grace jumps in Junimo Kart: when you run off the track, you can still jump for a short time to recover.
Your score is now saved if the minigame forcibly exits while playing endless mode.
Noxious gas emitting mushrooms no longer appear in pairs.
Reduced bubble spawn rate on whale level.
Adjusted machines:
Worm bins now need a lower fishing level (level 8→4) and produce more bait (2–5 → 4–5).
Loom now has a higher chance of double cloth when processing quality wool.
Fish ponds now have a chance to produce extra roe whenever they produce roe.
Geode Crushers no longer require coal to operate.
Adjusted penalties when knocked unconscious:
You can no longer lose the Golden Scythe, Infinity weapons, or tools.
You can no longer lose more than three items.
The amount of money lost now scales to how much you have. It's now less punishing if you don't have much money, but more punishing if you have a lot. This also raises the maximum lost from 5000g to 15000g.
Adjusted food buffs:
Joja cola now gives a very short speed buff.
Green tea now gives +0.5 speed.
Mine and dungeon changes:
Added coal nodes to the volcano dungeon.
Barrels now spawn on skull cavern levels divisible by 5.
Reduced the maximum possible effect a bad luck day can have on finding a prismatic slime.
Reduced number of bugs to kill for monster slayer goal from 125 to 80.
Bundle changes:
Made remixed specialty fish bundle (and analogues) reward 5 Dish O' The Sea, to make it consistent with the classic bundle.
River fish bundle now gives deluxe bait.
Improved some community center rewards.
Adjusted crafting recipes:
speed gro now requires 5 Moss instead of 1 Clam .
deluxe speed-gro now requires 5 bone fragments instead of 1 coral.
quality fertilizer now requires 4 sap instead of 2, but produces 2 per craft (still only requires 1 fish).
Spouse changes:
Spouses now have a seven-day ""honeymoon"" period after marriage which prevents them from laying in bed all day due to being upset.
Kissing your spouse, and giving them a gift on the previous day, each reduce the ""minimum heart level threshold"" for a bed-ridden day by one heart (12→10 if you do both).
Friendship gain is reduced by 33% for spouses.
Rebalanced the crop fairy event: the chance no longer depends on the number of planted crops, it can no longer happen on the last day of the season (to avoid growing crops that might die overnight), and it can no longer choose dead crops to grow.
Increased the shaving enchantment's effect on giant crops.
Each giant crop now has a 60% chance of dropping an extra six crops while the shaving enchantment is equipped, spread across the number of hits needed to break it. For example, a base axe which breaks the giant crop in three hits gets three 20% chances of dropping 2 crops.
The mushroom cave now provides mushrooms every second day. It was unintentionally changed to daily in Stardew Valley 1.5.
You can no longer plant trees in the beach farm tunnel.
Randomization no longer produces simple repeating patterns in many cases (e.g. clay farming, mushroom level prediction, crab pot fish offset, etc).
(You can enable ""Use Legacy Randomization"" in the advanced save options to use the old randomization, though some specific patterns may still change due to the underlying changes. That option is mainly intended for speedrunners; most players should keep it disabled for the intended experience.)
Spreading weeds can no longer destroy artifact spots.
Increased the number of monsters that daily monster quests will ask you to slay (in some cases). Added more custom quantities, for example Dust Spirits will ask for 10-20 kills.
Tilled dirt on the island farm now decays in the same way as the regular farm.
Slightly increased time you have to push against farm animals before passing through them.
Slightly boosted quarry output. Daily quarry output now increases each year, up to a limit.
You can no longer plant trees in town.
Secret notes are no longer created during festivals (except passive festivals like the night market and desert festival).
Adjusted fish variety in ice fishing festival.
Quality of life changes
Performance improvements.
NPCs now shove chests out of their way instead of destroying them.
If Pam won't be coming to the bus for any reason, she now leaves a sign informing you and you can drive yourself to the desert.
Audio changes:
Made more sounds positional (mainly players, tools, and machines).
Positional sounds now fade with distance when off-screen, instead of cutting off abruptly.
Softened the bomb fuse sound.
The music now ""ducks out"" and then resumes when certain sounds are played, instead of stopping.
You can now ""strafe"" while charging a watering can or hoe, allowing you to reposition your tool hit area without changing your facing direction.
You can now refill slingshot ammo by right-clicking it with the same ammo. Previously that would just swap the item stacks.
Planting cactus seeds on the farm now fails with a message, instead of the seeds dying overnight.
Holding a tea sapling or seed over a garden pot now shows the green/red placement tile.
You can no longer pick up rugs if there's something on it.
Checking a pet bowl will now show a text bubble with the pet's name.
Added a new post-fishing sparkling text to indicate when you've caught something for the first time.
Torches can now be placed on sprinklers.
You can now sit in chairs during festivals.
You can now move filled chests by hitting them twice with a heavy tool (previously only with bare hands). The chests will shift one space at a time.
You can now place flooring underneath most buildings.
Crystalariums now have to be removed and replaced before a different gem can be put inside, to help avoid wasting gems accidentally.
Daily billboard quests now have a more informative tracker notification when you make progress on them.
Added a small checkmark icon on special orders you've completed before (only on town special orders board).
You can now skip the pet adoption scene, which causes you to adopt the pet.
Reduced the amount of time you need to push against a pet before they start shaking and then let you pass through them (1.5 → .75 seconds).
Reduced time for mini-obelisk warp (750ms faster).
Male farmers are no longer forced into wedding clothes on their wedding day, so you can choose your own outfit for the wedding.
Emptying a fish pond with fish still in it will cause the remaining fish to flop out of the pond.
Slime hutch changes:
You can now change the flooring of the slime hutch.
You can now remove the starter incubator in the slime hutch.
Slime balls no longer appear on crafted flooring.
UI improvements:
Added an hourglass cursor shown when you're waiting for something to load on the title screen.
Notification messages in the bottom-left corner now last 50% longer before going away.
""Sound in the night"" events now show an icon on the screen to indicate that a sound is playing.
Dialogue question selectors no longer remain selected when you hover away from them, to reduce accidental selections.
Robin's building menu now shows how many days a building will take.
If an item menu exits while you're holding an item, the item is now always retrieved. (Previously only some item menus like chests would do it.)
Marnie's animal shop now centers the camera on an appropriate building when purchasing an animal.
Marnie's animal shop now shows prices in the tooltip, in case the tooltip covers the price display.
The achievement menu now lists all potential achievements. Hidden achievements you haven't unlocked yet are shown as ""???"".
The museum reward menu now prevents picking up a reward that won't fit in your inventory.
The museum reward menu now lets you exit while holding a reward. It'll be added to your inventory if possible, otherwise it'll drop at your feet.
The save creation farm selector now has two columns instead of one, and no longer overlaps the back button on small screens.
The shipping menu category pages no longer take up the whole screen, as long as the screen is beyond a minimum size.
Inventory tooltips for food that gives a buff now shows the buff duration in the tooltip.
The map now closes when you press the map button again.
Exiting the Junimo Note menu from within the game menu now returns to where you were in the game menu rather than exit the menu.
Shops now truncate item names which would overflow the menu width.
Shops now have a slight delay before you can buy/sell items, to help avoid double-clicks doing it accidentally.
Deleting a save on PC is now much faster.
Significantly reduced save loading time when there are many custom locations.
If a default farm building (like the greenhouse) is somehow removed, it'll now be rebuilt next time you load the save.
""Menu background"" is now a drop down option, which includes ""Standard"", ""Graphical"", or ""None"".
Attempting to put something in a machine but failing will no longer cause you to try and consume or activate the object you're holding.
Kent intro event and Robin flute block event are now skippable.
(PC) You can now shift + right-click an item on the toolbar to throw it out of your inventory (if possible).
(PC) You can now press the Y or N key to confirm or cancel the ""leave festival"" confirmation box.
(PC) Holding left-shift + left-ctrl + 1 when buying from a shop will attempt to buy a stack of 999.
Other changes
Adventure guild now stays open until 2am, though the music will not play after midnight.
Gender-specific clothing variants can now be worn by any gender.
If you have 12 hearts or more with your spouse, the chance they'll say a neutral dialogue in the afternoon (which sometimes comes across as negative) is significantly lower (25% → 5% chance).
Penny's forest picnic event and Leah's forest picnic event now only happen if it's sunny.
NPCs now try to avoid walking through trees and other terrain features.
Cabins have been combined into one entry in Robin's menu.
Replaced pet icons on the inventory tab with the current date.
The “Organize” button now sorts items in a more intelligent way.
You can now drink mayonnaise and jelly, and eat pickles.
Some colored objects now count as their color for qi quest and dye menu.
Clam is now considered a ""fish"" just like all the other shellfish.
Added more descriptive titles to daily quests.
Added Cyrillic sprite text and some translated world pixel art text.
Adjusted the date/time/money box in Chinese.
The about page now shows the build number.
The Skull Cavern now have a chance to also play music from the upper mines.
Changed parrot flap sound to be different from bat flaps.
Pans now have a chance to yield bone fragments.
Made the character randomization a little less random, and added some of the newer hairstyles and accessories to the mix.
Pan no longer yields the same thing if you pan in the same spot twice in a day.
The about page no longer hides the version if a tip message is shown.
The order that you'll get forge enchantments is now unique per player rather than per farm.
The Ginger Island shrine item pedestals are now normal items. Modded players can spawn them to display items decoratively (though they're not obtainable in vanilla currently).
Map no longer closes if you click on an area of interest.
Fixed some NPC schedules that weren't previously applied:
Lewis visiting the library on winter Sundays;
Maru and Penny hanging out on summer Sundays;
Maru tinkering on summer Mondays;
After reaching six hearts with some NPCs, they won't visit their rival love interest anymore. This affects Alex visiting Haley, Elliott visiting Leah, and Haley visiting Alex.
Gameplay fixes
Fishing fixes:
If you're fishing when the pirate cave changes to pirate mode, you will catch your fish.
Fixed artifacts found in fishing chests not counting for the collection tab.
Fixed fishing crash if a treasure chest appears while the player has a large number of stacked fishing buffs.
Buff fixes:
Fixed a range of bugs like attribute drift and double-debuff.
Fixed food/drink buffs discarded if they have a balanced total of effects (e.g. +2 attack and -2 speed).
Fixed negative custom buffs shown with a double-negative (e.g. ""--2 speed"").
Fixed Squid Ink Ravioli buff not remaining active if another food buff is added later (although the buff icon was still shown).
Fixed Squid Ink Ravioli not resetting the buff duration when you eat another one, unlike other food buffs.
Fixed Squid Ink Ravioli description having an extra blank line.
Fixed Burnt not showing the -3 Attack effect.
Festival fixes:
Fixed unable to leave the Festival of Ice until it ends.
Fixed two-second pause when entering the Stardew Valley Fair.
Item fixes:
Fixed dropped tools not recovered if they're inside a constructed building.
Fixed potted tea bushes planted in town not being harvestable.
Fixed bee house dropping a generic ""Honey"" item when you break it, instead of the flavored honey it would produce when harvested normally.
Fixed farm computer counting fiber crops as unwatered.
Fixed geode and lost book message not shown if your first one is from a fishing chest.
Fixed magic bait disabling some fish area checks (e.g. so you could catch river fish in the forest pond).
Fixed heavy tappers not tracked correctly. That caused issues like heavy-tapped trees not ignored when creating mushroom trees in fall.
Fixed rain totem used during a storm incorrectly making the next day stormy instead of rainy.
Fixed secret note #2 not revealing Kent's gift taste if he hasn't returned yet.
Fixed wedding rings being giftable to NPCs.
Fixed watering can's water left and capacity resetting on right-click into chest.
Fixed wilted bouquet not cancelling the wedding if it hasn't happened yet.
Fixed various cases where item-received logic wasn't applied consistently.
Combat fixes:
You can now kill any slime for the ""Initiation"" quest, not just green slimes.
Fixed player stuck in swimsuit if they collapse from damage while wearing it.
Fixed broken event if the player is defeated in the mines and their name contains a slash.
Fixed issue where you could clear a forest-themed mine level without finding a ladder.
Fixed Magma Sparker's debuff ignoring squid ink ravioli's protection.
Controller fixes:
Pressing the trigger buttons on a gamepad while in the junimo bundle menu now snaps the cursor to the ingredients slot or back.
Improved controller navigation on the Junimo bundle page and world map.
Fixed issue which could cause Junimo Kart to be unplayable with a controller, if you had rebound keys to certain values.
Fixed issue where viewing a letter with multiple pages and positioning the cursor directly above the skip button could cause the next viewed event to be skipped when the A button is pressed to progress text.
UI fixes:
Fixed issue where clicking an item attached to a letter could exit the letter if your inventory was full.
Fixed items from dialogue being lost if your inventory was full. You'll now see a menu to collect them when the dialogue closes.
Fixed gates popping off when opening or closing them while the player overlaps the tile.
Fixed random seeds on Ginger Island being based on the valley's season.
Fixed exploit where you could keep temporary items like Qi Fruit by selling them to Pierre and buying them back later.
Fixed fruit trees being plantable on stone tiles outside the greenhouse.
Fixed tree stumps dropping seeds like full-grown trees.
Fixed riding an orphaned horse (e.g. spawned using debug commands) preventing you from owning or riding other horses.
Fixed some mail not received on the intended day if you don't leave the farmhouse that day.
Fixed bug where it was faster to harvest left-to-right than right-to-left.
Fixed some spots where forage could appear out-of-bounds.
Fixed spot where you could run into the water on the beach farm.
Fixed minecarts unlocked before you've seen the cutscene.
Fixed able to place things underneath the traveling merchant cart and pig.
Fixed some undiggable tiles in Ginger Island dig site.
Fixed geode crusher ""off-by-one"" bug/exploit.
Fixed speed of Taro growth when grown with agriculturist and hyper speed gro (and generally fixed issues with negative phase durations).
Fixed invisible menu preventing you from moving after closing out of the bulletin board Junimo note, if you were holding a useable object.
NPC fixes
Dialogue fixes:
Fixed NPCs not having custom dialogue when accepting a movie invite for non-English players.
Fixed Abigail showing both default and custom item delivery dialogue.
Fixed Lewis skipping his final dialogue at the Festival of Ice on subsequent player wins.
Fixed the fortune teller nearly always showing the text for your combat skill, instead of your highest skill.
Fixed issue where an NPC who bought an iridium-quality item you sold to a shop could show a low-item-quality dialogue for it.
Fixed issue where NPC dialogue could reveal gift tastes for an invalid item.
Fixed monster grave text only shown in English.
Fixed wrong message shown when placing indoors-only furniture outside.
Fixed the Krobus vs Dwarf event able to happen on Fridays.
Fixed giving a gift not counting toward socialization quests.
Fixed invisible spouses asking to have children.
Fixed able to invite Leo to the movies before he moves to the valley.
Fixed able to interact with NPCs during the ice fishing event.
Fixed NPC spouse chores sometimes not applied on the intended day (e.g. saying they filled the pet water bowl but it's not filled).
Fixed NPC spouses watering crops that don't need it (e.g. fiber seeds or already watered), and almost never showing the crops-were-already-watered dialogue.
Fixed NPCs sometimes not reacting to dumpster diving if the horse is nearby.
Fixed Penny losing items when redecorating if they were placed on a stone slab (or a custom table without Table in the name).
Multiplayer fixes
Mitigated potential remote code execution issue.
Possibly fixed frequent disconnections for some players.
Fixed old/incorrect Steam display names being shown in some cases.
Fixed phantom action cursor over backwoods gravestone if the current player hasn't seen the related event yet.
Fixed copy & paste for invite codes on some Linux/macOS platforms.
Fixed issue where the mines were checking for the luck level of players in the host's current location, rather than players in the mines.
Fixed global chat info messages (like ""Abigail caught PlayerName digging through the trash"") sometimes showing text in the sender's language instead of yours.
Fixed egg festival requiring 12 eggs to win if there are 5+ players. It now applies the four-player requirement to any player count beyond that.
Fixed farmhands sometimes able to walk out of bounds at festivals.
Fixed farmhands who disconnected while fishing sometimes stuck frozen on reconnect.
Fixed farmhands who disconnected while playing a minigame (like Journey of the Prairie King) still hearing the music on the title screen.
Fixed farmhands who disconnected while in the building placement view being invisible on rejoin.
Fixed farmhands seeing spouse's outdoor area in the wrong spot on the beach farm.
Fixed farmhands experiencing latency able to click a horse, walk to a different location, then get teleported out of bounds.
Fixed farmhands able to collect multiple statues of perfection.
Fixed farmhands unable to enter a movie without all players.
Fixed farmhands' local effects sometimes based on host data (e.g. the host's equipped rings).
Fixed farmhands' prismatic shards sometimes spawning at the host's location instead.
Fixed farmhands' rings leaving lights behind when they disconnect.
Fixed farmhand crash while fishing in rare cases.
Fixed farmhand crash when they warp just as certain things happen (e.g. pet sounds).
Fixed farmhand crash if train approaches while they're mid-warp.
Fixed other players seeing a farmhand's old spouse room if they divorced and remarried in the same session.
Fixed dinos being able to hurt the host when they aren't in the same location.
Fixed monster drops for farmhands sometimes applying the host's ring effects and secret note.
Fixed various visual glitches with other players' tool/fishing animations.
Fixed crash when displaying a player status list which is missing a connected player.
Fixed dialogue being interrupted if another player starts a broadcast event; the event will now start for you when you finish the current dialogue instead.
Fixed enemy projectiles targeted at farmhands able to hit the host player in a different location.
Fixed monsters not taking damage from some hits if multiple players are attacking them at once.
Fixed projectiles causing damage multiple times in multiplayer.
Fixed various cases where values weren't correctly synced between players.
Fixed watered dirt edges not updated for farmhands when they change overnight.
Fixed 'caught snooping' chat message not triggered when Marnie sees the player searching her trash bin.
Fixed cleared-landslide mail only sent to the main player.
Fixed watering can's water left and capacity not synced in multiplayer.
Fixed the local player's step/fishing stats being incremented by other players walking or fishing.
Fixed players sometimes seeing slightly different daily quests or crop fairy event.
Fixed issue where a farmhand donating the 60th museum artifact wouldn't grant the rusty key.
Fixed issue where reading another player's secret notes or journal scraps could lock them out of perfection.
Fixed issue where the host pausing the game while it's raining and a farmhand is moving would cause the frozen rain to slide across the screen.
Fixed exploit where a farmhand would have full energy after passing out if they disconnected while sleeping earlier in the day.
Display text and localization fixes
Spanish prices are now formatted like 15o instead of 15g to match the translated term (oro instead of gold).
Fixed unlocalized text shown for...
the building paint menu's region names (like ""Roof"" or ""Trim"");
NPC names in some movie theater dialogue;
NPC names in the spouse pregnancy question;
Professor Snail's name after his first event;
Leo's name in his introductory event for some languages;
fish names for some languages in certain cases;
the 'Miss' text when an attack misses.
Fixed Lewis' letter for Feast of the Winter Star saying it starts at 10am instead of 9am.
Fixed dust sprites called dust spirits in Clint's special order and the summit cutscene.
Fixed various typos, broken tokens, translations that don't match the original, etc.
Fixed NPC name translations applied to horses/pets that share a name with that NPC.
Fixed random name generator able to produce words similar to certain slurs.
Fixed world map tooltip for Pierre's shop still showing ""closed on Wednesdays"" after he extends his business hours.
Fixed some recipe names not matching the items they produce.
Fixed the hats added in 1.5 translating their internal name instead of setting the display name in some languages.
Fixed museum item descriptions not having an intended newline.
Fixed French formatting times past midnight like ""26h"" instead of ""02h"" (e.g. on the fishing TV channel).
Fixed French and Turkish showing broken dialogues about another NPC's gift tastes.
Fixed French-only display text bugs related to the ^ dialogue token.
Fixed German truncating Professor Snail's name in his intro event.
Fixed Japanese and Korean formatting buff effects inconsistently.
Fixed some broken Japanese movie reaction dialogue.
Fixed Korean credit line missing in other languages.
Fixed Russian event for Professor Snail's intro freezing.
Cosmetic fixes
UI fixes:
The perfection tracker now paginates better to fit the current screen size.
Moved ""?"" icon on dialogue question boxes.
Removed phantom action cursor over island farmhouse drawer.
Fixed toolbar drawn behind the time/money box.
Fixed cursor over NPCs sometimes showing a gift icon when they won't accept or react to the held item.
Fixed some in-game error messages not having an error icon.
Fixed inventory & crafting menus misaligned if you resize the window while they're open.
Fixed flashing tooltips when hovering between elements in the cooking page and collections tab.
Fixed cosmetic issues with the title & shipping screens for higher-resolution displays.
Fixed calendar tooltip when a modded festival and birthday occur on the same day.
Fixed calendar drawing child NPCs lower than intended.
Fixed the ! fish bite icon not shown when fishing in the Stardew Valley Fair and Festival of Ice.
Fixed quality fertilizer showing a green tile on sprouted crops, even though it can't be placed there.
Fixed perfection tracker not skipping the typing effect when it's paginated.
Fixed the forge preview always showing soul level one for a galaxy soul forge.
Fixed advanced save option tooltips being able to extend off-screen, and not shown for some field labels.
Fixed clothing/tools sometimes showing the item-received notification on unequip.
Fixed HUD messages sometimes overlapping.
Fixed fonts sometimes having a 'thin' look.
Fixed world map tooltip for JojaMart disappearing when the movie theater is built.
Fixed stack numbers not drawn for machine output when the stack is > 1.
Fixed animal purchase menu having unnecessary padding on the right.
Fixed wiggling bottom of energy bar.
Placed object fixes:
Crab pots no longer appear during the moonlight jellies.
Tub o' flowers now shows the correct sprite on the day it is placed, and now always blooms indoors.
Fixed beds or chests that were hidden for an event not reappearing afterwards.
Fixed furniture drawn over sitting players if it has no front texture.
Fixed bombs being invisible when placed on the summit.
Fixed bombs able to remove stick bugs' sticks.
Fixed colored objects not drawn correctly when placed on tables or eaten.
Fixed prismatic hats not displayed properly on alien rarecrows.
Fixed lanterns and other furniture lights not turning on after save/load if they are on a table.
Possibly fixed clumping issue for weather debris like the spring petals.
Fixed layering issues with seagulls and Willy's house.
Fixed fruit tree leaves not rustling in winter even if they're in a non-seasonal location like the greenhouse.
Fixed barren trees having a leafy shadow.
Fixed weeds being less varied than intended in summer.
Fixed weeds able to grow on top of buildings on spring 1.
Fixed intro cutscene position not adjusted when the farmhouse is moved by a mod.
Fixed pre-built cabins sometimes placed on top of a bush or log.
Fixed many things on Ginger Island applying the valley's season or weather (including fiber seeds, seasonal decor, seasonal plants, tea bushes, and tubs o' flowers).
Fixed Elliott appearing twice during the egg hunt.
Fixed Lewis' saloon drinking animation having an incorrect frame.
Fixed bombed objects becoming tiny and sometimes showing wrong sprite index while they flash before removal.
Fixed building placement visual/sound effects playing even for players who are nowhere near it.
Fixed speech bubbles sometimes shown over invisible NPCs.
Fixed crane minigame's music sometimes continuing a few seconds after the minigame ends.
Fixed 'you received X' message & animation showing wrong item if you switch held item right as it's triggered.
Fixed some events fading in after a scene change before they're fully ready, which caused issues like NPCs visibly warping into place or mid-fade flashes.
Fixed ginger showing a watered dirt tile if you water it. This incorrectly implied that ginger needs to be watered.
Fixed missing explosion tiles in mega bomb explosions.
Fixed town festivals not showing the right version of Pam's house and community center.
Fixed an incorrect frame in Lewis' saloon drinking animation.
Fixed layering draw issue in the fishing fair minigame.
Fixes for modded players
Festivals now only set sunny weather in their location context. (For example, a custom festival in the desert no longer changes weather in the valley.)
Monsters no longer spawn at night by default on custom farm types.
Non-binary NPCs can now pathfind through any gendered route (e.g. men's or women's locker room), instead of defaulting to female routes.
Fixed crashes when...
loading a save with unknown locations or farm type, NPCs with no sprite texture, or null world objects.
the farm map has no grass spawn tile and you walk through crops or grass.
the data for an item, animal, tree, etc no longer exists.
painting a building if its sprite changed to a smaller one since it was last painted.
playing audio which doesn't exist (it now logs an error and plays a default 'quiet click' sound instead).
a farmhand warps to a location which doesn't exist locally yet.
an NPC can't parse its dialogue (it now logs the error and defaults to ""..."" instead).
a special order's data can't be parsed.
a mini-jukebox has an invalid track selected (it now turns off instead).
a mini-shipping bin has null items.
a statue of endless fortune tries to produce a gift for an NPC whose first loved gift is a category or context tag (it now now chooses the first valid gift taste, and falls back to a non-birthday gift if none was found);
a farm animal warps home in rare cases;
the player's NPC spouse no longer exists (e.g. the mod which added it was removed).
Fixed calendar support for multiple events on the same day (including multiple weddings, multiple birthdays, birthdays on a festival day, etc).
Fixed horses in indoor locations or mine/volcano levels unable to return home.
Fixed event setup not allowing more than 9 players.
Fixed events not exiting correctly if they crash before the first script command.
Fixed event errors not logged to the console.
Fixed potential event crash if a custom NPC name contains 'farmer'.
Fixed save loading very slowly if it has a broken outdoors location in some cases.
Fixed some items being unremovable when spawned by players. This affects incubators, mushroom boxes, slime balls, and three unused items (locked doors and alternate wicked statues).
Fixed issue when warping to Sandy early using mods where she could give daily quests as soon as you met her, but didn't accept items until you fixed the bus.
Fixed issue where sleeping in a location where an event starts the next day would skip overnight events and soft-lock the game.
Fixed invalid game state when the day of month increases past 28 due to a mod issue. Any day past 28 is now treated as the last day of the season.
Other bug fixes
Fixed crash when taking a screenshot on macOS if the ~/.local folder doesn't exist.
Fixed crash when removing light glows in rare cases.
Fixed window resize on character creation resetting profit margins & starting cabins, and desyncing the 'skip intro' checkbox.
Fixed able to equip a staircase as pants to obtain trimmed lucky purple shorts.
Fixed being able to place a yellow couch in a loom.
Fixed wilderness farm spawning a stone in water.
Fixed hill-top farm spawning grass inside of stumps.
Fixed Trash Bear treated as a villager (e.g. it could theoretically be picked for quests).
Fixed save folder collision if you set the save's name & seed to the same value as an existing save.
Fixed selecting small buildings in the construction menu's farm view. This fixes the gap above the shipping bin where it can't be selected, and fixes small buildings selectable by hovering three tiles above them.
Fixed the build number not set on Linux/macOS.
Fixed unable to write vanilla error logs for players with special characters in their name.
1.5.6
Stardew Valley 1.5.6 was released on 21 December 2021. Hotfixes were applied on 22 December 2021 (twice) and 18 January 2022.

Quality of life changes
On multi-monitor setups, the game now remembers and restores on the last used monitor.
Bug fixes
Possibly fixed a random crash that could occur at the end of the night.
Fixed issue where some farm types' wallpapers and floors weren't applied when starting a new game.
Fixed pressing ESC to dismiss the level up screen also skipping the level up itself, causing it to reappear every night.
Fixed tilled dirt still decaying under forage items on the farm. This means that dirt under freshly grown seasonal forage seeds can no longer disappear randomly.
Fixed grass placed indoors dying when the season changes to Winter.
Fixed unable to harvest tea saplings placed in garden pots on Ginger Island.
Fixed the Z key unbindable after resetting controls to the default.
Fixed map screenshots not working on Linux.
Fixed modding issues.
Internal changes
Updated Steamworks.NET to 20.0.0.
Further changes were later released without changing the version number:
22 December 2021 (announcement)
Fixed crash when starting the game on the compatibility branch on macOS.
Fixed unable to move the game window from one desktop to another in some monitor configurations.
Fixed issue that prevented selecting a profession using a gamepad. You should be able to re-select any missed professions the next day.
Fixed garden pots automatically harvesting when moused over.
Fixed Emily's patio decorations being one tile above where they should be.
22 December 2021
Fixed issue initializing Steam SDK in 1.5.6.
18 January 2022 ""Hotfix #3"" (appended to 1.5.6 announcement)
The version shown on the title screen ? menu now reflects hotfixes.
Changed how the Stardew valley logo is scaled on the title screen for resolutions with lower heights.
Removed the numeric hotkey displays along the top of the inventory screen menu when playing on a controller.
Fixed player/horse placement issues after triggering an event on the farm while on horseback (including an issue that could cause a farmhand to become stuck in a wall after viewing Leah's 6-heart event).
Fixed Korean/English dialogue line swapped for collapsed-in-Volcano scene.
Fixed exclusive fullscreen mode being off-center or incorrectly sized when Window's zoom isn't set to 100% on main-branch builds.
Added failsafe when maximizing/windowing from the title screen to clamp to the upper-right corner of the current display in fullscreen mode.
1.5.5
Stardew Valley 1.5.5 was released to the Steam beta branch on 17 August 2021 for mod authors, announced in the forums on 07 September 2021, and released to all players on 30 November 2021. Hotfixes were applied on 3 December 2021, 5 December 2021, and 7 December 2021.

Architectural changes
Added support for Xbox Game Pass.
This migrates the game on Windows from .NET Framework 4.5.2 to .NET 5. You may need to launch the vanilla game once through Steam/GOG to install the new dependencies.
Added internal changes for mod authors (see mod author migration guide and completed modding wishlist items).
Quality of life changes
Holding Control + Shift now lets you purchase or craft stacks of 25 items at a time.
Many in-game settings now remember their last value when you start a new save. (Some settings like zoom level, gamepad mode, and multiplayer server privacy aren't copied over.)
You can now build or move farm buildings onto artifact spots (which will destroy the artifact spots).
You can now nudge pets out of the way while building/moving a building by clicking on them.
You can now buy back items accidentally sold in the same shopping session.
The in-game time is now shown on the upper-right hand corner of the screen when playing Journey of the Prairie King in multiplayer.
Grass placed using grass starters during winter no longer disappears when the game is reloaded (but it still won't grow or spread). Grass that's outdoors in winter now takes on the appearance of the grass found in the frost section of the mines.
Added /printdiag chat command which prints some diagnostic info to the chat window.
Localization fixes
Fixed Shane's 6-heart event soft-locking in the Hungarian translation.
Fixed incorrect '\' characters in various events in the Hungarian localization.
Fixed typos in the French localization.
Fixed a typo in Livin' Off The Land that incorrectly states that super cucumber can be caught in Winter.
Cosmetic fixes
Added commas to the price display on Robin and the Wizard's building menus.
Adjusted collisions on the desert map to avoid sorting issue on the top-left edge of raised cliffside.
Fixed issue where Sebastian's sprite would shift into an invalid position at 1:00 PM when he's working on his bike on the farm.
General fixes
After completing the Cryptic Note quest, reaching floor 100 of the Skull Cavern again will guarantee a chest.
Fixed issue where the harder Mines could become permanently enabled and unable to change back.
Fixed radioactive bars not affected by the Blacksmith profession.
Fixed explosion radius not centered when dislodging flooring and certain other types of items.
Fixed garden pots clearing their fertilizers when a crop is harvested.
Fixed placing a garden pot on Ginger Island on a rainy day not watering its dirt.
Fixed multiplayer hosts registering their lobbies before fully loading a save, causing the games not to appear in the multiplayer games list.
Fixed moving a cabin with a cellar causing the stairs to the cellar to warp the players out of bounds for the rest of the play session.
Fixed a case where entering the Farm from the south on horseback and triggering an event could cause the player to get stuck in a wall.
Fixed various invisible tiles preventing certain spots in the Mountain lake from spawning fish activity bubbles.
Fixed the tile holding the plant on Elliott's table not being passable.
Fixed some tiles on the left side of the beach farm being unbuildable.
Further changes were later released without changing the version number:
3 December 2021 (announcement, release notes appended to 1.5.5 post)
Fixed widescreen and UHD displays not handled correctly.
Fixed crash in the French localization for the ""It Howls In The Rain"" movie.
Fixed GOG builds not optimized for release, which caused errors with some mods.
Fixed gamepad issues on Linux and macOS with the main 64-bit branch.
5 December 2021
Reverted gamepad fix in the previous update, which caused crashes for some players.
7 December 2021 (see announcement for release, revert, and re-release)
Fixed a memory leak related to sound effects.
Fixed an issue where quitting the game in exclusive fullscreen would revert to windowed borderless upon reloading the game.
Fixed an issue where the last-used volume levels weren't properly applied when starting a new game.
Fixed special orders board appearing distorted during festivals.
(Modding) Fixed [HOURS_24_00] not properly zero padded.
1.5.4
Stardew Valley 1.5.4 was released on 22 January 2021. A hotfix was applied on 2 June 2021.

Changes
Pam no longer visits the Island resort on Sandy's birthday.
The Stardew Valley Fair shop no longer sells multiples of the randomly chosen items.
The Stardew Valley Fair shop now sells triple shot espresso instead of tea sapling.
Multiplayer fixes
Fixed a rare crash that could occur when watching an event in multiplayer as the clock hits midnight or 1am.
Fixed the sound of fiber crops being harvested being audible when you're in another area.
General fixes
Items related to Qi challenges and special orders are now properly removed from both Marlon's Item Recovery Service, and items requested to be recovered are now removed from the mailbox once the quest expires.
Fixed perfection check not accounting for games where the farm switched to the Joja route partway through a Community Center playthrough.
Fixed lucky rings losing their luck bonus overnight.
Fixed softlock when fishing with multiple rods in your inventory after catching a treasure and casting from a different rod. This fixes the common fishing rod softlock next to Willy's house and during the Festival of Ice.
Fixed exploit where entering the Skull Cavern would immediately unlock all 120 floors for the elevator in The Mines.
Fixed exploit where you could find lost books by storing and retrieving certain wallpapers in chests.
Fixed a rare issue where a charged hoe could dislodge a non-empty chest and destroying its contents if it was placed in a specific configuration next to other chests.
Fixed issue where the buttons in the CalicoJack and Calico Spin minigames wouldn't respond to controller input if the UI scale setting didn't match zoom level setting.
Fixed issue where bombing certain floor tiles in the quarry would spawn items.
Fixed palm trees in the desert producing golden coconuts.
Fixed being able to interact with objects while warping with the return scepter.
Fixed trees able to grow on top of placed furniture items.
Fixed a minor graphical inconsistency with the quality indicator on procedurally-colored objects such as flowers.
Fixed Leo's first event unable to complete when playing in the Korean localization.
Fixed the special order board being invisible during summer in the Korean localization.
Various localization corrections.
Further changes were later released without changing the version number:
2 June 2021 (announcement)
Adjusted the randomization of the mummified bat drop.
Fixed multiplayer issues on macOS.
Fixed text wrapping issue on the Special Orders board in the Korean localization.
Fixed some stardrops not always counted for the perfection score. Reaching max stamina now counts as having found all stardrops.
1.5.3
Stardew Valley 1.5.3 was released on 15 January 2021. A hotfix was applied on 16 January 2021.

Balance changes
The stone owl is now rarer.
The strange capsule now only spawns once per game.
Prismatic slimes are now more common (0.8% chance to 1.2% chance), and their spawn rate is now affected by daily luck.
Fixed exploit where Qi beans/fruit weren't removed from machines/craftables after the challenge expires.
You can no longer catch Qi Beans when fishing during festivals.
Minor changes
Adjusted Leo's gift tastes.
Tweaked stone owl sprite.
Penny's 14 heart event now places your existing bed in a chest rather than replacing it.
Furniture is no longer visible and can no longer block movement during events that take place outside of the farm.
Multiplayer fixes
Crab pots now check the professions of the player who refilled them. This fixes a case in multiplayer where a player's profession (such as Mariner) would appear to not work because the crab pot was originally placed in the water by someone else.
Fixed the summit event not triggered if the player is married to another player.
Fixed the menu backgrounds option displayed incorrectly when playing in split-screen mode on a rainy day.
Fixed incorrect button mapping shown on Local Coop Join screen.
General fixes
Experimentally fixed rendering on ultra-wide displays.
Potentially fixed various cases where the player could get stuck or softlocked in an animation after using a tool (including the the horse flute softlock and possibly fishing softlocks).
The Farmer no longer blinks while charging a tool. This was causing a minor visual issue.
Fixed summit event being repeatable if its music wasn't marked as heard. The music is now saved as soon as the event is started.
Fixed issue where a character could spawn on the upper-left corner of the island map when skipping the Ginger Island departure event while the character is still pathing.
Fixed Leo using the wrong schedule on Wednesdays after his six-heart event.
Fixed issue where the quality bobber wasn't applying its bonus.
Fixed magic bait not working to catch Night Market fish in the southwest corner of the beach.
Fixed issue where you could get softlocked if you hooked a fish just as the pirate cave event begins.
Fixed some minor data entry issues on normal bundles that also appear in remixed bundles (Animal Bundle requiring a small brown egg instead of a large white egg, and normal cheese not being an option in the Artisan Bundle).
Fixed bug preventing prismatic slimes from not dropping prismatic jelly in rare cases.
Fixed bug where the strange capsule would leave a residual light glow.
Fixed overnight lost & found tool logic not checking the island farmhouse fridge.
Fixed crash that could occur if a dust sprite's stone-breaking move would cause a Qi Bean to spawn.
Fixed a long delay after talking to Gourmand when you've got the crop he's requested growing, but it's not yet fully grown.
Fixed the daytime music from Ginger Island not being playable on the Saloon jukebox.
Fixed a crash when using a warp totem while the mermaid show is playing.
Fixed issue where you couldn't go back to sleep immediately after waking up after passing out.
Fixed item duplication glitches related to moving rings around in certain menus.
Fixed certain gemstones appearing on the daily quest board before they're actually accessible in-game.
Fixed various localization issues.
(Modding) A randomized bundle containing an invalid item now shows a more detailed error message.
(Modding) Fixed big craftables losing mod data when picked up.
(Modding) Fixed issue where going from one floor to another on horseback inside the Volcano would position the player incorrectly on the next floor.
Further changes were later released without changing the version number:
16 January 2021 (announcement)
Fixed exploit where Qi beans/fruit weren't removed from Marlon's item recovery service after the challenge expires.
Fixed a multiplayer fishing softlock added in 1.5.3.
1.5.2
Stardew Valley 1.5.2 was released on 8 January 2021.

Feature changes
You can now dismiss the level-up menu by pressing the ESC key.
Balance changes
Qi fruit can now be grown on the farm during winter.
Tools now track the previous two enchantments applied to them so they're not reselected when you reapply an enchantment.
Coral and other aquatic forage items are now included for the ""Botanist"" profession.
Added dark cowboy hat to Skull Cavern chest loot table.
You can no longer increase speed buffs using Qi seasoning.
Buff durations from Qi seasoned food/drinks are now 50% longer.
You can no longer complete the ""Danger In The Deep"" Qi Challenge by visiting the Skull Cavern.
Reduced price when selling cookout kits from 3000g to 80g.
Reduced price when selling ducks from 4000g to 1200g.
Fixed ducks not selling for the appropriate price.
Ginger Island fixes
Adjusted a journal scrap treasure map to better represent the in-game location.
Added failsafe to prevent players from warping to the upper-left corner of the southern island map.
Passing out in Qi's secret room no longer causes you to wake up on the mainland.
Fixed Junimos not harvesting crops if you spent the night on the island or when you're not physically present.
Fixed unable to place mini-fridges in the Island farmhouse.
Fixed unable to collect honey from bee houses on the island farm in winter.
Fixed garden pots on the island farm becoming watered on days when it's raining in the valley.
Fixed forage crops disappearing overnight on the island farm on Saturdays/Sundays.
Multiplayer fixes
Fixed most minigames appearing misaligned in split-screen mode.
Fixed a crash that could occur in the mermaid show in split-screen mode.
Fixed farmhands getting stuck if two split-screen players choose the same farmhand at the same time.
Fixed a crash when crab pots are removed by a farmhand.
General fixes
You can no longer sit on chairs if the game is currently pausing your character due to an animation (such as using a tool or kissing your spouse).
You can no longer instantly kill Fector as a zombie in Journey of the Prairie King.
Adjusted the rendering of very wide furniture items so that they do not extend too far past an inventory slot.
Fixed issue preventing 100% perfection score in the Joja route.
Fixed graphical issues with the post-perfection event.
Fixed coral not considered red instead of pale violet red for dyes, etc.
Fixed hearts dropped during Qi's Hungry Challenge not collectable if the inventory is full.
Fixed the level-up screen's OK button positioned offscreen in some cases.
Fixed the Key to the City not allowing you into Pierre's shop if you completed the game through the Joja route.
Fixed issue where large amounts of beets placed in a mill wouldn't produce the proper amount of sugar.
Fixed gifting-related special order objectives not correctly checking the minimum liked value of gifted items.
Fixed special order-related items stored in Junimo chests not properly removed once the special order expires.
Fixed fruit tree saplings and seeds sold at festivals not properly reflecting the profit margins setting.
Fixed sign items no longer showing their placement indicator once a fish pond had been built on the farm.
Fixed combined rings only showing the stat buffs from their first component ring.
Fixed combined rings not properly using their localized name.
Fixed a ring duplication glitch in the dye pot menu.
Fixed a crash and other issues related to obtaining the Iridium Krobus item.
Potentially fixed a random crash that occurs when going to bed.
Potentially fixed a crash in the mines.
Fixed a crash when fishing from the pond on your farm using magic bait.
Fixed a bug causing cheese cauliflower to not display in the collections tab.
Fixed breaking open geodes and artifact troves incorrectly counted as two item collects in the collections tab.
Fixed a crash that could occur when clicking in the scrollbar region of a shop menu when there are less than four items available.
Fixed issue where placing a bomb and sleeping could cause the fuse sound to play indefinitely.
Fixed issue where weeds could appear as batteries.
Fixed weapon cooldown timers no longer displaying on weapons in the shop menu.
(Modding) Fixed order/naming of tilesheets on the beach farm inconsistent with the other farm types.
Localization fixes
Fixed a missing event in Willy's shop from the Hungarian localization.
Fixed an issue with Leo's six-Heart event in the German localization. This fix is applied retroactively.
Fixed an issue where some weapons added in 1.5 would display an incorrect weapon name in the German localization.
Fixed crashes related to Lewis' phone call in various localizations.
Fixed various minor typos.
Various localization fixes.
1.5.1
Stardew Valley 1.5.1 was released on 24 December 2020.

Feature changes
The crystal cave puzzle now gradually gets easier after repeated misses.
Spouses no longer stand around looking at rug-type furniture items. This fixes a case where a spouse might stare at a floor divider, which could cause them to block the player's way through a narrow hallway.
Multiplayer fixes
Fixed exploit where you could regain stamina in bed in multiplayer even if the game was paused.
Fixed rare issue where the host player would warp to an invalid position if a farmhand had just completed a Community Center bundle.
Localization fixes
Various localization fixes and changes.
Fixed a crash when bombing the cave entrance next to the Field Office when playing in Spanish.
Fixed issues with Lewis's phone call in various localizations.
Fixed bug in the Spanish translations preventing Birdie's introduction event from completing.
Fixed bug in the German translations preventing male characters from completing Birdie's quest.
General fixes
When dismissing the forge menu, items that can't fit in your inventory are now dropped downward instead of upward to avoid items being dropped into the lava.
Fixed pets sometimes warping out of bounds.
Fixed being able to place objects in some parts of the void in the farmhouse.
Fixed ""spawn monsters on the farm"" advanced game option resetting when you choose a different farm type after changing it.
Fixed redeeming a stack of items at the Stardew Valley Fair granting a single item, instead of the whole stack.
Fixed an issue related to having a child named Leo or Kent, where existing friendship points for these characters would be transferred over to your children.
Fixed some new hairstyles clipping through hats.
Fixed issue where purchasing the master slingshot would give the player a normal slingshot.
Fixed unable to select the Beach Farm on the new game screen when playing with a controller at minimum resolution.
Fixed Leo available as secret santa if he's not in town.
Fixed being able to place objects in certain tiles that block player movement such as the landslide that blocks the path to the Mines.
Fixed issue where killing big slimes carrying hearts during Qi's Hungry Challenge, while wearing the burglar's ring, would cause the hearts to be collectable in the player's inventory.
Fixed a crash that occurs when demolishing cabins.
Fixed a crash that could occur overnight if you place the Auto-Petter anywhere that isn't a coop or a barn.
Fixed Maru standing on the wrong tile outside the farm on Saturdays if you're married to her.
Fixed hair #56 accidentally replaced.
Fixed Birdie's item trade quest removing all stacks of an item when they're given to their intended recipient.
(Modding) Fixed the return scepter not returning to overwritten farmhouse entry positions.
1.5
Stardew Valley 1.5.0 was released on 21 December 2020. Hotfixes were applied on 21 December 2020, 22 December 2020, and 23 December 2020.

New world area
1.5 adds an entire new region of the world, Ginger Island in the Fern Islands:
Added new locations, dialogue, events, minigames, puzzles, and a quest line involving multiple NPCs.
Added Leo, a new NPC with his own backstory, plot line, events, etc.
Added new secondary NPCs: Birdie, Gourmand Frog, and Professor Snail.
Added a volcano dungeon which changes each time you visit it similar to the mines.
Added Qi challenges which unlock rare or unique items.
Added an unlockable island farm and farmhouse area, where you can plant crops in any season but can't construct buildings.
Added an unlockable resort you can open to let villagers visit the islands, including new beach attire and schedules.
Added hidden pages of a lost sailor's journal to uncover his story and learn about the islands.
Added golden walnuts hidden throughout the islands, which can be used to unlock new areas and content.
Added golden coconuts, which Clint can crack open to find rare island items.
Added secrets and puzzles to discover around the island.
Added new enemies:
dwarvish sentry;
false magma cap;
hot head;
lava lurk;
magma sprite;
magma sparker;
magma duggy;
spiker;
stick bug;
tiger slime.
Added new cosmetic critters: crabs, caldera monkeys, overhead parrots, marsupial, and tropical butterflies.
Added new island obelisk building.
New farm features
Added a new beach farm layout.
Added ostrich as a farm animal.
The immovable dressers found in Farmhands’ cabins have been removed.
Added home renovations, which let you further expand and adjust the farmhouse after it's fully upgraded.
Beds are now normal furniture you can pick up, move around, and replace. (You can even have a house with no bed, but your spouse might have something to say about that.)
Once unlocked, the greenhouse can now be moved at Robin's shop.
The default shipping bin can now be moved or demolished at Robin's shop.
Added advanced game options, which can be used to customize a new game:
set seed value used in randomization;
choose default vs randomized community center bundles;
choose default vs randomized mine chests;
make red cabbage seeds guaranteed to sell at least once at the travelling cart in year one, so it's always possible to finish the community center in the first year;
change profit margins and cabin-related options that were previously inaccessible when creating a single-player farm.
can toggle monsters spawning.
You can now apply cosmetic paints to the farmhouse and buildings.
Ducks can now swim in water, and certain coop animals will now follow adults around.
Slimes now drink from slime hutch troughs in random order.
Other new content and features
Added tool and weapon enchanting.
Added ability to combine two rings into one.
Added special orders, more dynamic late-game quests which let you help villagers with their personal projects through a special orders board in town. These can include more varied goals and rewards, temporary world changes (like new enemies for the duration of the quest), permanent changes (e.g., new shop inventory), and post-completion events.
Added a second community upgrade.
Added a more difficult version of the mines and the Skull Cavern, which includes new and more powerful monsters along with better drops. These can be accessed late-game, and can optionally be toggled permanently using the Shrine of Challenge.
Added new enemies which appear in the difficult mines:
shadow sniper;
skeleton mage;
spider (jumps, often found near webs that can trap players and dust sprites);
prismatic slime (quest only);
putrid ghost (causes nauseated debuff);
blue squid;
royal serpent;
slime variants ('cool' and stacked).
Added new fishing TV channel.
Added new crops and trees:
mahogany tree (drops hardwood)
mahogany seed;
fiber seeds;
ginger (forage);
banana;
mango;
pineapple;
taro root (paddy crop);
New palm tree variant;
Qi fruit (available during special Qi quest).
Festival changes:
added new even-numbered year dialogues for Egg Festival;
added shops to the Dance of the Moonlight Jellies, Feast of the Winter Star, Festival of Ice, and Luau;
added new items to various festival shops;
Children now attend festivals.
You can now change your name and gender in the Wizard's basement shrine.
You now have a perfection rating which tracks the percentage of game content you've completed. Reaching full completion lets you buy golden chickens, adds a random Golden Witch event, unlocks access to the Summit with a new event, and adds new bird critters.
You can now further customize sprinklers with attachments:
enricher automatically applies loaded fertilizer while planting seeds nearby;
pressure nozzle increases watering range.
Added new craftable items and machines:
auto-petter auto-pets animals in a coop or barn (this keeps your animals content, but can't replace the full benefit of human touch);
bone mill turns bone items into fertilizer;
coffee maker brews a fresh cup every morning;
cookout kit lets you cook on the go;
dark sign displays an item without consuming it;
deconstructor destroys crafted items, but salvages their most valuable material;
farm computer scans the farm and displays useful information;
geode crusher consumes coal to break open geodes.
heavy tapper works twice as fast as a normal tapper;
hopper auto-loads items placed inside into the machine in front of it;
Junimo chests are linked to a global shared stash;
mini-obelisks let you warp between two obelisks when placed on the farm;
mini-shipping bins is a smaller shipping bin that can be placed outside the farm;
ostrich incubator when placed in a barn, hatches ostrich eggs into baby ostriches;
solar panel slowly generates battery packs when left in the sun;
statue of true perfection produces a prismatic shard each day;
stone chest is a variant of chest crafted with stone instead of wood;
telephone calls stores to check hours and inventory, and occasionally receive random phone calls (no effect on gameplay);
warp totem: island warps to the Fern Islands.
Added new food items:
bug steak;
banana pudding;
ginger;
ginger ale;
mango sticky rice;
piña colada;
poi;
taro root;
tropical curry;
squid ink ravioli (temporarily prevents debuffs).
Added new fertilizers:
deluxe fertilizer increases chance of higher-quality crops;
deluxe retaining soil always keeps soil watered overnight;
hyper speed-gro increases growth rate by at least 33%.
Added new items:
bone fragment;
Fossilized Skull, Spine, Tail, Legs, and Ribs and snake skull, snake vertebrae;
cinder shard;
dragon tooth;
tiger slime egg;
fairy dust can used on a machine to have it finish processing;
golden walnut;
magma cap;
monster musk causes more monsters to spawn;
mummified bat and mummified frog;
ostrich egg;
Qi gem;
Qi seasoning increases quality of cooked recipes;
radioactive ore and radioactive bar;
taro tuber;
horse flute summons your horse when outside;
mushroom tree seed.
Added new fishing tackle:
curiosity lure increases the chance to catch rare fish;
quality bobber increases fish quality;
magic bait catches fish from any season/time/weather for a given location.
Added new quest items:
Advanced TV Remote
Arctic Shard
Ectoplasm
Gourmet Tomato Salt
Pierre's Missing Stocklist
Pirate's Locket
Prismatic Jelly
Stardew Valley Rose
War Momento
Wriggling Worm
Added new rings:
glowstone ring provides both light and increased item collection radius;
hot java ring increases your chance to find coffee drinks when slaying monsters;
immunity band reduces chance of status debuffs by 40%;
lucky ring increases luck by +1;
phoenix ring restores some health after being knocked out once per day.
protection ring keeps you invincible longer after taking damage;
soul sapper ring restores a bit of energy after slaying a monster;
thorns ring damages enemies when they attack you.
Added new hats:
???;
dark cowboy hat;
deluxe pirate hat;
forager's hat;
goggles;
golden helmet;
Mr. Qi's hat;
pink bow;
Qi mask (secret);
radioactive goggles;
star helmet;
sunglasses;
swashbuckler hat;
tiger hat;
warrior helmet.
Added new clothing:
banana shirt;
ginger overalls;
hot pink shirt;
island bikini;
magenta shirt;
tropical sunrise shirt;
yellow suit.
Added new shoes:
cinderclown shoes;
mermaid boots;
dragonscale boots;
crystal shoes.
Added new weapons:
dwarf sword, dwarf hammer, and dwarf dagger;
dragontooth cutlass, dragontooth club, and dragontooth shiv;
infinity blade, infinity dagger, and infinity gavel;
iridium needle;
ossified blade;
some previously inaccessible items have also been made accessible.
Added new fish:
stingray;
lionfish;
blue discus;
legendary fish:
Glacierfish Jr.;
Legend II;
Ms. Angler;
Radioactive Carp;
Son of Crimsonfish.
Added new furniture:
35 paintings (many only found in festival shops or while fishing certain locations);
14 bed variants;
ten rugs:
blossom rug;
funky rug;
icy rug;
large cottage rug;
large green rug;
large red rug;
light green rug;
modern rug;
oceanic rug;
old world rug;
eight 'floor divider' rugs that can be placed between rooms to join visually different floor types;
five fish tanks you can put fish in (including an Easter egg involving hats);
decorative trash can;
gourmand statue;
iridium krobus;
large brown couch;
plain torch and stump torch;
squirrel figurine;
tropical chair.
Added a new 'sconce' furniture type with seven options.
Added new flooring:
rustic plank floor;
stone walkway floor.
Added new debuffs from enemies:
burnt (Speed -2, Defense -3);
darkness (dims lighting, making it harder to see in caves);
frozen (speed -8);
nauseated (prevents you from eating or drinking anything);
weakness (-20 Attack).
Added 17 new hair styles.
Added mail from Krobus with his own stationary.
Added new decorations and tweaks to many maps.
Added new Easter eggs:
Easter egg when you change your name at the Shrine of Illusions to include item spawn codes;
title screen 'happy face' secret;
new lucky purple shorts secret;
new strange capsule secret.
Emily now visits Sandy on her birthday.
Clicking the pigs at the Festival of Ice now plays a pig sound.
Quality of life changes
Added lost & found box in Mayor Lewis' house. This can be used to retrieve items donated to failed special orders, lost quest items and tools, items from offline players, hats from children that have been turned into doves, and items left behind in the Stardew Valley Fair grange display.
You can now sit on chairs (both placed furniture and chairs that are part of the map).
You can now place most furniture outside.
You can now talk to people while mounted on a horse.
You can now donate items of different qualities for the same community bundle ingredient.
You can no longer plant fruit trees on tiles they won't be able to grow on.
Improved game menus and HUD:
The social tab now shows whether you talked to an NPC today.
The cooking tab now shows the recipe for an item when you hover over it.
The cooking tab now fades icons for recipes you know but haven't made yet.
The cooking/crafting tooltips now show the number produced.
The inventory menu's organize button now combines partial item stacks.
The inventory menu for a chest now shows the Community Center button.
The shipment screen now shows the unit price of shipped items instead of displaying the shipped quantity twice.
The quest log now shows an arrow in the morning when you have pending completed quests.
Buff icons now subtly pulse when they're close to expiring.
Improved shop menus:
You can now press `ESC` or controller `B` to move the held item to your inventory (or drop it if you have no more inventory room).
While holding a purchase, the inventory now highlights items you can stack it with.
Improved sprinklers:
Sprinklers can now water slime hutch troughs.
Hoes no longer remove sprinklers.
Improved Journey of the Prairie King:
Progress can now be saved and resumed, so it can be completed in multiple sessions.
You now also replay the original difficulty even if you've already completed it. (Previously it would increase difficulty automatically, which is now a New Game+ mode.)
Improved game options:
added option to choose between multiple fish-bite chime sounds;
added option to mute farm animals and pets;
you can now scale the UI independently of the view zoom;
increased maximum zoom from 120% to 200%.
Improved various items:
The jukebox now has a ""Random"" setting.
Removing a crystalarium that isn't ready for harvest will now drop the gem that was placed in it.
You can now push chests containing items by holding the interact button on them with no tool equipped (the same way you could previously remove empty chests without using a tool).
Improved inventory sorting:
no longer changes tool order;
now sorts by quality too;
now sorts names in alphabetical instead of reverse alphabetical order.
Improved museum donation interactions:
Clicking an item now grabs one instead of the whole stack.
Pressing back while placing an item now returns to the inventory instead of closing the whole UI.
Other minor improvements.
Improved HUD:
Added a notification when you enter a farm building if an incubator is ready but the building is at max capacity.
The ""infestation"" indicator in the mines is now drawn alongside the floor number, rather than replacing it.
Balance changes
Combat changes:
Immunity now reduces the chance of status debuffs.
Changed special move cooldowns on daggers (6→3 seconds) and clubs (4→6 seconds).
The dagger special attack now pins the target in place until the last stab (which then knocks them back), so dagger hits are less likely to miss.
Reduced desperado profession's damage bonus from 3x to 2x.
Slightly improved dagger critical hit chance.
Crafting changes:
Changed skull brazier recipe (was 10 hardwood + solar essence + coal; now 10 bone fragments).
Repairing a fence now restores full fence health, instead of half the health of a new fence.
Farming changes:
Lowered the cost of ducks (2000→600g).
Doubled the value of duck feathers.
Slightly increased the duck feather spawn rate.
Fishing changes:
Fishing rod tackle no longer loses endurance when catching junk.
Catching a non-fish item no longer triggers the minigame.
Reaching an effective fishing level of 15+ now increases the rod casting range by one tile.
Mining changes:
Gem nodes now provide mining XP and can drop diamonds.
Gem node drops are no longer limited by mine level.
Item drop changes:
Stumps and hollow logs now have a 10% chance of dropping mahogany seeds.
Any monster in the Secret Woods now has a 10% chance of dropping mahogany seeds.
Skeletons now have 4% chance of dropping a bone sword.
Wilderness golems now drop rice shoots more often.
Some Skull Cavern monsters now have a small chance of dropping red cabbage seeds.
Pure-white slimes now drop diamonds and refined quartz.
Some containers now drop better items after reaching the bottom of the mines at least once.
Different drops now unlock on mine levels 60/80/100 and the first level of the Skull Cavern.
Reduced chance of lead rod drops on mine levels 60–79 (or Skull Cavern level 40+ before the previous fix), and added chance for shadow dagger and wood mallet.
Mine wood barrels can now drop basic retaining soil instead of sap.
Mine frost barrels can now drop quality retaining soil instead of sap.
Reduced chance of frost barrels dropping aquamarine, frozen geode, hardwood, jade, or equipment from 35% to 26%.
Shop changes:
Pierre will now sell his seasonal items year-round once you complete a certain new quest.
Added two more items to the Stardew Valley Fair star token shop.
Coconuts can be purchased from Sandy on Monday. This is now limited to ten per day.
The slingshot and master slingshot are now sold at the Adventurer's Guild.
Reduced workbench price (3000→2000g).
Reduced earth obelisk and water obelisk prices (1000000→500000g).
Machine changes:
Some machines' processing time was previously affected by the time of day. These have been standardized so they're always ready when the morning starts: bee house (every 4 days), mushroom box (every day), strange capsule (3 days), Tapper (depends on input), and worm bin (every morning).
Doubled star token rewards for the slingshot and fishing minigames at the Stardew Valley Fair.
Fertilizer in the greenhouse no longer disappears on season change in some cases.
The random stone owl and strange capsule night events are now much less rare.
Multiplayer changes
Added local split-screen multiplayer mode.
The join co-op screen now remembers the last IP you entered.
Farmhands can now move buildings through Robin's menu. The host can configure how this works (disabled, enabled, or only for a player's own cabin).
Using the return scepter now returns farmhands to their cabin's front door instead of the host's farmhouse.
Added various new chat messages when a player does something.
When viewing a shared event in multiplayer (such as the Community Center event, or Morris' introduction), you are no longer forcibly warped to the event location.
When the host player sees the year 3 event, Grandpa's Theme is now added to all players' jukeboxes even if they weren't in-game at the time.
Interaction changes
Reworked slingshot controls: it now aims at the cursor position, and is fired by holding and releasing the mouse/gamepad fire button. The previous mode (where you'd hold the button and pull back on the cursor to aim) can be re-enabled in the options menu if desired.
To simplify bulk actions like harvesting, holding down a button while moving will no longer repeat weapon special attacks, open/close barn/coop doors, or interact with boulders, chests, furniture, hollow logs, meteorites, shipping bins, or scarecrows.
Constructed shipping bins now have a click-to-ship hitbox that better matches the original map-based shipping bin's hitbox.
Flooring can now be applied by clicking on the tile the farmer is standing on. Previously, flooring had to be applied to a tile next to the farmer.
Purchasing recipes with a full inventory no longer shows a message that the inventory is full.
Clicking an empty tile right above an interactive element (like a machine or chest) now activates the element, similar to clicking below it in previous versions.
Trees chopped from above or below will now take into consideration the player’s horizontal position when falling to the left or right.
Clicking near the bottom-left corner of the screen no longer brings up the chat box.
You can no longer open the quest log by pressing the on-screen journal button if you're currently doing something.
You can now use the ""Y"" key to answer dialogue boxes for holes and exit ladders in the mines.
Controller improvements:
When buying/betting tokens in the Stardew Valley Festival with a controller, holding the number selection button now causes the amount to increase faster.
Events can now be skipped on the controller even if a dialogue box is on-screen.
Pressing B on a controller while on a specific quest page now returns to the quest list instead of closing the menu.
Pressing B on a controller while an item is held on the crafting menu now snaps to the trash can (similar to the inventory screen).
Other changes
Save names are now based on the farm name instead of the player name. (Saves originally created before 1.5 will continue using the player name.)
Penny and the Dwarf now like artifacts, all other NPCs dislike (instead of hate) them as gifts.
Improved dinosaur sleep sprites.
Characters now face you when you talk to them instead of when the dialogue box closes (in singleplayer).
On the final day of a tracked quest, the timer now says ""final day"" instead of ""1 day"" for clarity.
Added many changes for modders; see completed wishlist items and mod migration guide for details.
Cosmetic tweaks to various sprites.
Fixes for multiplayer
Fixed an issue where fences would degrade faster in Multiplayer.
Fixed issue where a farmhand crashing while completing the final bundle could permanently prevent the unlock from properly triggering. It is now unlocked the next time a character enters the area.
Farmhands are now properly positioned under the elevator instead of the ladder when using it to navigate the mines.
Fixed an issue where other farmers’ emotes would not playback properly if you were watching an event.
Fixed a minor issue where multiple players getting the free coffee gift at the Night Market at the same time could prevent anyone from receiving it.
Fixed the note from Grandpa reappearing for farmhands.
Fixed issue where cows in Haley's photoshoot event could be seen sliding around for farmhands. (Non-actor characters in events are now controlled by the host.)
Fixes for player interaction
Fixed rare controller and mouse input drops.
Fixed being unable to remount a horse after dismounting it behind a shipping bin.
Fixed issue where it would sometimes be impossible to name an animal with a controller because the onscreen keyboard would appear and disappear in the same frame.
Fixed character customization screen's hair color slider not properly updating when hair color is randomized.
Fixed some farm tiles incorrectly preventing buildings from being placed on them.
Fixes for visual or cosmetic issues
Adjusted the sorting of item ready indicators for tappers.
Unified when night tiles get applied to fix inconsistencies when windows change from light to dark.
The critters list is now cleared out overnight. This fixes an issue where having a Butterfly Hutch in your house and repeatedly sleeping without leaving would spawn endless butterflies in the house.
Improved various text:
Fixed some localizations using inches for fish measurements to reference centimeters.
Fixed mill description not mentioning rice.
Fixed typos in dialogue and events.
Fixed missing characters in Chinese and Korean fonts.
Fixed credits not showing localizer names correctly if the current language's font doesn't have the needed characters.
Improved many translations.
Fixed various cosmetic/layering issues with character shadows.
Fixed characters in events snapping to strange animation frames when skipping an event.
Fixed various instances where the camera would pan unnecessarily after changing locations.
Fixed greenhouse trees taking on seasonal appearances.
Fixed tooltip for purchasing edible but non-health-restoring items (like crocus) showing price outside of the tooltip.
Fixed a single-frame flicker in Haley’s 10-heart event.
Fixed a single frame flicker of the farm when starting a new game.
Fixed stamina bar showing sweat particles when fishing during the fishing competition.
Fixed the scythe showing the weapon ability cooldown effects.
Fixed the erroneous door tile visible on the south end of the standard farm during winter.
Fixed issue where unlocking multiple community center bundles at once would make the screen pan to the same room multiple times.
Fixed various issues related to the Festival of Ice map, like being able to escape the festival bounds or prematurely end the festival from certain tiles, and minor visual fixes.
Fixed terrain features showing an ""interact"" cursor when mousing over their tiles during festivals.
Fixed options page scrollbar sometimes going past the end of the scrollbar region.
Fixed various text overflow issues on the options page.
Fixed chests spawned on certain Skull Cavern floors being shifted down by 4 tiles.
Fixed tiling of…
floor tiles in the farmhouse so applied flooring in adjacent rooms appear contiguous instead of having hard edges;
wall tiles in the farmhouse to fix sorting of certain objects that extend outside normal tile boundaries (e.g., grass starter).
the fountain so that it has a more consistent appearance throughout the seasons;
Pierre's stand at the Egg Festival to fix layering issues.
Other bug fixes
Fixed crash when resizing game window in some cases.
Fixed rare crash on launch due to invalid game window size options.
Fixed Iridium Bat kills not counting towards Adventurer's Guild bat eradication goal.
Fixed an issue where the Mermaid Show reward was delayed incorrectly.
Fixed some events showing a dark blue screen after fading out when viewed during rainy days.
Fixed museum-related rarecrows not sold at the Night Market after being unlocked.
Fixed issue where non-wooden gates would revert to wooden gates after saving & loading a game.
Fixed being able to add staircases to the Luau.
Fixed an item duplication exploit related to chest organization.
Fixed a minor desync in character pathfinding that could happen when characters pathed to the JojaMart on modded games.
Fixed Penny walking through walls on rainy days if you've completed the community upgrade.
Fixed Penny's 4-heart event not accessible once you finish the community upgrade.
Fixed fences not collected correctly after a divorce or Penny's 14-heart event.
Fixed characters greeting you when you enter a location when they're not present.
Fixed characters you divorced still greeting you when you enter a location.
Fixed issue where an NPC spouse fails to pathfind when a player/NPC is blocking the front door when they check, causing them to stand in place instead.
Fixed crops sometimes growing a day slower than they should.
Fixed applying flooring for the first time in an upgraded house sometimes causing the flooring in the kitchen to switch to the default wooden flooring.
Fixed issue where riding a horse while travelling from one location to another diagonally would repeatedly warp back and forth between the two locations.
Fixed watered slime hutch troughs emptied when you load a save. (This mainly affects loading a mid-day save on mobile.)
Fixed the community center not properly loading the refurbished map variant on game load. This caused characters like Maru to path incorrectly in them if the player had not visited the Community Center during that playthrough.
Fixed the weekly friendship boost for giving an NPC two gifts not applied if you also gave them a third gift for their birthday.
Fixed overnight lightning not randomized correctly, so either every overnight lightning would strike or none of them would.
Fixed issue where stones, wood, and weeds could spawn on top of terrain features when a new year begins.
Further changes were later released without changing the version number:
21 December 2020 (appended to 1.5.1 post)
Fixed a crash when interacting with the Junimo notes in the Community center when playing in non-English.
22 December 2020 (appended to 1.5.1 post)
Fixed malformed item data causing the game to crash when playing in Turkish.
23 December 2020 (appended to 1.5.1 post)
Players can now choose another player's character in local multiplayer. This allows players continuing multiplayer farms in local multiplayer to resume their old farmers.
Fixed shared events in multiplayer potentially putting players out of bounds.
Fixed the am/pm display not displayed properly in non-English languages.
Various localization fixes and changes.
1.4.5
Stardew Valley 1.4.5 was released on 11 February 2020.

Bug fixes
Fixed an issue on Windows where the game would initialize incorrectly and start without audio.
Fixed an issue where passable objects, such as torches, could block you from attacking enemies that are standing nearby.
You can now reclaim the ancient seed crafting recipe from Gunther if you donated an Ancient Seed artifact and collected only the seed and not the recipe.
Fixed an issue that would allow animals to walk on certain water tiles alongside some bridges.
Wallpapers can no longer be used in place of items to be processed into artisan goods.
Fixed the Lewis statue letter not granting its reward in the Simplified Chinese localization.
1.4.4
Stardew Valley 1.4.4 was released on 31 January 2020.

Changes
Adjusted dropped item pickups so that they're more responsive for clients in multiplayer and will ""rubber band back"" to their previous locations less frequently.
Adjusted item collection code so that machines tucked in corners can be refilled without using cursor mode.
Fixed Elliott's 14-heart not properly working in non-English languages.
Fixed a data entry issue that caused some fish ponds to use incorrect data.
Fixed the Shrimp Enthusiast Shirt applying the wrong shirt artwork.
Fixed a soft-lock when watching the 2nd Fall movie in a non-English languages.
Fixed the cursor not appearing in the level up profession selector if the Controller Style Menus option is toggled off while using a controller.
Unsuccessfully crafting an unstackable item while already holding an item on your cursor no longer grants you credit for crafting it.
Fixed an issue where garden pots would consume seeds without planting them if they were planted from more than a tile away.
Fixed the color selector sliders on the farmhand creation screen having incorrect navigation on controller.
Fixed the logic for fruit tree obstruction checking the wrong tile.
Fixed the dagger swipe sound effect repeating an extra time in multiplayer if another player is in the same area.
Krobus will no longer get jealous if you gift characters you're dating after he moves in.
Penny now leaves garden pots alone in her 14 heart event.
Fixed an issue that could cause Sam's 6 heart event to soft-lock.
The game now re-issues the player the Qi's Challenge quest if they had previously opened the mail, but dismissed it without accepting the quest.
Items placed in an ex's room are now collected and placed in a chest after divorce.
You can no longer give away quest items or non-giftable items during the Feast of the Winter Star.
Adjusted some NPC gift reactions (Vincent and Jas now hate triple shot espresso, the Dwarf now loves lemon stone, and Sam now likes concession stand Joja Cola).
Adjusted Desert map so that crab pots placed in the water no longer render incorrectly, and objects can no longer be placed on top of the Desert Trader.
Removed catfish from Willy's fishing quests during Winter, as it's impossible to catch one without a rain totem.
Fixed a crash that could occur on the host when a client connects to a game that was set to Friends or Invite Only that is now being hosted on a platform that doesn't support those online modes.
Fixed incorrectly looping ambient noise in the Deep Woods at night.
Fixed an issue where the Japanese localization's dialogue for a spouse watering your crops would incorrectly show both gender variants of the text.
Fixed the Simplified Chinese localization incorrectly showing sunflower seeds as a Fall only crop.
Removed the ‘<‘ from the Simplified Chinese localization of the ""Pet loves you"" dialogue.
Fixed an issue where Junimos would show an empty text bubble when bringing a bundle to the Junimo hut after completing a bundle in Simplified Chinese or Korean.
Fixed an issue where viewing the Junimo note before learning to read it would cause certain text to permanently display much larger in the Simplified Chinese, Japanese, and Korean localizations.
Filled in some missing characters in the Korean fonts. This fixes some instances where an asterisk appears instead of a Korean character.
Fixed a minor typo in Grandpa's note in the Korean localization.
1.4.3
Stardew Valley 1.4.3 was released on 13 December 2019.

Bug fixes
Fixed an issue where you could collect multiples of a bundles reward in the Community Center if you closed the menu without collecting them.
Fixed an issue where naming your pet the name of an NPC could have your pet appear on the calendar on that NPC’s birthday.
Fixed an item duplication exploit when using the add to existing stacks button on an inventory with a full stack.
Made some fixes to the Turkish localization.
Fixed a bug in the crafting menu that could cause extra resources to be consumed in certain situations.
Adjusted the Four Corners farm map to fix layering issue with buildings.
Fixed a bug where you could enter the Wizard's Tower before unlocking the Community Center.
Taking a map screenshot while the option to show menu backgrounds is on will no longer render the menu in the screenshot.
Fixed an issue causing certain wallpapers to grant a Lost Book when selected in the catalogue.
Fixed an issue where you could get your grange score immediately after asking Lewis to judge your grange during the Stardew Valley Fair.
1.4.2
Stardew Valley 1.4.2 was released on 4 December 2019.

Changes
Reverted placement rules introduced in 1.4.1 which prevented players from placing flooring or torches on their spouse's porch tiles. Instead, spouses now ignore flooring and will ""pop off"" any torches that are placed on those tiles when they move there in the morning.
Bug fixes
Fixed the crane game crashing when it loads on Mac.
Fixed workbenches not pulling items from chests that are on rugs.
Fixed Robin staying in bed if she was upgrading certain buildings on game load.
Fixed a bug where buying a magic rock candy, exiting to title, and then reloading the game would cause the candy to not re-appear in the shop.
Fixed a crash when mashing the dialogue advance buttons while transitioning between maps during an event.
Fixed another way the club smash could cause the player to get ""stuck"" holding out a weapon.
Fixed some issues where non-user-initiated mouse movements could cause changes in mouse button states to not register.
Fixed a potential soft-lock if you passed out on another player's bed.
Fixed the screen being partially faded out when Demetrius comes to ask about the farm cave.
Fixed an issue where characters would greet you via speech bubble even if they weren't in the same location (e.g., Elliot welcoming you into his shack even though he's standing at the beach).
Fixed being able to perform certain actions (like using a return scepter) during overnight farm events.
Fixed farmers who are knocked out while holding an item not properly showing the knocked-out animation.
Fixed an issue where the tool being upgraded at Clint's could be set to an invalid value, causing Lewis to incorrectly send you a set of tools every night.
Clicking the emoji menu toggle button on gamepad no longer shows the onscreen keyboard.
Adjusted rumble on third-party controllers. This may not fully solve issues on certain third-party controllers, but will at least prevent states where the rumble is stuck on.
1.4.1
Stardew Valley 1.4.1 (previously known as 1.4.0.1, but renamed for compatibility with SMAPI) was released on 2 December 2019.

New features
Added Gamepad Mode to the options menu. This has three values: ""auto"" (default) switches between keyboard/mouse depending on the last input; ""force on"" forces the game to use gamepad logic, similar to earlier versions of Stardew Valley; and ""force off"" disables the gamepad and can be used if you only play the game using keyboard/mouse.
You can now buy the museum rarecrows from the Night Market after you've unlocked them.
Some non-spouse NPC's now have custom movie ticket acceptance dialogue (in English only).
Balance changes
Explosive ammo now costs 300g each in the Adventurer's Guild, won't appear until you've unlocked the crafting recipe, and now only explodes with a 2-tile radius instead of 3.
You can now only buy one piece of magic rock candy per Thursday at the Desert Trader.
Iridium ore abundance now increases much more slowly beyond level 100 of the Skull Cavern.
Weapon cooldowns now only happen if game time should pass.
Other changes
You can now buy recipes with a full inventory.
Penny's 14-heart event no longer removes placed dressers. Furniture that was supposed to be placed in an occupied tile is now added to the same chests Penny puts your objects in.
Vincent now loves snails.
Adjusted shorts easter egg to give precedence to any normal interactions with the NPCs it affects (gifting, completing a quest, etc).
Slightly optimized debris code.
Bug fixes
Fixed farmhands' separate wallets resetting to 500g if the host was playing on Linux or Mac.
Fixed the incorrect ordering of NPC gift tastes, so tastes should now match 1.3.
Fixed trash bear appearing on festival days (which could cause a potential crash).
Fixed an issue where reclaiming the insect head, Neptune's glaive, forest sword, or carving knife from Marlon would cause Clint to appear to be upgrading that weapon, losing any tool already being upgraded.
Fixed problem harvesting crops with the golden scythe.
Fixed being able to warp into the sewer on your horse, causing you to warp into the void.
Fixed an issue allowing torches and flooring to be placed on the spouses's porch position, so they couldn't come outside.
Fixed the golden scythe still appearing on the reaper after you've already received it.
Fixed a minor issue where players who have connected to a game and are creating a character while an achievement triggers would send a message saying ""- has earned the '(Achievement Name)' achievement.""
Fixed a hole in the refurbished saloon room, which let players walk out of the map boundaries.
Fixed an issue where a farmhand entering a freshly built cabin could cause the farmhand that owns it to spawn in the upper-left corner of the main farmer's farmhouse.
Fixed fish pond silhouettes being incorrect after clearing the pond in multiplayer.
Fixed a typo causing plant seeds being removed from the inventory when unsuccessfully attempting to plant them outside of the farm.
Fixed pre-1.4 untyped ""Honey"" items that were misnamed ""Wild Honey""; they're now just converted to ""Wild Honey"" that stacks with other ""Wild Honey"" items.
Fixed a crash when tree fertilizer recipe is added to a save that already has it.
Fixed the dove subplot enemy leaving visual trails on the host's machine regardless of the location they were in.
Fixed non-synchronization of items recovered from Marlon at the Adventurer's Guild.
Fixed the display name of flounder in Russian being the same as halibut.
Fixed a botched after-movie dialogue from Evelyn.
Fixed the direction of the fold on the Gi clothing items.
Fixed tea saplings not being placeable on certain tiles on the farm.
Fixed a crash when checking the calendar while engaged to another farmer that is currently disconnected from the game.
Fixed problem where you could soft-lock a shop menu in an edge case scenario.
Controller fixes:
Fixed some inconsistencies with cursor placement when item stowing is on.
Fixed an issue where using the trigger buttons to swap pages in the Junimo Bundle menu while holding an item would cause it to be lost.
Fixed an issue in the quest log where if you moved the cursor right from the ""back"" button when viewing a quest description on the second or higher page of the quest log, the cursor would move to an invalid position.
Highlighting a dropdown menu option in the options screen on the controller now snaps to the drop down button, no longer obscuring the text.
When a dialogue question is asked, the first selectable option is now selected by default in controller mode, regardless of whether the controller-style menus option is enabled.
Corrected the fix for the Stardew Valley Fair festival's petting zoo area appearing incorrectly if certain post-game conditions had been unlocked.
Added a graceful fail state to the function that gets a wedding for the day in case there's a completely invalid farmer ID in the farmers list (potentially from a removed cabin).
Players that could have missed increases to their max health, crafting recipes due to skipping combat skill levels will now earn them on level load.
Pending level-ups are now removed from characters as they are applied, instead of all at once at the end of the night. That ensures that a character who gets disconnected/crashes during a level-up can now reapply their level-ups the next time they are connected.
Adjusted the requirements to be able to respec a skill at the Statue of Uncertainty to allow users who have ""missed levels"" to reclaim their missed perks levels using it.
Potential fix for switching over to the scythe after using the club slam repeat quirk causing the next used tool to soft-lock the player in place.
Fixed various typos.
Fixed crash if a location no longer exists in the game, for players with residual mod data in their save file.
Fixed the /friendAll debug command granting friendships to NPCs that aren't friendable, which could possibly cause issues on any game mechanics that randomly select a friend (e.g., daily quests).
1.4
Stardew Valley 1.4.0 was released on 26 November 2019.

New content and features
Added movie theater unlocked in late game, with related content and features.
Added 14-heart events for every spouse.
Added new events and dialogue (including a new heart event for Caroline).
Added clothing items equipped on the player (replaces former clothing appearance options).
Added clothes tailoring and dyeing.
Added 24 new hair styles, including bald heads.
Added Four Corners farm map (meant for co-op, divides the land into four areas with their own perks).
Added fish ponds to breed fish and produce items.
Added new items:
181 shirts;
35 hats;
14 pants (including dresses, skirts, and shorts);
2 boots;
38 decorative items obtained in various ways (e.g., new events);
17 new flooring options;
2 new secret notes;
two fish: flounder and midnight carp;
two rings: crabshell ring and napalm ring;
artifact troves (can be broken open by Clint to yield artifacts);
caviar, roe, and aged roe;
dark sword;
deluxe scarecrow (recipe given when you collect all rarecrows, has double the radius);
dinosaur mayonnaise (processed from dinosaur eggs);
golden scythe;
grass starter recipe (purchased from Pierre);
magic rock candy;
mini-jukebox (place anywhere on the farm or in farm buildings to play previously-heard music);
mini-fridges (place inside to extend your fridge capacity);
rice crop;
seafoam pudding (fishing level 9 cooking recipe);
shrimp cocktail (recipe learned from the Queen of Sauce episode on Winter 28 Y2);
squid ink;
sunflower honey (produced by beehouses);
tea bushes, tea leaves and green tea;
training rod (easier fishing but only catches common fish);
tree fertilizer (non-fruit trees grow more quickly);
triple shot espresso;
void ghost pendant;
warp totem: desert;
wilted bouquet (crafted from a bouquet), which lets you break up with NPCs you're dating;
workbench (lets you craft with ingredients in adjacent chests);
wood chipper (breaks hardwood and driftwood into wood).
Added new mine content:
a quarry mineshaft dungeon with new haunted skull and sludge monsters, and a Golden Scythe;
prehistoric floors in the Skull Cavern with a new pepper rex monster;
rare alternative floors in the 1-120 mines after unlocking the quarry similar to the quarry mineshaft dungeon.
Added shed upgrade (doubles the interior size).
Added cat/dog breeds selectable from the new-character customisation menu or the Shrine of Illusions.
Added Krobus as a potential roommate if you're unmarried.
Added a desert trader that sells new items.
Added Trash Bear to clean up trash around town in year 3+.
Added 4 new monster eradication goals & prizes.
Added 4 new museum reward levels (for donating 35, 70, 80, and 90 items).
Added trash can upgrades purchasable from Clint (gives 15-60% of trashed items' value as money).
Added a new sunroom in Pierre and Caroline's house.
Added the ability to perform various emotes through a new menu (default key is Y).
Added a new farm building: desert obelisk.
Added winter flooring sprites.
Added hidden subplot if you've turned children into doves. (Spoilers: check the television on Fall 26, then visit the Witch's Hut for a recurring effect and fish on the Four Corners farm for a one-time effect.)
Added experimental support for letting farmhands move buildings. Use the /movebuildingpermission command to toggle between on (farmhands can move any building), owned (they can move their own cabins or buildings they've built themselves), or off (default).
Added a new trash bin in front of JojaMart.
Added map export feature (except on 32-bit Linux), accessed via a button in the options menu or the /mapscreenshot chat command. Screenshots are saved in the appdata folder on Windows, and ~/.local/share/StardewValley/Screenshots/ on Linux; click a button in the options screen to open it. When using the command, you can optionally specify a filename and percentage size like /mapscreenshot test 25.
Added VSync option. Disabling VSync may improve overall framerate at the expense of frame rate stability.
Added an NPC gift log accessed through the social tab (tracks birthdays, their liked/disliked gifts as you discover them in-game, etc).
Added 'advanced crafting info' option which shows more info on the crafting menu (including number of times crafted and the number of each ingredient available).
Added OST files for new music tracks.
Added new easter egg on the title screen.
Junimo Kart has been completely redone and is now actually fun.
Dressers can now be used to store clothing, hats, shoes, and rings.
When you catch a silver or gold-quality fish, a 'perfect' catch now increases the quality by one. (This is the only way to get iridium-quality fish.)
You can now put hats on your children (once they can walk).
You can now play Elliott's piano.
You can now put gemstones in Junimo huts to affect the color of new Junimos.
Divorced spouses no longer attend your wedding.
Divorced spouses no longer treat their exes normally during Festivals.
Auto-Grabbers now work in the coop too.
Successfully parrying a slime attack now prevents the slimed debuff.
Seaweed can now be foraged along the bottom shore of the tidal pool beach.
The adventurer's guild now sells any boots you already found in mine chests.
You can no longer load saves created in a newer version of the game than you have. (This will only affect future versions of the game.)
Multiplayer changes
Added optional separate money in multiplayer.
Added support for private chat messages.
Added more multiplayer ""server messages"".
Added a /ping chat command which prints the current ping to the host (for farmhands) or to each farmhand (for the host).
Added a /kick <farmer name> chat command.
Added /ban and /unban chat commands. Bans are per-farm, and the command can be called with a farmer name, user ID, or IP address. in all cases it will ban the user, not the farmhand character.
Added multiplayer synchronization for…
daily luck;
bulletin board quests (in most cases);
trains;
lightning bolts;
mine fog events;
lost book collection;
merchants' limited stock;
drum/flute blocks;
adjustments to the fishing bobber's position while it's midair;
the animation when a player has a fish on the line.
When creating a new multiplayer form, the Starting Cabins option now defaults to 1 instead of None.
Each player now has a separate mailbox and spouse porch area.
Each player can now build a separate cellar.
When a player drops an item in multiplayer, other nearby players now have priority for picking it up.
You can now see other players' scores during certain festivals in Multiplayer.
Quests in multiplayer now set difficulty based on the highest-level player.
In the Skull Cavern, mine shafts now drop players to the same level in multiplayer.
In multiplayer, many random checks now use the team's average luck instead of only the host's luck.
If connecting to a multiplayer server fails, the game will now retry internally before giving up.
The reward for completing the Bulletin Board bundle is now applied to all players in multiplayer.
Moving a Shipping Bin no longer leaves behind its lid for other players on a multiplayer farm.
Quality of life changes
Added ability to rearrange the museum artifacts (without donating a new one).
Added ability to reread received letters anytime via the collections tab.
Added ability to rotate the toolbar (using Tab and Ctrl+Tab by default).
Added ability to fill existing stacks in a chest from your inventory.
Added notification when a tool being upgraded is ready.
Added tabs to the Catalogue and Furniture Catalogue.
Added current Qi coin balance to Calico Jack minigame UI.
Added support on Linux for pasting invite codes via an onscreen button (requires xclip).
Added upcoming weddings to in-game calendar.
All events should now be skippable.
Gates can now be attached to one piece of fencing (instead of needing fences on both sides), which allows for double-width gates.
The hallway to the spouse's room in a two-story farmhouse now matches the bedroom flooring.
Changed interaction cursor when pointing at farm animals you haven't pet today.
Giving datable NPCs a gift no longer makes your spouse jealous unless you're currently dating them.
You can now construct/move farm buildings onto tilled dirt and tree seeds.
Trying to construct/move farm buildings onto an animal now poke them out of the way.
Collapsing in the mines no longer makes you forget levels.
After collapsing in the mines and losing items, a list of lost items is now shown and you can pay Marlon to recover one of them.
You can now remove horse hats (by interacting with the horse while holding another hat).
You can now trash copper pans and slingshots.
You can now buy copper pans from Willy's shop after receiving the first one.
You can now stack craftable items in inventories (like kegs or furnaces).
You can now attach baits, tackles, etc. to your fishing rods directly from a treasure chest.
You can now shift-click items to move them out of the toolbar.
You can now hold down the interact button without retriggering the eat item prompt. That lets players quickly refill rows of kegs/preserves jars without being interrupted by the prompt.
You can now go to sleep while holding an item.
You can now pause Junimo Kart by pressing Enter or P.
You can now play Junimo Kart entirely with the keyboard.
You can no longer place iridium-quality items in casks.
Made it easier to collect milk/wool from farm animals. Animals now have a larger hitbox, and using shears/pail while facing multiple animals now finds the best match instead of the first one.
Interacting with a Mill that only has one type of item in it now automatically collects the item instead of opening a chest menu.
Pointing at an inventory item needed for a bundle now makes the Junimo bundle button pulsate.
Pointing at an item to sell in the shop menus now shows the ""Gunther can tell you more"" text if the item hasn't been donated yet.
Mushroom tree sprouts are now distinguishable from other trees.
Wild Bait now provides a chance to catch two fish at once.
Pets no longer spawn in positions that block the farmhouse door or corridors.
When exiting a Junimo bundle menu, the cursor highlights the bundle that was exited.
Farm animals standing on crops no longer prevent the crop from being harvested.
A message now appears when fruit tree growth is prevented by a surrounding obstruction.
When an NPC walks over a chest, it will now dump its contents out instead of destroying them.
When viewing a bundle, inventory items for completed slots are now grayed out.
Jumping down a mineshaft just above level 100 will no longer drop you below level 100 in the Skull Cavern.
Players can now ""push"" through NPCs at festivals, to avoid getting trapped in some cases.
Queen of Sauce reruns will now choose an already-aired recipe that a player doesn't know.
The animation played when finding a new lost book is now only played once per player.
Unclaimed bundle rewards can now be picked up from a Junimo bag in the Community Center.
Wild tree seeds can now be placed in any diggable tile outside of the farm without needing to hoe it first.
When you dismiss the map by clicking on it, you're now returned to the previous menu tab (if any).
Wilted trellis crops no longer block movement.
The Hat Mouse now sells hats you've won in festival competitions.
Moving the cursor over a crab pot while holding the action button no longer picks it up.
Moving the cursor over a farm animal while holding the action button no longer opens the inspection menu (so it's easier to pet animals).
Moving the cursor over a scarecrow while holding the action button no longer checks it (so it's easier to harvest large fields).
Improved the 'Organize Inventory' logic. Now properly handles empty spaces on Linux, and sorts stacks from highest to lowest quantity.
Changed processing time of tapped mushroom trees so that they're harvestable in the morning (instead of midway through the day).
Obelisks now have a 3x2 footprint instead of 3x3. (Their sprites are unaffected.)
Dropping an item when collecting rewards from Gunther now throws the item downwards instead of upwards, to prevent players without magnet rings from dropping items into out-of-reach positions.
Balance changes
Fruit trees no longer need the surrounding tiles to be empty to increase in quality once fully grown.
Fruit trees are no longer blocked from growing by non-colliding objects (notably artifact spots).
Mushroom trees no longer drop wood (and debris chunks are now white instead of brown).
You can now increase friendship by talking with NPCs at festivals.
Trains now drop more items.
Some items no longer appear in random shop stocks: void eggs, void mayonnaise, and sweet gem berries.
The quarry now spawns oak & maple trees.
Charcoal kilns now require 2 copper bars (instead of a gold bar).
Cheese now sells for more.
Cloth now drops more often from mummies.
Dressers now cost more.
Looms can now produce multiple cloths when higher-quality items are input.
Pancakes can no longer be bought from Krobus before winter Y1.
Sturdy ring is now easier to craft.
Journey of the Prairie King has been rebalanced.
Calico Jack and slot machines in the casino now statistically favor the player. (Previously the player was more likely to lose.)
The Forester profession now causes 25% more wood to drop from trees/stumps/logs, instead of raising the value of wood by 50%.
Speed-gro and retaining soil can now be applied to crops anytime.
Basic and quality fertilizers can be applied to seeds (but can't be applied once a seed has sprouted).
Rebalanced the frequency that secret notes are dropped.
Fishing tackle now sells for less once damaged, depending on its remaining durability.
Sweet gem berry can no longer be bought from the traveling cart (you can only buy the seeds).
Bean hotpot now grants max energy +30 and magnetism +32, instead of showing ""max energy +2"" but granting nothing.
Spring onions quality is now determined at the start of the day.
The Legend can now only be caught once.
Made bottom edge of fishing bobber bar slightly more generous.
The effect of fishing level on fish size no longer caps at 10.
Debris now respawns at the start of spring in Cindersap Forest, Town, and the Railroad.
Reduced cases of items splashing into water when they're visually on dry land.
Lightning rods are now always harvestable immediately the day after being struck.
Wheat now has a 40% chance to also drop hay on harvest.
Wild bait can now be obtained from fishing treasure chests if you know the crafting recipe.
Fixed the burglar's ring not affecting some monster loot items (those added programmatically).
Reduced recipe prices in the Stardrop Saloon.
Improved controller support
Added an on-screen keyboard to type into chat or text fields using the controller.
Added ability to stow an item when playing with the controller, freeing up your hands to interact with NPCs without accidentally gifting them items.
Added a left/right bumper hotkey in inventory menus. when the new 'add to existing stacks' button is available, pressing the hotkey will automatically snap to that button for easier inventory consolidation.
You can now highlight the list of required items in the Junimo bundle screen while using a controller, so you can see a tooltip with the item's description.
Improved controller navigation/use in…
the museum donation screen;
the Load Game and Join/Host Coop Game menus;
the bundle screen (no longer need to scroll all the way to the right to access the second row);
chest and item grab screens;
crafting menu.
Improved switching between keyboard/mouse and controller mode. For example, having a controller plugged in no longer snaps mouse input to buttons.
Improved placement logic with a controller (planting seeds, placing furniture, etc).
You can now direct your fishing casts with a controller.
The buffs display tooltip is now hidden if the mouse cursor isn't visible.
Fixed cursor snapping to upper-left item when clicking the organize items buttons on the controller.
Fixed D-Pad not correctly selecting dialogue options.
Fixed an issue making the animal pregnancy disable option unselectable with a controller.
Fixed navigation of the languages menu using a controller.
Fixed an issue where rearranging artifacts in the museum didn't work with controller-style menus.
Fixed various issues related to selecting dialogue choices with a controller.
Fixed being able to move the cursor on the shipment screen after the screen is dismissed with the OK button on a controller.
Fixed cursor slightly misaligned on the skills screen when you first enter it using a controller.
Fixed various issues related to the display of the cursor in the level up screens with a controller.
Fixed some issues with controller navigation of the items in the wallet section of the skills tab.
Fixed controller navigation of dropdown lists in the options screen.
Fixed an issue where the controller's left thumbstick was moving the mouse cursor around while playing minigames.
Fixed an issue where using the face buttons to aim shots in Journey of the Prairie King on the controller would sometimes cause the shots to go in the wrong direction and cause the player to get stuck shooting.
Fixed chest color picker buttons being selectable if the color picker isn't shown.
Other changes
Various copyediting, improved text, tweaked sprites, and tweaked map tiles.
Made some optimizations that may improve performance on some machines/platforms or in some cases.
Improved many translations and fixed missing font characters in Korean.
Changed midnight from 12:00 to 0:00 in Japanese.
Added an open sprite to the fridge.
Adjusted the mail received after collapsing to more accurately reflect timeline changes in the game.
Adjusted a dialogue option in Penny's 2-heart event to be more considerate of George's perspective.
Removed the Junimo Note/Community Center Button from non-inventory screens.
Improved Elliott's cabin interior design a bit.
Items found in the trash now pop out of the trash instead of going directly into your inventory.
Updated credits.
In single-player, the mines now reset as soon as you leave instead of waiting for the next 10-minute update.
NPCs now destroy trees in their path (and will trample wild tree seeds underfoot).
NPCs now close their eyes and do a sleep pose when they go to bed.
Players now close their eyes when they go to bed.
Penny no longer randomly reveals Pam's taste for alcoholic items.
Fish sold to Willy are now resold in his own shop (fish are no longer sold at Pierre's).
Tweaked hoed dirt color on the beach in fall.
Made David's cage look more appropriate for guinea pigs.
Reduced splash effect of small cosmetic debris to make it easier to distinguish splash of actual items.
Added animation when digging through trash.
Adjusted sound made when picking up forageables, eggs, etc.
Added changes for modders.
Added log for unexpected network disconnects (in a separate DisconnectLogs folder).
The ordering of the items on the collections page is now consistent between languages.
Fixes for gameplay
Dismissing a letter with unclaimed items now automatically picks up the items.
You can no longer activate the Statue of Uncertainty if you haven't met the level requirements.
You no longer take damage from monsters while passing out at 2am.
Fixed various exploits, including…
opening the journal while fishing paused time but still caught fish;
opening the journal while dying skipped the death event and prevented losing items;
you could use an incorrect item in some cases (like using a wallpaper to obtain the galaxy sword, incubate a dinosaur egg, or when crafting);
you could predict the mushroom level pattern in the mines;
you couldn't be damaged while the eat confirmation prompt was up in multiplayer;
the invincibility timer after taking damage didn't decrease in multiplayer if a menu was open;
right-clicking a fishing rod attachment in the inventory reset its durability;
using glitches to leave the spa while in swimming mode would result in infinite energy/health regen;
you could obtain up to 109 statues of perfection by storing each one in a cabin's inventory chest;
items of different qualities could be combined into one stack when buying from Pierre;
you could sell items to Pierre's at an upgraded price (e.g., salmonberries with the Bear's Knowledge perk) and buy them back at their normal price. He now resells them at the same price he paid.
Fixed players getting stuck…
if you cast a fishing rod just after opening a treasure chest in the mines;
in the blocking pose when you use a sword's special ability in rare cases;
on top of a mine ladder if multiple ladders were spawned;
in the traveling merchant's cart when dismounting from a horse in some cases (or getting your horse stuck that way);
on top of Abigail when playing the minigame in her two-heart event.
Fixed rare issue where no ladder would spawn on a mine floor.
Fixed monsters spawning too close to the start of a floor in the mines.
Fixed mummies dealing damage while downed.
Fixed invisible, interactable elevators on treasure floors of the Skull Cavern.
Fixed various issues where using bombs could cause items to spawn at the main player's current location instead of the location where the bomb was placed (e.g., artifacts found via secret notes, fruit tree items, giant crops, mystic rocks, hardwood, crystals, and cave grass).
Fixed issue where weeds could be spawned on the tile directly beneath the elevator in the mines.
Fixed objects on the farm sometimes skipping their daily updates, which could cause issues like crops surviving a day into Winter or batches of crops not maturing at the same rate.
Fixed various cases where an entire stack of held items was incorrectly consumed.
Fixed certain artifacts not spawning as intended.
Fixed cooking ingredients spread between your inventory and fridge not correctly counted together.
Fixed issue where leveling up would incorrectly affect the related skill for the rest of the day (i.e., common trees wouldn't drop seeds after leveling up foraging).
Fixed artifacts appearing on grass after winter turns to spring.
Fixed bug when getting a JojaMart membership shortly after completing the Community Center.
Fixed weed item obtained from breaking a mushroom seed or shaking a mushroom tree.
Fixed Livin' Off the Land announcing the start of blackberry season on the wrong date.
Fixed certain placeable items being placeable on tiles occupied by other furniture.
Fixed a few missing boundary tiles in certain areas, including in the Stardew Valley Fair.
Fixed player's fishing level and bait not taken into account for the wait time if the first nibble was missed.
Fixed Tiller profession bonus sometimes not applied to combined foraged and grown grape stacks.
Fixed Prospector profession not applied to rocks destroyed in the mines.
Fixed forage sometimes spawning out of bounds.
Fixed removing tappers from trees in certain ways making them become un-choppable.
Fixed fishing cast distance not determined when the bobber hits the water (previously set at the end of the cast).
Fixed TV weather forecast not always matching actual weather.
Fixed Queen of Sauce sometimes airing a new recipe on the Wednesday before it's scheduled on the following Sunday.
Fixed Queen of Sauce repeatedly airing the Stir Fry episode at the end of year 2 and the beginning of year 3.
Fixed players playing a minigame at 2am not properly quitting the minigame and passing out.
Fixed players able to continue fishing or charging tools past 2am.
Fixed giant crops growing in a noticeable pattern.
Fixed discrepancy between an item's shown health recovery and the actual recovery when consumed.
Fixed the chicken statue furniture being stackable with the chicken statue artifact.
Fixed issue where players could still take damage and die while warping with an obelisk or return scepter.
Fixed issue where ore/mineral veins destroyed with a bomb outside of the mine would only drop stone.
Fixed issue where going to bed early caused machines to process more quickly overnight. (Machines previously processed 100 minutes per hour slept; they now only do so between 2am and 6am, and the remaining time slept will be processed at the normal daytime rate of 60 mins/hour.)
Fixed issue where harvesting a tapped mushroom tree in winter would cause it to not produce again until mid-Spring, rather than Spring 1.
Fixed issue where snow yams and winter root could be dug up in the desert in winter.
Fixed lopsided bee house flower range.
Fixed some Witch Swamp and Mutant Bug Lair water tiles not correctly marked as water, causing inconsistent fish quality in those areas.
Fixed issue where entering a house the moment an NPC passes through a door would close the door on top of the NPC, causing them to get stuck.
Fixed issue where fruit trees wouldn't grow if there was a monster or other NPC near them overnight.
Fixed issue where closing a mine chest with the OK button while your inventory was full destroyed the item; instead the item is now dropped.
Fixed issue where a player at the Stardew Valley Fair would get their score calculated based on the contents of the Grange upon closing the minigame (instead of when Lewis had actually finished his judging), letting that player have a different score than intended.
Fixed minor issue where zero shots fired at the Stardew Valley Fair slingshot shooting gallery would give a large negative calculated accuracy.
The Cookie recipe will be sold at the Saloon if Evelyn's 4-heart event has been seen but the recipe wasn't obtained.
Fixes for player interaction
You can now collect lost books even if your inventory is full.
Fixed horses being unmountable if they're very close to a pet.
Fixed being able to trash items from the crafting menu even though the trash can is invisible.
Fixed answering dialogues on a small screen causing the cursor to interact with the toolbar.
Fixed interacting with objects in the world while holding an item sometimes causing both the interaction and object to be used at the same time.
Fixed interacting with a fireplace sometimes not toggling it.
Fixed interacting with objects so that objects behind the character are longer interacted when the character is facing upwards.
Fixed interacting with a trash can on horseback simultaneously dismounting the horse and searching the trash (now just dismounts).
Fixed issue where players couldn't harvest or pick up items by clicking and holding the tool button with a melee weapon or scythe in hand.
Fixed issue where attempting to charge an upgraded watering can to the left of a body of water on the farm would prevent the farmer from charging it.
Fixed unable to fill water can from the left side of the water trough in the greenhouse.
Fixed issue where depositing ore into two nearby furnaces could activate both of them.
Fixed large rotated furniture being placeable on top of tables, causing them to disappear.
Fixed issue where players could move while warping out of the Witch's Swamp.
Fixed issue where players couldn't push through farm animals in some cases if they were offset by half a tile.
Fixed issue where interactable furniture like fireplaces couldn't be picked up with left-click when standing close to them and holding a non-tool item.
Fixed issue where it was hard to click on NPCs if you went into a festival while holding a placeable item.
Fixed crawling-stage babies in the crib not being interactable.
Fixed minor issue where you could cause the farmer to stop shaking during a tool charge by pressing a movement key.
Fixed being able to interact with NPCs when your farmer is set to be immobile. (For example, this prevents you from trying to gift your spouse a second gift immediately after kissing them and getting stuck in an erroneous animation frame.)
Fixed opened gates orphaned by destroying the fence post they're attached to not blocking movement.
Fixed wild tree seed placement cursor not accounting for seed-placeable rules.
Fixed crab pot placement cursor always green, even if an invalid placement tile is selected.
Fixed palm trees not shaking and dropping leaf debris when interacted in winter.
Fixed the ""Warrior"" option not being selectable during Sebastian's 6-heart event in Korean.
Fixed being able to use the dagger while bathing.
Fixed museum reward collection menu not dismissible using the exit hotkey.
Fixed museum donation screen issue where panning the camera would cause the cursor to scroll off of UI elements.
Fixed issue where watering a small tree sapling played a scythe sound.
Fixed issue where hitting a fence that has a torch on it would destroy the torch instead of dropping it.
Fixed invisible blocking tile front of Lewis's house during the Stardew Valley Fair.
Fixed blank or whitespace-only chat messages being sendable.
Fixes for festivals, events, and quests
Dismissing a quest letter no longer discards the quest. (It's now added to your quest log to avoid permanently missing out on quests.)
You can no longer place objects at the beach while the Night Market is occurring.
Removed invisible trees from festivals.
Fixed exploit where you could pause time when the Night Market submarine was ascending/descending to skip the wait time.
Fixed exploit where you could watch the Night Market Mermaid Show while time was paused (the show now pauses too).
Fixed exploit where you could fulfill the requirements for a gathering quest by taking items out of a chest one-by-one.
Fixed players getting stuck on doors or placed objects during cutscenes in some cases.
Fixed players getting stuck in Harvey's 8-heart event if it was triggered when leaving his bedroom.
Fixed players getting stuck in Harvey's 10-heart event if a train came by.
Fixed introductions quest not considering Willy done after you meet him to receive the fishing rod.
Fixed Mr. Qi's Challenge not considered complete if you fulfilled the conditions before getting his letter.
Fixed the egg in Lewis' truck not being accessible during the egg hunt.
Fixed time not passing for machines inside of farm buildings during festivals.
Fixed players getting trapped in the hospital until the festival starts if they were knocked out for the first time on a festival day.
Fixed time passing during festivals not reflecting when the player(s) started the festivals.
Fixed some out-of-season items appearing as targets for ""Help Wanted"" quests.
Removed strawberry from Summer quests, catfish from Winter quests, and sardine from Summer quests.
Fixed bug limiting which NPC would request gathering, slay monsters, or fishing quests.
Fixed Help Wanted quests not reflecting that they pay 3x the market value, instead of 2x.
Fixed occasional spacing issues with the messages on the daily quest board.
Fixed Robin sometimes thanking you for the wrong item when completing a billboard quest.
Fixed winning festival events not counting towards farm's total earnings.
Fixed Penny's 6-heart event not triggering after purchasing the Community Upgrade.
Fixed Sam's 8-heart event being silent if you skipped his 2-heart event.
Fixed ""A Winter Mystery"" quest not triggering if exiting the farm from the bottom tile of the exit to the bus stop.
Fixes for shops and shipping
Fixed merchants refilling limited-stock items when you reopen their shops.
Fixed traveling merchant sometimes having the same item for sale for two different prices.
Fixed items bought from shops being sellable at a different price than produced/grown versions.
Fixed issue where honey would sometimes sell for more than expected.
Fixed pressing Alt + Enter while buying from Pierre destroying the held item.
Fixed selling animals not counting towards farm's total earnings.
Fixed issue where you couldn't buy animals while Robin was upgrading the pertinent building.
Fixes for NPCs
Fixed several cases where NPC dialogues were out of sync with the actual state of the game world (e.g., saying the wrong dialogue in bed).
Fixed issue where villagers could get stuck doing their daily schedules late into the night after a festival.
Fixed issue where Vincent could get stuck on a flower barrel during the flower dance.
Fixed issue that prevented some villagers from hanging out with one another in town on certain days.
Fixed various issues related to naming horses.
Fixed various bugs caused by giving a villager's name to a horse, pet, or child.
Fixed friendship-related achievements not properly triggering as soon as the conditions are met.
Fixed various friendship-related bugs and inconsistencies, including 10-heart events being unlockable without a bouquet.
Fixed NPCs reacting incorrectly to certain gifted items.
Fixed issue where gifts couldn't be given to the dwarf if their dialogue was exhausted for the day.
Fixed Abigail visiting the farmer on the same day as her 8-heart event.
Fixed Bouncer just saying ""Hi."" instead of his unique line.
Fixed Bouncer not accepting the club card if you've already exhausted his dialogue for the day.
Fixed Demetrius plowing through bushes by the fountain in summer.
Fixed Dwarf always responding negatively to items it feels neutral about.
Fixed Dwarf not liking cave carrots, despite having dialogue which suggests otherwise.
Fixed Emily's four-heart event unlockable at night.
Fixed Gus offering Bread, Pizza, Salad, and Spaghetti as dish of the day (he already sells them as part of his normal shop stock).
Fixed Henchman dialogue box showing a friendship jewel.
Fixed Krobus secret note event not triggered if riding a horse.
Fixed Krobus not keeping silent on Fridays when accepting gifts.
Fixed Leah's summer schedule causing her to walk into the water and get stuck there for the rest of the day.
Fixed Penny not having her own introductory line.
Fixed Penny talking about the sound of rain on the roof of her trailer after the community upgrade is built.
Fixed Penny talking about living in a trailer after the community upgrade is built.
Fixed Pierre talking about time off after the Community Center has been restored.
Fixed Shane sending two recipes at 3 hearts. (The second is now sent at 7 hearts as intended.)
Fixed Welwick saying two different things on the same day.
Fixed free gift at the Night Market being available every ten minutes instead of once a day.
Fixed buggy interaction with Mayor Lewis when getting a Joja membership before starting the Community Center.
Fixed townsfolk sending players the wrong cooking recipes for their current friendship level.
Fixed villagers you're already dating still accepting bouquets from you.
Fixed a rare crash when attacking ""slimes in love"".
Fixed attacking a rock crab with a pickaxe not dealing damage after its shell is broken.
Fixed issue where villagers that respond differently depending on their current location will no longer say those location specific lines when talked to again later.
Fixed spouse continuing their ongoing dialogue after being interrupted with giving you a Stardrop.
Fixed most cases of crawling-stage babies waking up in the crib at the start of the day. (They'll now spend longer trying to find a non-crib position before giving up.)
Fixed NPCs sometimes detecting players rummaging through their trash or private possessions from a different location.
Fixed NPCs saying something upon entering an area (e.g., Pam entering the Saloon) doing so if you're watching an event.
Fixes for multiplayer
Farmhands' spouses now move around, run their daily schedules, and react to their spouses like they should.
Farmhands now return to their own porches after a festival, instead of the main farmer's porch.
Players can no longer give quest items as gifts to other players.
You can now connect to the host via IP while they're in the process of loading the game.
Fixed an issue where if a server was shut down on the same day a remote player had leveled up overnight, the level up would not be applied until the following night upon save load.
Fixed rearranging the museum in multiplayer showing a message that the player donated a new item.
Fixed farmhands not seeing the notification that a train is approaching.
Fixed players stuck if a bundle is completed while they're on a tile that becomes unwalkable after the room gets renovated.
Fixed issue where a spouse could get stuck in a farmhand's cabin after divorce. Save files that already have this issue will be fixed after the first night.
Fixed bundles button visible on the inventory screen for farmhands that had missed a bundle completion cutscene.
Fixed issue where you couldn't interact with something behind another player (e.g., to talk to an NPC).
Fixed farmhands seeing bundles for completed areas when looking at the bundles menu.
Fixed farmhands unable to pick up lost books if the host has a menu open.
Fixed married NPCs saying marriage-specific lines to players who aren't their spouse while they're on their married schedules.
Fixed the door frame from Robin's construction animation not properly removed for remote clients after construction is done.
Fixed messages like the one about spreading weeds only shown for the host player.
Fixed Kent sometimes missing from clients' festivals.
Fixed players that collapse in multiplayer seeing the screen briefly flash, showing them in bed.
Fixed fishing rod cast sound affected by other players' casts.
Fixed fishing rod animation not properly synchronized when clicking and holding to recast.
Fixed issue where Lupini would only stop selling a painting if the host player was the one to buy it.
Fixed issue where horses could lose their hats in multiplayer.
Fixed issue where farmhands couldn't activate the Dark Shrine of Night Terrors.
Fixed issue where monsters wouldn't move while host was watching an event or getting knocked out.
Fixed various issues related to child placement and multiplayer synchronization.
Fixed issue where light sources from disconnected farmhands wouldn't get cleaned up.
Fixed issue where animals in barns built by farmhands in multiplayer were unable to breed.
Fixed issue where completing a Vault bundle wouldn't send a global chat message in multiplayer.
Fixed issue where the Witch event would create a void egg for each logged-in player.
Fixed issue where items shipped, minerals found, recipes cooked, fish caught, and artifacts found during a multiplayer session could be forgotten if a client disconnected before the end of the day.
Fixed several bugs relating to Mummy behavior and multiplayer sync.
Fixed sleep stamina recovery being based on the last player who went to bed (now tracked separately for each player).
Fixed baby slimes in the mines destroying rocks on contact if a non-host player killed the big slime.
Fixed various multiplayer data sync issues.
Fixed time slowing for all players when the host is in the mine (no longer slows at all in multiplayer).
Fixed missing tools in multiplayer not always being recovered.
Fixed various issues related to pets in multiplayer.
Fixed remote players not properly showing jump/jitter animations during emotes or horse mounting/dismounting.
Fixed the host player seeing flashes or feeling controller rumbles from bombs in other locations.
Fixed explosion flashes not showing up on farmhand machines.
Fixed player getting stuck on the ""Waiting for players"" check after a festival if another player joined at the same time.
Fixed Joja purchase form not reflecting upgrades bought by other players.
Fixed issue where getting killed while paused in multiplayer could leave you in a perpetual getting-knocked-unconscious state; this now unpauses instead.
Fixes for visual or cosmetic issues
Items that come in various colors (like tulips) now show their proper color in the item received pop-up.
The mouse cursor is now hidden in scenes that lock user input.
The lighting in the Secret Woods now changes more consistently with the rest of the world as time passes through the day.
Lit furniture placed on top of tables will now properly light upon nightfall.
Fixed various visual map issues, including…
various tile issues in the Mines and Skull Cavern;
glow from certain lamps in the town interiors tilemap bleeding into adjacent tiles in the tilesheet;
incorrect corner edge tile used inside the walls in mine level 6;
incorrect grass tiling under the fence on the bottom of the mountain path to town, and missing tile on the tree near that location;
incorrect tiles in the Flower Festival;
a missing stone tile under the JojaMart sign in town;
visual issue with the kitchen floor tile;
visual bugs with the eyes of the statue in the secret woods;
visual issues with hat mouse's house;
artwork of the Bulletin Board in the Community Center not lining up with the walls around it;
mismatched walls behind the plant and picture frame in the hallway between Alex and George and Evelyn's rooms;
mismatched wall with the power outlet for George's TV;
look of the carpet under Sam's bed;
incorrect shadowing on some tiles in the mines on floor 7 and 14;
Secret Woods statue eyes staying red after activation until the season changes or you reload the save.
Fixed various text display issues, including…
various typos;
various line wrapping issues in dialogue boxes;
line wrapping issue when leaving the feast of the winter star;
line wrapping issue in tooltips for boots, rings, and melee weapons with long names;
extra space at the bottom of tooltips for edible items that don't grant any health;
sizing issues in carpenter menu for non-English languages;
spacing issues in bulletin board request text;
one of Haley's lines having a blank dialogue page;
certain items with long names causing tooltips to word wrap incorrectly;
stardrop message easter eggs incorrectly appending the favorite thing after the special text.
Fixed various sprite issues, including…
layering issues (e.g., when the farmer is facing upwards while using some tools, or bushes drawn over chests);
character sprite coloration issues;
some monsters having two shadows (affected Ghosts, Dust Sprites, and Bugs);
Bouncer trying to face the player in some cases, resulting in an invalid sprite;
Linus starting some days slightly clipped into a wall;
Maru reverting to her plain clothes after her event in the clinic;
Penny's hair missing some pixels when she's sitting on the bench in town;
player hat rendered incorrectly in the inventory at night time;
player with an item raised above their head rendering with hands lowered in some specific cases;
player sprite rendered incorrectly after collapsing while in a bathing suit;
player sprite facing upwards after interacting with a child or festival NPC from above;
player 'wiggling nose' animation issue when casting or reeling in a fish;
player eye color drawn incorrectly when reeling in a fish;
player sprite flickering to invalid animation frame when tapping a movement key while aiming the fishing rod;
kissing your spouse in multiplayer sometimes showing an incorrect idle frame (sometimes facing the wrong direction);
various issues related to animations played as part of an NPC's daily schedule;
the light halo overlay on outdoor lamps sorting incorrectly at some positions;
health bar not fully consistent with stamina bar.
Fixed Penny washing dishes too far from the sink.
Fixed issue where rebinding keys would cause the game to display a blank options screen instead of properly prompting the player to press a key.
Fixed switching toolbars causing incorrect animations to play in multiplayer.
Fixed various issues related to the frogs that sometimes spawn on rainy days.
Fixed ancient seed not visible in inventory.
Fixed edible items that don't grant energy displaying the poisonous icon in tooltip.
Fixed being able to click title menu buttons before they're visible.
Fixed visual issues when swapping toolbars away or onto light sources.
Fixed some dialogue boxes being positioned incorrectly.
Fixed issue where hovering over the dialogue friendship jewel would show a friendship level tooltip, even if the jewel wasn't currently visible (e.g., when answering a question).
Fixed visual seams appearing in Junimo Kart.
Fixed visual bug in the character's health bar when their health is low.
Fixed screen fading out twice when using Robin's building related functions.
Fixed resizing the window below the minimum size causing black bars on screen.
Fixed resizing the window or adjusting zoom resetting the scrollbar position on the options screen.
Fixed social tab showing one gift given if you gave two gifts that week plus a birthday present.
Fixed minor visual bug when switching between the ""Host"" and ""Join"" tabs in the co-op screen.
Fixed non-centered text alignment in the header of Robin's build menu.
Fixed the host player seeing light sources from Ghosts in the mines, even while not in the mines.
Fixed purchasing stacks of player-sold items from Pierre not visibly reducing the stack available.
Fixed lighting in the mine changed as soon as you start going down a ladder, instead of after loading the next level.
Fixed screen briefly fading to black during Grandpa's evaluation.
Fixed players sometimes getting stuck on a black screen during weddings.
Fixed pulling the last piece of hay out of a hopper not making it display as empty.
Fixed several issues with weather debris displaying incorrectly.
Fixed issue where white or black customization colors would cause the HSV sliders to show invalid numbers upon using the Shrine of Illusion.
Fixed visual issues with the cutscenes while riding the bus.
Fixed issue where holding 'up' while taking the bus back from the desert would cause the camera to pan during the cutscene.
Fixed trees planted in the desert showing their snow-covered sprites in winter.
Fixed issue where player animations weren't properly looping during events.
Fixed issue where Journey of the Prairie King showed -1 lives after losing all of your lives.
Fixed issue in Robin's menu where the Deluxe Barn's description overlapped the materials list.
Fixed minigames / cutscenes not always centered when the game is zoomed out.
Fixed the order of recipes in the Collections menu and the kitchen stove menu not matching.
Fixed weather debris clumped up after playing a minigame.
Fixed alignment of several UI elements in Calico Jack minigame.
Fixed falling leaves in the Secret Woods not showing their seasonal variants in fall.
Fixed campfire lighting misaligned.
Fixed map showing the player in the mines when they're in the Skull Cavern.
Fixed menu backgrounds not shown in the Community Center bundles menu.
Fixed the map's location label partly rendered off-screen in some cases.
Fixed white borders sometimes drawn on pieces of debris/dropped items.
Fixed mouse cursor not showing gift icon when gifting items to an NPC you're meeting for the first time.
Fixed mouse cursor showing ""talk to"" when hovering over NPCs during non-interactable events.
Fixed issue where harvesting from a crab pot while your inventory was full would result in multiple 'inventory full' messages without the red 'X'.
Fixed crab pots positioned/rendered over tiles they should be being drawn behind.
Fixed issue where opening a door would briefly show a ghost door.
Fixed windows in barns, coops, and spouse rooms providing light on rainy days or when it's dark out.
Fixed inconsistent particle effect when you right-click vs left-click to sell in the shop.
Fixed digging up an artifact spot in the rain leaving a dry spot.
Fixed hoeing in the desert when it's raining elsewhere leaving a wet spot.
Fixed outdoor garden pots not showing watered sprite when it's raining.
Fixed the health bar 'bleeding' at low health values if the health bar isn't currently being drawn or if the screen is currently fading.
Fixed double cursor in the Stardew Valley Fair slingshot minigame, and disabled drawing of it when playing with controller.
Other bug fixes
All mine songs now show up in the Jukebox list.
Fixed crash on startup if no audio output device is detected.
Fixed resolution issues for some players.
Fixed music not properly playing at the start of a new day.
Fixed Junimos not behaving properly after completing the community center.
Fixed issue where other players could affect the music that's playing in the mines.
Fixed various bugs caused by simultaneous buffs and debuffs.
Improved confusing error if certain game files are missing.
Fixed issue where players wouldn't get museum achievements until the next time they donated something themselves.
Fixed Master Angler achievement awarded before catching all fish.
Fixed issue where the music in Leah's house didn't stop after leaving her house.
Fixed issue where the wallpaper and flooring would shift rooms as you upgraded your house.
Fixed skipping an event sometimes making the player wait on a black screen for a while.
Fixed placing a bomb and leaving the screen before it explodes causing a buggy sound effect.
Fixed placing a bomb that destroys weeds and leaving the area before the explosion happens still playing the weed destruction sounds.
Fixed options menu being scrollable while a dropdown list is active.
Fixed an issue where a female character's pants could have their color changed to black during a wedding ceremony.
Fixed issue where you'd need to click twice to clear the dialogue that appeared after falling down a mineshaft.
Fixed issue where hotkeys (like opening the inventory) triggered after sending a chat message.
Fixed issue where dialogue couldn't be advanced using keyboard inputs.
Fixed issue where the music would sometimes briefly play again after you had turned the volume down.
Fixed issue where rebinding the menu key closed the options menu.
Fixed Prairie King game over screen only accepting input every 1.5 seconds.
Fixed Prairie King music overlapping if you use the zombie powerup multiple times in quick succession.
Fixed mouse repositioned when clicking HUD zoom buttons, making it harder to zoom in/out multiple times.
Fixed issue where loading a save would play the furnace and fireplace sounds.
Fixed volume levels not properly applied when connecting to a game as a farmhand.
Fixed rare crash or invisible grass caused by grass sizes becoming invalid.
Fixed clicking without dragging with the slingshot consuming ammo but firing nothing.
Fixed crash after eating a Maki Roll while playing in French.
Fixed Prehistoric Tibia and Amphibian Fossil not appearing in the forest (or anywhere) while playing in Spanish.
Fixed Wild Plums listed as forage instead of fruit in non-English languages.
1.3.36
Stardew Valley 1.3.36 was posted to the beta channel on 24 January 2019 and released on 1 March 2019.

Changes
Added French, Korean, Italian, Hungarian, and Turkish language support
Added a new Credits menu to replace the ""About"" menu on the title screen, and removed the /credits chat command.
Removed unneeded tilesheet files under Content.
Removed translated map files under Content/Maps (translated tilesheets are now loaded automatically).
Centered shed door.
Bug fixes
Fixed bug where music doesn't correctly play in the morning.
Fixed bug where giving a gift to an NPC caused the gift action to happen twice in rapid succession.
Fixed furniture rotation bug for ""stools"".
Fixed Junimos not disappearing after the final goodbye.
Beta Versions (1.3.35–1.3.36)
1.3.33
Stardew Valley 1.3.33 was posted to the beta channel on 18 December 2018 and released on 8 January 2019.

Changes
Improved performance in locations with lots of light sources (e.g., torches).
Improved performance on farms with lots of animals.
Tweaked new ""H"" health icon.
Bug fixes
Fixed multiplayer visual glitch when another player uses a tool and then starts moving in a new direction.
Fixed ""fishing stance"" visual glitches in multiplayer.
1.3.32
Stardew Valley 1.3.32 was released on 19 November 2018. (Versions 1.3.29–1.3.32 were all posted on the beta channel, and are included in 1.3.32.)

Changes
Optimised network performance.
Added more ways for the host to unpause the server (by pressing ESC, B, Back).
Added alternative way to show the chat box (press right stick button on a controller).
Added internal changes for modders (should have no effect on vanilla gameplay).
Bug fixes
Fixed slimes not pouncing correctly.
Fixed items-crafted stat being 0 after loading a file until you craft something again.
Fixed rabbit's foot not affecting the breakup scene in non-English language modes.
Fixed some rare tool-related crashes in multiplayer.
Fixed an audio-related crash often caused when using bombs.
Fixed Junimos dropping crops on the floor instead of collecting them.
Fixed NPCs walking through the saloon doors.
Fixed a few rare-ish crashes that can occur when connecting to another player's game.
Fixed eye color being reset to brown after reloading.
Fixed inability to use rod / slingshot in festival minigames.
Fixed the title menu back button being hidden behind submenus.
Fixed inability to move after receiving a gift at the Feast of the Winter Star.
Fixed crash when saving after the game adds Lewis' shorts to Marnie's house.
Fixed crash viewing the map when another player is in certain events.
Fixed ""double sound"" when using singing stone.
Fixed a Geneva Convention violation (by replacing red crosses in graphics).
Fixed clicks on the chatbox not being registered when the game is paused.
Fixed players sometimes walking off in a straight line through all terrain when they get disconnected.
Fixed babies sometimes spawning in houses that don't belong to the parents.
Fixed the potential for overnight events to cancel or skip a wedding event.
Fixed a desync that could occur if a player tried to get into the casino after a different player has removed the bouncer.
Fixed players able to simultaneously build overlapping farm buildings.
Fixed only one player being able to get the dark talisman.
Fixed the inability to place donated items on the bottom two rows of the museum.
Fixed farmhands being unable to pick up items they dropped in festivals.
Fixed Pam's upgraded house interior event not happening.
Fixed visual glitches caused by triggering a shared event simultaneously with the return scepter.
Fixed softlock caused by mistaken ability to use daggers during events.
Fixed players all receiving the same personal overnight events on the same day (e.g., spouse asking if you want a baby).
Fixed attempting to demolish cabins of abnormally disconnected farmhands causing the cabin to be destroyed after you leave the buildings menu.
Fixed farmers getting stuck in the fishing casting animation after picking up someone else's rod.
Fixed host sometimes appearing stuck using tool in shared events.
Fixed soft-lock when you play the Journey of the Prairie King past 2am.
Fixed glitched chest lid appearing when you destroy a cabin.
Fixed duplicate songs in the jukebox.
Fixed only one player's glow ring working at a time.
Fixed farmhands not being able to see each others' bundle changes.
Fixed bug causing some players to become married to two other players in 3-4 player farms.
Fixed player sometimes getting stuck in bed after another player got in and then got back out.
Beta Versions (1.3.29–1.3.32)
1.3.28
Stardew Valley 1.3.28 was posted to the beta channel on 10 August 2018 and to the main channel on 14 August 2018.

Fixed bug preventing buildings from being placed where there would be a path/flooring tile in front of the door.
Fixed music not playing at the Luau festival and in town.
Fixed clients occasionally crashing in rainy weather.
Fixed server crash when there are no available network adapters.
Fixed the load menu showing 'no saved games' while searching for files.
Fixed crash that occurred if you collided with a monster on the first frame you entered a new location.
Fixed Robin forgetting that she's meant to upgrade a farmhand cabin if the farmhand disconnects abnormally.
Fixed farmhands not being able to set the next day's weather with rain totems.
Fixed farmhands 'swimming' inside their cabins if they're disconnected while swimming in the spa.
Fixed swimming-related visual bugs in on farmers' faces.
Fixed furniture rotation graphical glitch.
Fixed a crash that occurs if you try to copy an invite code while remotely logged into the machine the game is running on.
Fixed a German localization issue with '%Farm' appearing in text instead of the farm's name.
Fixed spouse NPCs not kissing farmers.
Various internal changes requested on the modding wishlist.
1.3.27
Stardew Valley 1.3.27 was released on 1 August 2018. (Versions 1.3.3–1.3.26 were only posted on the beta channel, and are included in 1.3.27.)

New content
Added multiplayer over GOG, Steam, or LAN. This includes chat with custom emotes, and new cabin buildings.
Added Night Market event.
Added Community Upgrade to carpenter's shop.
Added buildable shipping bins to Carpenter's Shop.
Added Abigail encounter at level 20 in the mines.
Added new heart events for Jas, Linus, Pam, Vincent, and Willy.
Added new heart events when player has given a bouquet to all bachelors or bachelorettes.
Added rare treasure chest rooms in Skull Cavern.
Added new monsters in the Skull Cavern: carbon ghost, iridium bat, and iridium crab.
Added secret notes and related quests.
Added new items: auto-grabber, cactus seeds, garden pot, wood sign and stone sign, solid gold Lewis statue, pearl, wedding ring, new paintings, and decorative items sold at shops and festivals throughout the year.
Added bear's knowledge, special charm, spring onion mastery.
Added Statue Of Uncertainty to change professions.
Added different stationery for letters from Sandy and the Wizard.
Added horse hats.
Balance changes
Increased purchase price of wood, stone, ores, and coal in the year 2+. The gold received for selling them is unchanged.
Reduced price of tub o' flowers from 1000g to 250g, and recipe from 2000g to 1000g.
Breaking small tree stumps now provides +1 foraging XP.
Once the player reaches the mine bottom...
prismatic shards and diamonds may rarely drop from any monster;
gems and purple mushrooms may be found when searching trash bins;
gems and purple mushrooms may be requested in NPC quests;
some monsters will be stronger.
Other changes
Once the player reaches the mine bottom, the Dwarf will attend weddings.
Golden pumpkin is now a universal love (instead of universal hate).
Tweaked several seed packet sprites.
Tweaked Mr. Qi dialogue portrait.
You can now add Mayor Lewis' shorts to the Luau soup.
You can now put fire quartz in the furnace to produce refined quartz.
You can now refill a Watering Can at the kitchen sink.
Fireplaces are now furniture, so you can move them around and buy different fireplaces.
Fruit Trees cannot be planted on top of decorations in the greenhouse border, or in the corners of the Greenhouse.
Replaced gold display on player menu from ""g"" to ""金"" in Chinese.
Improve Chinese translations:

Item name changes
Item description changes:

Items Changed
Bug fixes
Fixed crops harvested with a scythe not giving XP.
Fixed bee houses always giving wild honey when used on custom farms.
Fixed seeds planted out of season disappearing.
Fixed player collapse being cancelled when they open the journal.
Fixed animal mood & happiness bugs.
Fixed crystalarium item swap exploit.
Fixed furniture being usable to complete bundles.
Fixed some recipes allowing Milk, but not Large Milk.
Fixed crash that occurred after 596 hours of gameplay.
Fixed staircase being spawned on mine level 120 when a placed object is destroyed.
Several grammar/spelling fixes.
Fixed wheat seeds and bok choy seeds descriptions in Brazilian-Portugese.
Fixed spring sign on bathhouse in Chinese.
Beta Versions (1.3.0–1.3.27)
1.2.33
Stardew Valley 1.2.33 was a small bugfix patch released on 10 July 2017.

Changes
Fixed Linux/Mac builds distributing with MonoGame-built content instead of XNA-built content (which made modding more difficult).
1.2.32
Stardew Valley 1.2.32 was a small bugfix patch released on 10 July 2017.

Bug fixes
Fixed Linux install failing due to broken symlink in 1.2.31.
Fixed save files potentially getting overwritten if you start a new file after using exit to title.
1.2.31
Stardew Valley 1.2.31 was a small bugfix patch released on 7 July 2017.

Bug fixes
Fixed crashes related to use of slot machines.
Fixed some potential sources of unnecessarily increased memory usage.
Fixed some options in dialogue not being usable with a controller.
Localization fixes
Fixed the 'loading...' message on loading screens wrapping incorrectly in some languages.
English: fixed grammar in Luau community soup text.
Portuguese: fixed some dialogue options in Linus' 0 heart event being missing.
Portuguese: fixed the cut-off 'level up' title.
Russian: added more space for the '(single)' text on the social page.
1.2.30
Stardew Valley 1.2.30 was released on 12 May 2017.

Changes
The window mode preference is now shared between all save files.
Bug fixes
Fixed crash when the game switches window mode while loading a save.
1.2.29
Stardew Valley 1.2.29 was released on 02 May 2017. (Versions 1.2.27–1.2.28 were only posted to the Steam beta channel, and are included in 1.2.29.)

Bug fixes
Fixed crashes while loading save files.
Fixed crashes when going fullscreen.
Fixed crashes while saving due to slay-monster quest.
1.2.26
Stardew Valley 1.2.26 was released on 24 April 2017 with in-game support for six new languages. Beta versions were posted to the Steam beta channel starting 07 February 2017.

Changes
Translations for German, Spanish, Brazilian Portuguese, Russian, Japanese and Simplified Chinese.
When using a controller, the cursor will snap between menu buttons by default. If you disable that, the cursor will instead accelerate while moving.
When using a controller, pressing the back button will skip events that are skippable.
""Show sharper digits"" on Options menu removed.
Exit to Title has returned.
Added panda hat (TGP only).
Bug fixes
Fixed lighting shader not covering the whole screen on Mac and Linux.
Switching from ""Windowed Borderless"" to ""Fullscreen"" should now go straight to fullscreen instead of Windowed mode.
Wallpapers and floors no longer have a tile placement indicator showing a random object.
Increased stability of preference-saving code. This should fix infrequent crashes that require the player to delete startup_preferences.
Fixed a crash that can happen when a pig tries to spawn a truffle but there’s no space for it.
Fixed a couple of rare crashes that could have occurred at any time during the game.
Fixed a typo of the word ‘pronounce’ in the marriage event.
Fixed the player not getting the recipe for Cookies if they skip Evelyn's event.
Fixed farm animals not producing goods at high happiness.
Farming level now affects crop yield prior to level 10.
Beta Versions (1.2.0–1.2.25)
1.11
Stardew Valley 1.11 was released 06 October 2016, with a beta posted to the Steam beta channel on 05 October 2016.

Minor changes
All the new farms now provide some kind of fishing opportunity, although the riverland farm is still superior.
The riverland farm now has the potential to spawn splashing fish nodes and ore panning nodes.
You can place buildings on most grass areas.
On the riverland and forest farm maps, some bushes can be destroyed with an upgraded axe.
Digging spots now appear on the farm, although less frequent as elsewhere. The hill-top quarry also has a chance of spawning them.
The forage berry bushes are now affected by the Botanist perk.
Slime egg prices increased.
Placing a wicked statue in a slime hutch prevents the witch from visiting it.
On the forest farm map, forage items have a chance to spawn on any grassy area, not just in the west.
Spouses will now speak to you when they come home on Friday nights.
Bug fixes
Price of galaxy dagger has been corrected to 35,000g (it was at 350,000g).
The Artisan Perk description has been corrected to reflect the actual bonus (it's 40% now, but the description still says 50%).
Weeds in the Mutant Bug Lair no longer change with the season (and no longer turn to battery packs in the winter).
The Mutant Bug Lair now replenishes itself a bit each day.
Monsters in the Mutant Bug Lair will now always be mutant... not revert to the standard grub/fly after saving and reloading.
Meteorites can no longer land on top of stumps or boulders.
You can once again place buildings in the little-shaded strip right below cliffs.
Fixed some tiles on the farm maps.
Fixed problem with forage spawning under stumps.
Spring onions can be iridium quality with the botanist perk.
Truffles should no longer spawn in the water.
Pets should no longer be able to walk down the cellar stairs into the void.
The Outlaw boss in Journey of the Prairie King will warp back to the center of the map if he happens to run too far off screen in either direction.
Coop animals can now actually produce higher quality produce like they were supposed to.
Can no longer fill your watering can at a well that isn't fully constructed or in construction.
Trees can no longer spread off the map into the void, causing the game to start getting slower with each day. The new patch will retroactively remove these problem trees as well.
You should now properly see the final Joja cutscene, even if you switched to JojaMart after completing most of the Community Center.
Forage items shouldn't spawn in inaccessible places anymore.
Fixed issue where lightning that struck an object on the farm would cause that object to ""pop"" into the current map when the current map was not the farm.
When you reach level 5 or 10 in a skill, but haven't slept yet, it no longer shows your new profession as ""Desperado"".
Minor convenience adjustments and bug fixes.
1.1
Stardew Valley 1.1 was released 3 October 2016, with a beta posted to the Steam beta branch on 29 September 2016.

Marriage candidates
Marriage candidates now have an outdoor area on the farm that is unique for most spouses, and sometimes spend time out there, usually doing a unique behavior.
Sam has a little halfpipe and he does skateboard tricks.
Maru has a little gadget that she works on.
Abigail plays her flute.
Leah has a sculpture to carve.
Sebastian works on his bike.
Alex lifts weights.
Penny, Harvey, and Elliott read a book in a little potted plant garden.
Emily meditates in a crystal garden.
Haley takes pictures between two potted palm trees.
Shane has a little chicken hutch, and he stands by it holding his chicken ""Charlie"".
Shane can be married and has new dialogue, scheduling and heart events. There's a new door for Shane in Marnie's shop which is locked. Shane's ""spouse room"" is a mess, with a mini-fridge and a muddy path of footprints leading up to it.
Emily can be married and has new dialogue, scheduling and heart events.
Buildings
Robin has a few new buildings available for sale.
shed: an empty room which players can do whatever they like with. They can decorate it the same way as their house.
mill: lets players place wheat in it to make flour, or beets to make sugar. One wheat makes one flour. One beet makes 3 sugars. When they place wheat or beets inside, the flour or sugar will be ready the next morning. The little box on the right side of the mill acts like a chest and it will contain any product that has been milled the previous day.
The wizard now offers new farm buildings after completing a quest line. These buildings are very expensive and intended for end-game. They'll appear instantly after purchase, unlike Robin's buildings which require construction.
Junimo hut: Junimos will come out to harvest any mature crops within a certain distance of the hut. The Junimos will place the crops they harvest in the hut for players to grab at their leisure.
Water obelisk: when interacted with, it warps players to the beach, exactly like the beach warp totem. It's basically an infinite totem that lives on the farm.
Earth obelisk: just like the water obelisk except it warps players to the mountains.
Gold clock: prevents debris from appearing on your farm. Keeps fences from decaying.
Farm maps
There are now five farm maps to choose from when starting a new game. The basic map that exists currently, as well as a new map themed around each ""skill"". Each special map has a feature that makes it a little different, and each map also comes with unique decorations inside the players house.
Riverlands: lots of water on this map... limiting your farm space. Looks nice. Fishing is actually viable on this farm... In fact, players can catch the same fish that are available in town.
Forest: the forest encroaches on the land, limiting farming space. However, there are renewable stumps on the east edge of the map, and seasonal forage items spawn there as well. Also, there is a new type of ""weed"" that spawns on the farm, which looks unique and will always drop mixed seeds when cut.
Hill-top: lots of cliffs... and there is a special mining area in the southwest where ore will spawn, as well as unique geode-bearing stones.
Wilderness: lots of space to farm, looks a little more interesting than the basic farm with a large lake in the southwest corner, and a cliff along the east and south. On this map, bats and golems will come out at night. There is a new monster called a wilderness golem which is unique to this map. It behaves just like the stone golem, but has different drops. Its stats scale with player combat level.
Farm
Added coffee beans, which can be used to brew coffee.
Added a new farmhouse upgrade which adds a cellar.
Added casks, which can be used to age alcohol and cheese to higher quality and value. This includes a new iridium quality, which doubles the value of the base item.
Fruit trees can now produce iridium-quality fruit. The fruit trees increase quality by 1 star per year of age after reaching maturity.
Divorce
Players can now divorce their spouse by filing in a little book inside the mayor's house.
After divorce, your spouse moves out and returns to their old life, but any children will stay.
After divorce, ex-spouses are less friendly toward the player and have unique 'rejected' dialogue.
Using the Dark Shrine of Memory in the Witch's Hut will erase the ex-spouses memory, making it seem like you had never been married.
Other new content
Return scepter now sold by Krobus in the sewer for 2,000,000g. This item acts like a permanent warp totem to the farm. You can use it any time to warp back to the farm.
Two new fish can be caught in the wild: slimejack (Mutant Bug Lair) and void salmon (Witch's Swamp).
Inside a new building, the Witch's Hut there are three new shrines:
Dark Shrine of Memory - Will erase a divorced spouse's memory, making it seem like you had never been married.
Dark Shrine of Selfishness - Allows you to turn your children into doves (Gets rid of them) in exchange for a prismatic shard.
Dark Shrine of Night Terrors - Allows you to toggle having monsters spawn on your farm in exchange for a strange bun.
There are two new quests available which unlock the new Wizard farm buildings (available after completing either the Community Center or JojaMart goals).
Dark Talisman - Sends the player to a new area, the Mutant Bug Lair to retrieve a powerful magical artifact.
Goblin Problem - Sends the player to a new area, the Witch's Swamp.
honey can now be placed inside kegs to make mead.
casks can now be crafted to age alcohol and cheese which increases quality.
void mayonnaise can now be crafted from void eggs.
catalogue - Sold by Pierre, this expensive furniture piece allows players to purchase wallpaper and flooring from their home.
furniture catalogue - Sold by Robin. Like the catalogue, this can be placed at the farm and players can interact with it to purchase furniture.
New furniture - anchor, bamboo mat, boarded window, bonsai tree, burlap rug, candle lamp, carved window, ceiling flags, decorative lantern, floor TV, green cottage rug, hanging shield, Junimo plush, l. light string, metal window, miner's crest, monster danglers, monster rug, mystic rug, nautical rug, ornate lamp, ornate window, porthole window, s. pine, small crystal, tree column, woodcut rug, world map.
Galaxy swords, galaxy daggers and galaxy hammers are now purchasable from Marlon once you have obtained a galaxy sword.
Minor changes
New mail messages after the player passes out from exhaustion or dies. They'll sometimes end up at Harvey's clinic after passing out.
Strange bun had its price and cooking ingredients adjusted.
Giving someone a gift on their birthday will never make your spouse jealous.
You can now wallpaper the little hallways in your upgraded house.
When you beat Journey of the Prairie King, you can now start over in a harder mode, keeping your upgrades and coins.
2 new ""Lost Books"" to collect for the library.
You can now choose to color your chests with one of 20 color options.
NPC's now appreciate quality level in gifts, but it only has an effect on gifts they ""like"" or ""love"".
You can now move your buildings via Robin's construction menu.
Krobus now sells void eggs.
Slime balls now have a chance to drop petrified slime.
When paused, the time now blinks from black to gray, to black.
Added a graphics option to display ""sharper"" stack number digits.
Kegs no longer require clay to craft.
You can once again plant fruit trees around the edge of the greenhouse interior.
After seeing Shane's 8-heart event, each chicken you purchase from Marnie has a 25% chance of being blue. Aside from appearance, the blue chicken is identical to the white chicken.
Sunflower seeds can now be purchased and planted in summer in addition to fall.
Balance changes
All animal products are increased in value by 25% (rounded up to the nearest 5g)
The Rancher profession now increases the value of animal products by 20%, up from 10%
The Artisan profession now increases the value of Artisan goods by 40%, down from 50%
The Blacksmith profession now increases the value of metal bars by 50%, up from 25%
The value of blueberry is now 50g, down from 80g
The value of starfruit is now 750g, down from 800g
The value of cranberry is now 75g, down from 130g
The value of ancient fruit is now 550g, down from 750g
Cranberry sauce has had its value reduced to 120g
Stuffing has had its value reduced to 165g
Cranberry candy has had its value reduced to 175g
Blueberry tart has had its value reduced to 150g
Fruit salad has had its value reduced to 450g
Reduced sell price of cranberry seeds to 60g
Bug fixes
Wild plums are now labeled as fruit.
Grandpa's Shrine should now always properly give the reward for reaching 4 candles.
Fixed issue where gathering an item with the ""botanist"" profession would fail if the inventory was full, even though the gold-level item was present in the inventory.
Slime charmer ring should now protect against giant slimes.
You can no longer tap a stump.
Fixed Joja Warehouse graphic issue in winter.
""Check action""-mapped keys should now work to attach bait to a rod.
Rain ambient sound should no longer play in Sandy's Oasis under any conditions.
Your baby should now be properly born, even if you pass out in the mines on the eve of the birth.
Moonlight Jellies engagement crash.
Galaxy sword should now be truly unloseable.
You can no longer lose hay to a hopper because you have no silo.
You can no longer plant fruit trees off the farm.
Typo fixes
1.07a
Stardew Valley 1.07a was released on 29 July 2016.

Changes
Added Mac OSX and Linux compatibility.
1.07
Stardew Valley 1.07 was released 04 April 2016, with a beta posted to the Steam beta channel on 31 March 2016.

Gameplay
Spouses now say unique dialogues during festivals (most of the time), rather than the generic handful of dialogues.
Spouse stands next to you at the Dance of the Moonlight Jellies.
In-law dialogue... when you marry someone, their parents/relatives dialogues will change slightly to account for the change.
Sam, Sebastian, and Abigail now go to the saloon on Fridays after you marry them.
Spouses won't leave on rainy days, unless they have to go to work.
Fruit trees produce higher quality fruit as they age. (once per year, up to gold star).
Fruit tree harvesting now involves shaking the tree to drop the fruit.
Lightning strikes now have a unique effect on fruit trees.
Lightning is more likely to strike trees and crops, but lightning rods now have a very good chance of intercepting lightning strikes (if they aren't already processing a lightning bolt).
Charcoal kiln now requires 10 wood to produce 1 coal, down from 20.
Maxed-out friendship levels will no longer decay.
Added another digit to the shipping menu money counters.
Dying in the mines is less harsh: money lost caps at 5000g, rate of item loss reduced, can't lose rings or the galaxy sword.
Your assigned movement keys are now used within Journey of the Prairie King.
You can now retrieve powerups and coins that drop on the edge of the map in Journey of the Prairie King.
The last two hearts are now greyed-out for marriage candidates until you give them the bouquet.
Willy now likes most fish dishes.
Adventurer's Guild now sells monster slayer rewards after you've unlocked them (you still get the free reward).
Hardware mouse cursor option.
Bug fixes
Tools left in chests within farm buildings won't count as ""missing"".
Poppy properly consumed in poppyseed muffin recipe.
Can no longer incorrectly ""consume"" trap bobbers (click to make them disappear with strange sound).
Agriculturist profession no longer causes an extra 25% growth rate when speed-gro isn't present. Speed increase effects should now properly apply to very slow-growing crops.
Fixed sunflower seed price at JojaMart.
Chests should no longer appear on top of your in bed after upgrading your house.
Item placement with the gamepad improved, and there is now an option to show an item placement indicator.
Snow yams shouldn't appear in the desert anymore.
Sea cucumber should properly appear in the fishing tab of the Shipping menu.
Farm animals should no longer get stuck on fences placed directly to the right of the barn door.
Various minor bug fixes.
Minor changes/fixes to dialogue.
1.06
Stardew Valley 1.06 was released 20 March 2016.

Changes
Added unique dialogues for all spouses.
Spouses now leave the house on Mondays.
Value of most animal products increased.
Holly is now poisonous.
Missing events problem shouldn't happen anymore.
Minor bug/grammar/graphics fixes.
1.051b
Stardew Valley 1.051b was released 17 March 2016.

Changes
Fixed disappearing item problem. (items can still be destroyed by farm debris).
Kegs now require oak resin to craft.
Bee houses now require maple syrup to craft.
Tortilla price changed from 75g to 50g.
Nautilus shell (artifact) renamed to nautilus fossil.
1.051
Stardew Valley 1.051 was released 12 March 2016.

Changes
Fixed weird rectangle lighting problem.
Flooring can now be removed by bombs.
You can press a ""menu"" button (Esc or E by default) to close out of yes/no dialogues.
If no other function is mapped to it, the Y key will choose ""yes"" in a yes/no dialogue.
You can use the trigger buttons on a gamepad to navigate through the Community Center menu.
Minor fixes.
More secrets.
1.05
Stardew Valley 1.05 was released 9 March 2016, with a beta posted to the Steam beta channel on 4 March 2016.

Gameplay changes
Only read if you've gotten to year 3: Grandpa's had a change of heart... he feels he's been a little too harsh with his judgements. He no longer mentions ""great honors"", and his dialogue is a lot softer. If he's already visited you, check his shrine for a new opportunity...
Restored a ""lost"" Shane event.
Changed earthquake to Summer 3rd... to make it clear that it's the season change that kills crops.
Increased opportunities for iridium. The chance to find iridium in the Skull Cavern increases significantly every ten levels.
Added a zoom in/out feature to the options tab.
Added volume sliders for ambient sounds and footstep sounds.
Added snow transparency slider.
Added option to turn off flash effects.
Added lighting quality option.
Added quest (Rat Problem) to make it clearer that you have to investigate the Community Center.
Bug fixes
Leah's schedule has been fixed.
Spouses who have jobs won't get stuck in the bus area anymore.
Upgrading a house with crafted flooring should no longer cause a mess.
Restored more advanced NPC end-point behavior.
""Secret"" NPC's should no longer show up on calendar until you meet them.
Escargot, chowder, etc. should now properly give fishing buff.
You now truly cannot pass the bouncer.
You can no longer get stuck trying to board the bus.
Fixed issue with invisible trees preventing interaction with tiles.
Dead flowers no longer affect honey.
You can now dance with your spouse at the Flower Dance.
Game should now properly pause when steam overlay is active.
Fixed issue where inactive window was still responding to input.
Fixed fertilizer prices in Pierre's shop.
Fixed Fector's Challenge.
You can now press the toolbar shortcut keys (1, 2, 3, etc. by default) to change the active slot while the inventory menu is up.
Iron ore nodes can no longer be removed, only destroyed.
The dog or cat should no longer sit on chests...
Spouses less likely to run away into the dark abyss.
Naming your child after an NPC should no longer cause issues.
Fixed issue where recipes would sometimes consume more ingredients than they should.
Fixed crashes in certain cutscenes, when certain dialogue options were chosen.
Many small bug and typo fixes.
1.04
Stardew Valley 1.04 was released 1 March 2016.

Gameplay changes
Added a randomize character button to the character creation screen.
Robin now sells crafting recipes for wood floor, stone floor, and stepping stone path.
Added a secret new way to modify a rare item.
Increased grass growth rate.
Increased forage spawn possibilities, and made it much less likely for forage to spawn behind trees.
Reduced value of honey from 200g to 100g.
Raised Clint's ore prices.
Inventory menus now indicate which slot is the ""active slot"".
Made the meteorite look snazzier.
Bug fixes
Fixed problem with swinging sword while riding a horse.
Fixed strange lighting behavior when holding torches.
Fixed problem where stone fence was spawning debris.
Spouse should no longer get stuck on their way to town.
Wild seeds now produce the proper produce when in the greenhouse.
Secret gift exchange should now work properly.
All scarecrows now give reports on their crow-scaring activity.
Bouncer is now truly impassable.
Trees no longer grow directly in front of warp statues.
Willy's shop no longer counts as water.
The meteorite should no longer appear in the pond or buildings.
If an object is ever directly underneath you, preventing you from moving, right click to remove it.
Mariner and Luremaster professions should now work properly.
Tappers are now properly destroyed by bombs.
Fixed bathing hairstyle inconsistency.
Fixed various item duplication and stacking issues.
Poppyseed muffin now actually looks like a muffin.
Quest items should no longer disappear when you die.
You can no longer give quest items to the wrong person.
The Skull Cavern quest can no longer be completed before receiving the actual journal entry.
1.03
Stardew Valley 1.03 was released 28 February 2016.

Gameplay changes
The cooking menu now looks for items in your refrigerator as well as your inventory.
Scarecrow range reduced to an 8 tiles radius.
The price of mayonnaise and other artisan animal products now increased by the rancher profession.
Once you befriend someone to 2 hearts, their room is permanently unlocked, even if you go below 2 hearts again.
The 'auto run' option is now enabled by default.
Bug fixes
Fixed duplicate item issue in the mines.
Ladders should no longer spawn underneath the player, locking them in place.
Fixed problems with the Community Center menu. You can now throw items down and delete them (Delete key) in the Community Center menu.
Fixed item quality exploit.
You can now throw items down while in the crafting menu.
If you destroy the stable, you can now rebuild it.
Spa won't recharge you while the game is paused (e.g., steam overlay up).
Fixed problems with the Stardew Valley Fair fishing game.
Various stability fixes.
1.02
Stardew Valley 1.02 was released 27 February 2016.

Changes
If you never received your pet, you will now have another opportunity.
When you go to sleep, the game now checks if any of your essential tools are missing. If so, they will appear next to your bed in the morning.
The game now properly saves on level-up nights.
Eating skill food and then going to bed on a level-up night works properly now.
Fixed problem where completing the Crafts Room while standing in certain locations causes your character to get stuck.
Player character should now walk correctly while in a cutscene after drinking coffee.
Removed an item duplication exploit.
Forage items should no longer spawn in the woods above the beach.
Fixed crab pot problem (again).
Fixed exploding armchair bug.
Relaxing in the spa with your menu up no longer restores energy.
Removed Leah's doppelganger.
Debris should no longer spawn in the pathway between farm & forest.
Fixed disappearing tapper problem.
Fixed a few minor graphics problems & dialogue typos.
Other minor issues (spoilers).
Tools being upgraded no longer erroneously considered ""missing"".
Clay duplication bug fixed.
Pet cutscene won't refer to the ""cat"" as a ""dog"" anymore.
Leah's 10-heart event fixed.
Pepper Popper recipe now requires cheese.
1.01
Stardew Valley 1.01 was released 26 February 2016.

Changes
Fixed random crash when going to sleep after getting your pet.
Sound effect volume now properly applied on load.
The error item should no longer appear for sale in the saloon.
Z key is now bind-able.
No longer possible to exit the Community Center menus while holding an item.
Finding your first artifact from a geode no longer destroys remaining held geodes.
Mature wild crops no longer disappear on day of load.
Crab pots can now be placed and removed without issue.
Fixed crash when inspecting chests in the mines.
You can no longer place chests in the mines.
Tree seeds no longer incorrectly show a ""Wrong Season"" message.
Fixed some map issues.
1.0
Stardew Valley 1.0 was released on 26 February 2016."
17,https://alanwake.fandom.com/wiki/Alan_Wake_2,"Alan Wake 2
Why the hell did you kill Casey? What the hell were you thinking, man?
This article or a section of this article will contain full, or partial plot spoilers of an Alan Wake game or any other piece of media related to the franchise.
You have been warned...

""This story... is a monster. And monsters wear many faces.""
― Alan Wake

Alan Wake 2 (stylized as Alan Wake II) is a survival-horror video game that released on 27 October 2023 on digital storefronts for the PlayStation 5, Xbox Series X|S and PC through the Epic Games Store. It is the sequel to 2010's Alan Wake. The game had an expansion DLC released on June 8, 2024 called Night Springs and a second expansion DLC, The Lake House, is slated to be released sometime later. The game has four difficulties: Story, Normal, Hard, and Nightmare, the fourth of which is only available in ""The Final Draft"" mode. The Final Draft is a New Game Plus mode for Alan Wake 2, featuring new Manuscript pages, other new collectibles, a new expanded ending, and other changes to the game's story. ""The Final Draft"" was released on December 11, 2023.

Patch notes for Alan Wake 2 updates can be found here.


Contents
1	Synopsis
2	Plot
3	Chapter List
3.1	The Final Draft
3.2	Expansion 1: Night Springs
4	Gameplay
5	Development
5.1	Initial development
5.2	Remedy Connected Universe
5.3	Official development
6	Reception
7	Trivia
8	Gallery
8.1	Official Images
8.2	Concept art
8.3	Concept art (earlier iteration)
9	Videos
10	Sources
Synopsis
Quote1 A string of ritualistic murders threatens Bright Falls, a small-town community surrounded by Pacific Northwest wilderness. Saga Anderson, an accomplished FBI agent with a reputation for solving impossible cases arrives to investigate the murders. Anderson’s case spirals into a nightmare when she discovers pages of a horror story that starts to come true around her.

Alan Wake, a lost writer trapped in a nightmare beyond our world, writes a dark story in an attempt to shape the reality around him and escape his prison. With a dark horror hunting him, Wake is trying to retain his sanity and beat the devil at his own game.

Anderson and Wake are two heroes on two desperate journeys in two separate realities, connected at heart in ways neither of them can understand: reflecting each other, echoing each other, and affecting the worlds around them.

Fueled by the horror story, supernatural darkness invades Bright Falls, corrupting the locals and threatening the loved ones of both Anderson and Wake. Light is their weapon—and their safe haven — against the darkness they face. Trapped in a sinister horror story where there are only victims and monsters, can they break out to be the heroes they need to be? Quote2
― Epic Games Store page description
Plot
See also: Alan Wake, Alan Wake's American Nightmare, and Control
In 2023, thirteen years after the events of Alan Wake, a naked man emerges from the dark shores of Cauldron Lake, experiencing visions of a man violently screaming before he is found and has his heart cut out by a group of men wearing deer masks.

FBI Special Agent Saga Anderson and her partner Alex Casey are dispatched to the town of Bright Falls, Washington to investigate a series of ritualistic murders. They investigate the latest victim, the naked man, who is revealed as former FBI Agent Robert Nightingale. It is believed he was murdered by a group calling themselves the ""Cult of the Tree"". In addition to Nightingale's corpse, Saga finds a mysterious manuscript page that seems to predict the future. While in the town, they encounter Sheriff Tim Breaker, who promises to assist their investigation, as well as waitress Rose Marigold, who recognizes Saga and acts like Saga was a local of the area, despite this being her first visit to the town. She also claims Saga's daughter Logan drowned several years ago, despite being alive in Virginia with her father. At the morgue, Saga then performs an autopsy on Nightingale's corpse, but it suddenly reanimates, in search of the ""Clicker"", and escapes the morgue after Breaker mysteriously vanishes.

As they pursue Nightingale to Cauldron Lake, Saga uses her psychic powers to investigate the scene. Casey reveals he had investigated a murder cult in New York City dedicated to bringing back the missing author Alan Wake by reenacting the murders described in his books, and that Nightingale came to Bright Falls to pursue Alan thirteen years prior. Saga encounters Nightingale, now converted into what the manuscript called a ""Taken"", and is forced to kill him. Saga then finds a very-shaken Alan Wake washed up on the shore of the lake and takes him into custody while also discovering evidence that an organization called the Federal Bureau of Control has a presence in Bright Falls. Saga and Casey take Alan back to the Elderwood Palace Lodge in their field office where he recounts how he escaped from a dream-like dimension called the Dark Place.

While trapped in the Dark Place, Alan continually tried to find a way to escape. After appearing in a mysterious talk show called ""In Between With Mr. Door"", Alan found an Angel Lamp once belonging to Thomas Zane and connected to the Clicker that, in combination with his writing ability, allowed him to manipulate the Dark Place to better navigate it. Alan began navigating a dark, twisted version of New York City, following the trail of Alex Casey's investigation of the ""Cult of the Word"" led by Alan's evil doppelganger Scratch, who survived his erasure from existence, which brought him through the various scenes of the murders committed by the cult. Along the way, Alan also encountered Breaker, who was searching for the talk show host, Warlin Door, believing him to be responsible for trapping him in the Dark Place. Alan also occasionally made brief contact with Saga, the two of them trading vital information when they could. After believing he had killed Tom Zane, who had gone insane from his captivity inside the Dark Place, Alan was contacted by an alternative version of himself, explaining that his repeated attempts to escape the Dark Place are causing him to experience time loops.

Back in the present, Alan explains that he wrote a new novel, ""Return"", that helped him escape the Dark Place. However, Scratch re-edited the manuscript into a horror story now taking effect in reality. Alan warns that Scratch is searching for the Clicker, which is the key he needs to free the malevolent Dark Presence completely, but also the key to permanently defeating it. When one of the manuscript pages mentioning the Cult of the Tree possesses the Clicker, Saga decides to follow its trail in Watery and realizes that ""Return"" has rewritten her past so that her daughter Logan drowned in an accident. Angered that Alan wrote her and Logan into his story, Saga recovers the Clicker, but before she can get it to Alan, agents from the FBC led by Agent Kiran Estevez arrive and apprehend him as well as the Cult leaders, revealed to be town entrepreneurs Ilmo and Jaakko Koskela.

With no other options, Saga goes to the Valhalla Nursing Home and tracks down Odin and Tor Anderson, who know more about the Clicker. As she communicates with them and rescues Tor from a Taken Cynthia Weaver, she learns that Odin and Tor are actually her grand-uncle and grandfather, respectively, and she inherited their seer abilities, allowing her to discern the changes to reality ""Return"" is causing. Odin and Tor also explain that the Clicker does not do anything by itself, but instead dramatically enhances the creative power of the person using it, which is why it is important to Alan and Scratch. Saga then heads back to Bright Falls to get the Clicker to Alan, but finds out that the Alan who escaped Cauldron Lake was actually Scratch. With his identity now exposed, Scratch kills Jaakko and escapes from captivity, attempting to take the Clicker from Saga. He is temporarily thwarted and banished by the FBC.

Ilmo reveals that he and Jakko formed the cult in order to scare townsfolk away from the dangers of the lake, while they perform their ritual killings on whatever Taken emerges from it, such as Nightingale. Realizing that Alan is still trapped in the Dark Place, Saga enlists the help of Casey, Odin, Tor, and Estevez to carry out a ritual to summon him to the real world. Meanwhile, Alan continues to try and find a way out of the Dark Place, eventually making his way to his old apartment. There, he discovers his wife Alice had been tormented with visions of Scratch, eventually leading to her apparent suicide. In anger, Alan kills who he believes is Scratch, but is actually a version of himself attempting to fix Scratch's ending of ""Return"", perpetuating the loop.

Back in the real world, the summoning ritual fails to summon Alan in the current time and instead was responsible for his initial appearance earlier in the story. Both Saga and Alan come to the realization that Alan and Scratch were always the same person, with Scratch being Alan, who is possessed by the Dark Presence at the end of every loop, and thus indirectly responsible for Alice's death. Scratch arrives at the summoning site and Saga is able to banish him from Alan's body, only for Scratch to possess Casey instead, steal the Clicker, and throw Saga into the Dark Place.

With Scratch in possession of the Clicker, Alan concludes that the only way to stop him now is to find the manuscript and write a completely new ending for ""Return"". He returns to his writing room and tries to work out how to write a perfect ending that saves everybody while staying consistent with the horror genre. Meanwhile, trapped in her mind place, Saga fights off the Dark Presence's attempt to overwhelm her with her own negative emotions and self-doubt. Now in the Dark Place proper, she reunites with Breaker, who remains there in his search for Door, heavily implied to be Saga's missing father. She then receives aid from an anonymous woman who directs her to the Clicker and a Bullet of Light. Saga takes the two items and escapes back to the real world by mimicking Door's actions on a manuscript page given to her by Breaker. With the Clicker, Alan is able to banish Scratch from Casey's body. Scratch returns to possessing Alan, and Saga shoots him with the Bullet of Light, seemingly killing him and Scratch. As Casey recovers from his possession, Saga tries to call Logan to confirm her safety, but the scene cuts short before any answer can be heard.

In a post-credits scene, a recording left behind by Alice reveals that she had managed to fully regain her memories of what happened to Alan after consulting with the FBC at the Oldest House. She had tricked Alan into thinking she committed suicide by choosing to return the Dark Place as part of a plan to help him eventually escape, explaining that the only way he can escape the loops is through ""ascension."" Alan then revives from his gunshot wound and says, ""It's not a loop, it's a spiral.""

In the ""Final Draft"" ending, Alan realizes that the Bullet of Light was meant to cleanse a part of himself the Dark Presence used to create Scratch, finally killing his doppelgänger for good while finishing the spiral and freeing Alan from the loop. Saga's call connects, confirming Logan's survival, and Alan revives soon after to the surprise of her and Casey, declaring himself the master of not just two worlds, but many.

Chapter List
The chapters/parts listed here are played in the following order:
Prologue: The Cult
Return 1: Invitation
Return 2: The Heart
Initiation 1: Late Night
Initiation 2: Casey
Initiation 3: Haunting
Players have the choice to play the following of Alan and Saga's chronological chapters/parts in any order they wish:
Return 3: Local Girl
Return 4: No Chance
Return 5: Old Gods
Return 6: Scratch
Initiation 4: We Sing
Initiation 5: Room 665
Initiation 6: Return
Initiation 7: Masks
Initiation 8: Zane's Film
The chapters/parts listed here are past the point of no return and are played in the following order:
Return 7: Summoning
Initiation 9: Gone
Return 8: Deerfest
Return 9: Come Home
The Final Draft
Remedy released a New Game Plus update to the game on December 11th, 2023 named ""The Final Draft"".

Expansion 1: Night Springs
Set as ""episodes"" of the Night Springs TV show, these can be played in any order the player wishes.

Number One Fan
North Star
Time Breaker
Gameplay
Similar to the original, the game is a third-person shooter, with players using light to burn away the darkness of the enemy and various weapons to fight against the dark forces as well as various cult members. Unlike the first game, which was an action-adventure game, Alan Wake 2 emphasizes survival-horror gameplay. Players have an inventory with limited space, where they will need to conserve their ammo and healing items. Health items themselves consist of med kits and painkillers, with Safe Havens only healing the player to take them out of a ""critical state."" Players also encounter various puzzles and locked boxes that they'll need to solve in order to obtain upgraded gear.

The game features two distinct campaigns, one where you play as the titular Alan Wake, and one where you play a new protagonist, Saga Anderson. The two campaigns blend together, eventually allowing you to switch between the two at various points, using a dark puddle found in certain Break Rooms if the player wishes, with the two stories both foreshadowing and echoing each other. Eventually though, the player will be required to complete both stories before progressing past a point of no return.

Saga’s story takes place in the Pacific Northwest, consisting of three hubs: the small town of Bright Falls, the stunning primordial forest surrounding the mysterious Cauldron Lake, and the run-down town of Watery. Alan's story takes place solely within the Dark Place in a warped version of New York City, with his reality shifting and looping. Similarly to the episodic nature of the original, the narrative is divided into chapters, which each having their own accompanying end song. Between chapters, players are returned to the hubs and are allowed to explore or move to the next chapter at their own pace.

Whilst Saga's story involves a lot of talking to various townsfolk around the area in order to progress in her environment, Alan uses an Angel Lamp to allow him to change a scene by transferring light from one area of New York to another. Both characters have their own ""mind palaces"" to help progress their stories, with Alan having his Writer's Room to help change the story and Saga having her Mind Place to help her with her investigations, both of which can be accessed at the press of a button.

Development
Initial development
Throughout 2011 and 2012, several hints had been dropped of a potential sequel to Alan Wake. These hints include when ""Alan Wake 2"" was shown on the Artist Althea Suarez Gata's CV, only for it to be removed on the very same day. Whilst Alan Wake's American Nightmare (though the name was unknown at the time) was revealed not long after this hiccup, fans had been trying to find more evidence of a direct sequel. In Alan Wake's American Nightmare, the credits to the game also then indicated that ""Alan Wake's journey through the night will continue"".

Also in 2012, Sam Lake tweeted a link to a ""Blogspot"" called ""This House of Dreams."" At first, the site seemed like a mere blog for someone refurbishing a house, but one such blog talks about a dream the blogger had about a man ""wearing a funny jacket with old-fashioned elbow patches"" and that he wanted ""to turn all the lights on."" This tweet came just days before E3 2012, a gaming expo that reveals and shows brand new content of upcoming releases. People also found that the domain ""alanwake2.com"" was also reserved by Remedy Entertainment.

At the Xbox Event on May 21, 2013, Remedy revealed Quantum Break, which would be an exclusive to the newest Xbox console at the time, Xbox One. On February 2016, it was confirmed that it would also come to PC. Fans were disappointed that Alan Wake 2 wasn't revealed, but some still supported Remedy in their new franchise. A day after the Quantum Break announcement, Sam Lake took to YouTube to talk to the fans about why Alan Wake 2 wasn't announced. He revealed that the time just wasn't right yet, but mentioned he had not given up on the franchise.

In April 2015, Remedy announced that Alan Wake 2 had previously gone into the early stage of development shortly after Alan Wake was released, where a video of prototype demo was also released. While in the stage of showing this demo to publishers, Remedy went into conversations with Microsoft and the project was put on hold indefinitely while Quantum Break went into development. The sequel would have introduced new features, many of which were used in Alan Wake's American Nightmare. When asked about the possibility of an Alan Wake sequel being released in the future, Sam Lake replied, ""It's hard to guess,"" but that he would ""love to do that"".

Remedy Connected Universe
Easter eggs in Remedy's 2019 game Control revealed that the game shared the same universe as Alan Wake. Two typewritten pages found within the game (and which were said to have been written by Alan Wake himself) heavily imply that almost ten years had passed since Alan's experience in Bright Falls, and that it was not an isolated event, but rather what the Federal Bureau of Control classifies as an ""Altered World Event"" (AWE). In this way, Control revealed additional information about the events involving the Dark Presence, especially given the nature of its own primary enemy, the Hiss.

Furthermore, the second DLC for Control, AWE, was released in August 2020. While the letters can simply stand for ""Altered World Event,"" they also resemble Alan's own name. In addition to this, the top half of the cover art depicted a flashlight-wielding person in a dark forest, while the bottom half depicted a second figure surrounded by red, which figures prominently with the Hiss in Control.

In an interview given on May 14, 2020 with VG24/7, Sam Lake commented on this, explaining, ""Through AWE you’ll learn more about the Bureau’s involvement, research, and interest in Alan Wake and the events that took place 10 years ago,” Lake said. “We’re really happy to give fans of the original game a bit more of Alan Wake-related content.""[1] This suggested a deeper connection, possibly even a future crossover, between the events of Control and Alan Wake.

The plot of AWE confirmed this, tying up some loose ends from the previous Alan Wake entries while also setting up details which would come in a sequel work. In the ending of AWE, an alarm sounds in the Federal Bureau of Control that another AWE appears to be happening again in Bright Falls. This, along with Alan's narration, seemed to indicate Remedy's next game could be a follow up to Alan Wake.

Official development
In March 2020, Remedy signed a deal with Epic Games to publish two single-player games under Epic's new publishing initiative. The games were said to be part of ""the same franchise"". In March 2021, Remedy CEO Tero Virtala confirmed that they were indeed working on two unannounced project with Epic Games that are part of the same franchise, one smaller scale and one AAA in size. In April 2021, GamesBeat reporter Jeff Grubb told viewers that ""I've heard that Remedy is going to be making Alan Wake 2,"" as part of the Remedy-Epic deal. Grubb went on to say that the game ""should kind of be the follow-up that fans of that series want,"" but that there's little extra information and no word on an official announcement.

On the 7 September 2021, Alan Wake Remastered was revealed by Sam Lake, of which some people believed to have been the smaller project mentioned in March 2021, and the other AAA from the same franchise was to be Alan Wake 2. With the reveal of Alan Wake Remastered, which indicated Remedy are trying to generate interest in the franchise, a sequel was looking more and more likely.

At The Game Awards 2021, Alan Wake 2 was officially announced for 2023 and would be Remedy's first survival-horror video game. Like Quantum Break and Control, it was announced that Alan Wake 2 would use the Northlight engine and be a third-person game. On a later interview with IGN, Sam Lake revealed that the game would have a Mature 17+ age rating.

On May 24, 2023, Sony hosted a PlayStation Showcase livestream, in which a gameplay trailer for Alan Wake 2 was revealed, with a release date of October 17, 2023. It was revealed that players would control two protagonists within their own campaigns, Alan Wake himself and a new protagonist, Saga Anderson. At Summer Game Fest on June 8, 2023, Sam Lake shared further information on the ways Alan and Saga's campaigns would be connected to each other. Additionally, more gameplay and combat mechanics were shown.

On August 17, 2023, the game was delayed by 10 days to October 27, 2023 to make way for various other games also releasing in October. At Gamescom 2023, Alan Wake's side of the game in the Dark Place was focused on in a new trailer, which was later followed by a lengthy Alan-centric gameplay featured behind closed doors.

Reception
Alan Wake 2 received near universal acclaim from video game critics. Itr received five nominations at the 2023 Golden Joystick Awards and won the Critic's Choice Award. It was nominated for eight awards at The Game Awards 2023, and won for Best Game Direction, Best Narrative, and Best Art Direction.

The game went on to win more awards in Finland and around the world.

Trivia
Ilkka Villi and Matthew Porretta both reprise their role as Alan Wake in the sequel. James McCaffrey reprises his role as Alex Casey, who he previously voiced in the manuscript pages of The Sudden Stop.
Brett Madden, who played Alice Wake in the first game, sadly passed away in September 2020. She was memorialized in the credits of the game. Alice was recast in the sequel, now played by Christina Cole.
While the first game was an action-adventure game that implemented horror elements, Alan Wake 2 is a full-blown survival-horror game.
Unlike the first game that was published by Microsoft Studios, the sequel is published by Epic Games Publishing.
The game was initially a digital-only release due to various shifts towards a digital-only market. This was also done to keep the price of the game down at launch. However despite initially saying there were no plans for a physical release, in June 2024 along with a trailer and release date announcement of the first DLC it was revealed that a physical version would be released that October, a year after the digital release. Both a physical version of the digital deluxe version (inclusive of the DLC) and a limited run collector's edition were announced.
The second entry in Remedy's Max Payne series, Max Payne 2: The Fall of Max Payne, also had two playable protagonists (Max Payne and Mona Sax)."
18,https://www.polygon.com/23691206/best-fantasy-books-sci-fi-2023,"The best sci-fi and fantasy books of 2023
It’s been a stellar year in speculative fiction

By Nicole Clark, Sadie Gennis, and Polygon Staff  Updated Dec 8, 2023, 10:00am EST  34 Comments / 34 New
If you buy something from a Polygon link, Vox Media may earn a commission. See our ethics statement.

It’s been another banner year for science fiction and fantasy books. Many of our favorites once again blur the line between sci-fi and fantasy, but this year was a particular standout for books blurring the line between SFF and other genres. This includes everything from historical fiction — both speculative histories and Westerns — to fable retellings to intergenerational sagas in translation.

Though we seem to have crested the wave of pandemic novels, that sense of dread and discoloration has lingered, written into novels of new forms. There’s a preponderance of post-post-apocalyptic science fiction unpacking lofty ideas like sentience and humanity, often set on different planets or among the stars. It has also been a standout year for supernatural horrors and thrillers, particularly ones that mix queer longing with a dose of body horror. Last but not least, it’s been a great year for kissing books set in fantastical worlds.

RELATED

Looking for more recs? Here are our favorite books of 2022
So jump in and take your pick. Whichever direction you head in, it will be sure to grip you — and make you think. This list is in reverse chronological order, so the newest releases are listed first. We updated this list throughout 2023, sometimes retroactively adding in entries that we missed from earlier in the year. We’ve also included our favorite runners-up.

HONORABLE MENTIONS
Emily Wilde’s Encyclopaedia of Faeries by Heather Fawcett, Victory City by Salman Rushdie, The Crane Husband by Kelly Barnhill, The Mimicking of Known Successes by Malka Older, Monstrilio by Gerardo Sámano Córdova, White Cat, Black Dog by Kelly Link, Divine Rivals by Rebecca Ross, Our Hideous Progeny by C.E. McGill, The Cheat Code (Wisdom Revolution #3) by Misba, The Deep Sky by Yume Kitasei, Silver Nitrate by Silvia Moreno-Garcia, Vampires of El Norte by Isabel Cañas, Prophet by Sin Blaché and Helen Macdonald, Terrace Story by Hilary Leichter, Her Radiant Curse by Elizabeth Lim, Starling House by Alix E. Harrow, System Collapse (The Murderbot Diaries #7) by Martha Wells, Dark Heir (Dark Rise #2) by C.S. Pacat

Cover image for Ed Park’s Same Bed Different Dreams, a split image between what looks like Earth and Mars.
Image: Random House
SAME BED DIFFERENT DREAMS BY ED PARK
Same Bed Different Dreams is a remarkable achievement, and not for the faint of heart. Through three storylines, the book creates a kind of speculative history of Korea, with an emphasis on World War II and Japan’s colonial rule and aftermath (and, crucially, the United States’ involvement). One story thread builds out a hefty alternative history of the Korean Provisional Government’s role and reach. Another story thread focuses on a Black Korean War vet who wrote a sci-fi epic series called 2333, which is later adapted into a video game. And yet another story thread has a more futuristic flavor, focusing on a has-been writer who now works for a tech company called GLOAT. These threads periodically intersect — for example, GLOAT ends up owning the rights to 2333, and turns it into a kind of edutainment.

If it sounds like there’s a lot going on, it’s because there is. And it’s made even denser by the author’s Pynchonian sense of humor. Some of its best moments are utterly weird or feel like the writer was smirking — like a character’s dog who can’t stop “archiving” by burying found manuscript pages, the fact that GLOAT employees truly don’t know what the acronym stands for, or the idea that Marilyn Monroe is a member of the Korean Provisional Government. These absurd bits only make it harder to comb apart what’s real and what’s Ed Park’s “alternate history” in sections with realistic-sounding combinations of fact and fiction.

It’s got the same ambitious patchwork as Jennifer Egan’s The Candy House and Namwali Serpell’s The Old Drift. Critics have compared it to everything from David Mitchell’s Cloud Atlas to David Foster Wallace’s Infinite Jest. There’s also, of course, books within the book. It’s a fever dream of a thing, and one I’d heartily recommend, but perhaps with a notebook in hand or some sticky notes to help track the references. (Or perhaps, as I did, just letting the wave of information roll over you, until you’re left with a vast impression and a desire to reread.) —Nicole Clark

Cover image for Kylie Lee Baker’s The Scarlet Alchemist, featuring a woman in a red outfit with a large crown set against a dark skyline.
Image: Inkyard Press
THE SCARLET ALCHEMIST (THE SCARLET ALCHEMIST #1) BY KYLIE LEE BAKER
Do not go into The Scarlet Alchemist expecting typical YA fare. What Kylie Lee Baker delivers is a story of visceral brutality, interlaced with elements of Chinese history and thoughtful meditations on family, race, and belonging. It’s a book that can turn your stomach as easily as it can break your heart.

Set in an alternate Tang dynasty, the novel follows Zilan, a profoundly talented young alchemist who travels to the capital in hopes of landing a coveted position in the royal service. But being a poor, half Scotian girl means the odds are stacked inordinately high against her in the imperial service exams — and that’s before her skills with the illegal art of resurrection catch the prince’s attention and pull her into a dangerous political game. While the premise seems familiar (underdog competes in trials, falls into star-crossed romance), Baker’s skills with immersive world-building, knotty characters, and genuinely gruesome horror make The Scarlet Alchemist a dazzling and singular tale that left me rushing to read her back catalog. —Sadie Gennis

Cover image of C Pam Zhang’s Land of Milk and Honey, featuring rollicking hills of white, blue, and yellow.
Image: Riverhead
LAND OF MILK AND HONEY BY C PAM ZHANG
After I read How Much of These Hills is Gold in 2020, C Pam Zhang became an instant must-read author in my household. Land of Milk and Honey is entirely unlike her debut — where her debut’s language was sparse and pointed, this book is florid and indulgent — though similar in the extent to which it transported me somewhere entirely new, and more than a little threatening.

In Land of Milk and Honey the climate apocalypse has rendered fresh produce, at scale, a thing of the past — which is to say a provision of the extremely rich. The protagonist, listless and hungry, applies for a job as a private chef for a mysterious family in the Italian Alps (those who live around it call it “​​la terra di latte e miele”). While there, she unravels the family’s true intentions, while making them delicious meals from rare ingredients.

Zhang sensuously describes all pleasures of the tongue, moving from descriptions of lapping of culinary delicacies to the folds of the flesh. Food feels hyperreal, with an emphasis on the texture and taste of every ingredient — and sometimes the cruelty of that ingredient’s procurement. The same can be said of its scenes depicting queer intimacy; that texture and taste take precedent, and the cruelties of human emotion, too. Even after I finished, I was hungry for more. —N. Clark

Cover image for Megan Kamalei Kakimoto’s Every Drop is a Man’s Nightmare, featuring a red and yellow flower against a painted backdrop.
Image: Bloomsbury
EVERY DROP IS A MAN’S NIGHTMARE BY MEGAN KAMALEI KAKIMOTO
This short story collection initially caught my attention with its cover, which depicts a woman springing up from the center of a corpse flower, like a stalk standing against the wind. Each story weaves together Hawaiian mythology and the everyday lives of the Hawaiian and mixed-race Japanese women who live there.

These stories range from fabulism to science fiction, all speculative fiction in their own way. In one story, a woman’s encounter with a wild pig ends up foreshadowing a complicated pregnancy later in her life. In another story, a Brazilian waxing company allows people to pay for hairless skin by giving up personality traits. In another story, the narrator falls for a woman who lives with her family — in one of numerous queer stories in the collection — but has to cope with that woman’s decision to return to “what remains of Kaua’i” and join their protests.

The author’s own words, published in The Guardian, sum it up best: “There is a mythical idealisation of the islands of Hawaii as paradise, peace in the tropics; some even call it a modern utopia. Yet this flattening of Hawaii to a postcard image divests our homeland of its culture and colour, reducing us to a place and history that is easily digestible. But we are not easily digestible, and our stories are not meant to be easy for you.” —N. Clark

Cover image for Shelley Parker-Chan’s He Who Drowned the World, a painted image of ships on a yellow sea, with the moon looming over them.
Image: Tor
HE WHO DROWNED THE WORLD (THE RADIANT EMPEROR #2) BY SHELLEY PARKER-CHAN
An alternate history of the founding of the Ming dynasty, He Who Drowned the World shifts between four tragically ambitious figures willing to pay any price to materialize their destiny, whether that’s revenge on the empire or crowning themselves the ruler of it. They pursue these goals with unshakeable inertia, doing endlessly cruel and sadistic actions with only the occasional doubts as to whether happiness could be possible if they chose a different path.

This is a relentlessly brutal sequel, and there’s a hopelessness that weighs heavy throughout the book. But Parker-Chan’s penetrating ability to bring empathy and nuance into even the darkest corners of humanity sparks an undeniable connection with these characters, whose self-destructive natures would otherwise be too hard to bear witness to. He Who Drowned the World is a dark and difficult read, yet Parker-Chan’s prose is so brilliant, her character work so complex, that I still found myself sad to leave this world behind. —SG

Cover image for M.A. Carricks’s Labyrinth’s Heart, featuring a mask-wearing figure with purple wings sprouting out of the top of the mask.
Image: Orbit
LABYRINTH’S HEART (ROOK & ROSE #3) BY M.A. CARRICK
One of my favorite fantasy series of the past five years, Rook & Rose is an intricately layered trilogy where there are so many secrets, schemes, and conspiracies that at times it’s admittedly difficult to keep track of them all. Because of that, there were a lot of loose ends to tie up in the anticipated conclusion, Labyrinth’s Heart. (Ren alone was juggling four different identities at the novel’s start.) So imagine my surprise when I discovered M.A. Carrick not only managed to leave no question unanswered by the series’ end, but wrapped up even the most complicated storylines in big, bright bows.

There are elements of Labyrinth’s Heart that feel like they were precisely crafted to cater to fans, but here’s the thing: I don’t really care. Carrick created such a lush world populated by lovable characters, an interesting magic system, and a lived-in cultural history that I was just happy to be back in Nadežra after a two-year wait. While things may have been tied up a bit too neatly for my usual tastes, that didn’t stop me from whipping through pages and smiling the whole way through. Sometimes it’s nice to simply soak in a happy ending rather than bathe in the bittersweet. —SG

Cover art for Kiersten White’s Mister Magic, which features a melting television against a pink background.
Image: Del Rey Books
MISTER MAGIC BY KIERSTEN WHITE
The latest fantasy-with-an-irresistible-pop-premise from the author of Hide, Mister Magic revolves around a children’s TV show no viewer can forget … or prove it ever existed in the first place. There are no official records of it, no YouTube videos or merchandise or passed-around VHS tapes, and any discussion of it on the internet rapidly disappears. But the people who remember seeing it are convinced the special effects were remarkably vivid and realistic. They agree the central concept is unnerving: a creepy magician-figure leading a group of children in imagination-games aimed at teaching some decidedly non-standard lessons about embracing conformity and meekness. And they’re all sure that something horrible happened while they were watching, though they can’t agree on what.

A reunion between five of the former child cast members, taking place 30 years after the show ended, slowly unravels its mysteries, which are even weirder than the description above suggests. Mister Magic is a startling dark fantasy with a lot of foreboding, foreshadowing, and eerie twists. At heart, though, it’s also an incisive story about the kinds of people who revel in control over other people’s lives, and about what an act of rebellion imagination can be. —Tasha Robinson

Cover image for Rebekah Bergman’s The Museum of Human History, featuring a painted image of a naked figure with a red cloud over the top of their head.
Image: Tin House
THE MUSEUM OF HUMAN HISTORY BY REBEKAH BERGMAN
A poetic reflection on memory, loss, and connection, The Museum of Human History is a stunning debut reminiscent of the work of Emily St. John Mandel. Slipping backward and forward in time, this introspective mosaic weaves between an identical twin whose sister fell asleep at age 8 and has never aged in the 25 years since, a museum director who questions his place within the family legacy, a widower who lost his most cherished memories as a result of an anti-aging treatment, and others equally struggling with the passage of time. There is a lyrical detachment in Bergman’s prose that leaves you feeling like you’re watching events unfold through a pane of thick glass, never fully able to connect with the characters, yet you remain helplessly transfixed by the haunting cycle they’re caught in. It’s an incredibly melancholy book, but the kind of aching sadness you’re happy to sink into. —SG

Cover image for Sara Hashem’s The Jasad Heir, featuring what looks like statues of a snake,, a bull, and a griffin.
Image: Orbit Books
THE JASAD HEIR (THE SCORCHED THRONE #1) BY SARA HASHEM
“Arin of Nizahl was maddeningly elegant. I wanted to cut him open and compare our bones to understand why his gave him grace and mine gave me back pain.” This was the line that absolutely sold me on The Jasad Heir, an irresistible enemies-to-lovers fantasy that reminded me why I’ll never quit this genre.

Headstrong Sylvia is the presumed dead heir of Jasad, a kingdom that was destroyed by the neighboring Nizahl and saw its citizens’ innate magic outlawed. Sylvia managed to carve out a relatively normal life for herself as a chemist’s apprentice, but everything falls apart after she accidentally reveals her magic to the heir of Nizahl. Using her life as leverage, the calculating Arin strikes a deal with Sylvia to help him capture a group of Jasadi rebels and act as his champion in a series of deadly trials. It’s a familiar setup, but one impeccably done by Hashem, who delivers sharp political intrigue, sparkling banter, and touching friendships on top of Sylvia and Arin’s simmering romance. —SG

Cover image for Kritika H. Rao’s The Surviving Sky, featuring a floating island overgrowing with buildings and plant life, above a stormy planet.
Image: Titan Books
THE SURVIVING SKY (THE RAGES TRILOGY #1) BY KRITIKA H. RAO
After I finished The Surviving Sky, I wouldn’t shut up about it and tried (not always successfully) to get everyone I know to read it. So let me try once more, and maybe with less yelling this time:

With the planet’s surface made unlivable by catastrophic storms, the remains of humanity survive on floating cities constructed of and powered by plants that only a select group of people, known as architects, can control. An archeologist without the ability to traject plants, Ahilya has dedicated her life to finding a way to unshackle humanity’s survival from the architects’ powers and return to the surface. It’s not hard to see why this mission causes friction in her marriage to Iravan, one of the most powerful architects in their city, and one with an arrogance to match his revered status. Though estranged, Ahilya and Iravan come together to help clear his name after he’s accused of pushing his powers dangerously far, an accusation, which if proved true, carries dire consequences for the architect.

But the deeper they look into trajection and its risks, the more Ahilya and Iravan realize they don’t actually know much about where their people – and their powers – came from. And as the floating cities begin to sink toward the earthrages below, the race to save their civilization may also be the end of society as it stands, as Ahilya and Iravan uncover long-buried truths that previous generations worked hard to keep hidden.

So did I do it? Did I convince you to read this Hindu philosophy-inspired debut with some of the most inventive world-building and one of the most complex romances I’ve read in years? Please say yes. You’ll be doing us both a favor. —SG

Cover image for Alexander Darwin’s The Combat Codes, which features a metallic dragon against a black background.
Image: Orbit
THE COMBAT CODES AND GRIEVAR’S BLOOD (THE COMBAT CODES SAGA #1-2) BY ALEXANDER DARWIN
In the world of The Combat Codes, war no longer exists as it used to. Neither does justice — both concepts have been replaced by proxies who fight on behalf of nations or individuals, solving disputes with their fists.

Alexander Darwin’s debut novel effectively builds a world around this core concept, bringing it to life with compelling characters and locations (including a classic “magical school for gifted youngsters” situation). The Combat Codes follows Cego, a young abandoned boy skilled at fighting, and Murray, a washed-up former fighter now tasked with scouting the next generation of combatants, whose discovery of Cego changes his entire world.

Darwin is also a Brazilian jiu-jitsu practitioner and teacher, and uses that experience in the books’ excellent fight sequences. His evocative and visceral descriptions not only deliver excitement and suspense in this underdog story; they build your understanding of the characters through how they fight. The Combat Codes and its equally fun sequel, Grievar’s Blood, which adds new exciting characters and points-of-view, are the first two parts of a planned trilogy, and I can’t wait for the conclusion next year. —Pete Volk

Cover image for Katie Williams’ My Murder, showing a woman’s face peering outside of red vertical lines.
Image: Riverhead Books
MY MURDER BY KATIE WILLIAMS
Fans of Sarah Gailey’s The Echo Wife won’t want to miss My Murder, which shares some key elements and themes with Gailey’s novel while also taking them in a unique direction. In a near-future with only a few light sci-fi elements, Lou has been resurrected along with a handful of other women murdered by a single serial killer. The politics of resurrection in her world are complicated, and few people qualify. That leaves her and her fellow victims (whose therapy circle recalls Grady Hendrix’s The Final Girl Support Group) a bit at sea as they try to come to terms with their deaths, which none of them can recall, and their new lives as celebrities for all the wrong reasons.

Like The Echo Wife, My Murder ends up thoughtfully exploring issues around women subjected to violent men — not just the personal and internal response, but the society that shapes that violence, and responds to it in ways that raise endless questions. The victims all respond to their deaths differently, questioning their culpability and the possible failures that might have made them targets, and navigating their families’ unpredictable responses to their revival. There’s one big mystery at the heart of My Murder, and a whole lot of abrupt and compelling surprises. But at the core, it’s a sci-fi twist on the survivor story, letting some very different people explore what it means to be victimized, and how to reclaim the lives that have been abruptly handed back to them. —TR

Cover image for Ann Leckie’s Translation State, a minimalist drawing with red, orange, and green, a silhouette of a person, and circular lines.
Image: Orbit
TRANSLATION STATE BY ANN LECKIE
Set in the same universe as Leckie’s Imperial Radch trilogy, Translation State follows Enae, who leaves hir long-standing isolation for what was supposed to be an interstellar goose chase. After hir demanding grandmaman dies, Enae is given a diplomat title and assigned to investigate a missing Presgr translator no one expects to be found (but that the government still wants the goodwill for pretending to look for). Only, Enae doesn’t just pretend to look; sie discovers sie has quite the knack for investigating the 200-year-old cold case.

This is how hir path crosses that of Reet, an adopted maintenance worker whose mysterious origins and unsettling impulses might be explained by being the child of the fugitive translator, if you ask Enae, or the last descendant of a lost sovereign line, if you ask one particularly zealous diaspora social group. Rounding out the POV characters is Qven, a young Presgr terrified of their species’ ritual of merging with an elder, a rite of passage which will see Qven’s selfhood entirely dissolved. Enae, Reet, and Qven’s explorations of their own identities wind up having interplanetary consequences, but it’s the way Leckie gives weight to the small moments, both personal and shared, that make this book sing.

Though I’m sure there are layers that only those familiar with the Imperial Radch trilogy will notice and appreciate, the standalone Translation State and its rich exploration of self-identification and personhood serve as a fantastic introduction to Leckie’s world. So don’t hesitate to jump into Translation State if you’re – like me – new to Radch and simply drawn to a thrilling mystery where the most intimate emotions can fuel a universal upheaval. —SG

Cover image for Rita Chang-Eppig’s Deep as the Sky, Red as the Sea, with facial features set against a crashing wave.
Image: Bloomsbury Publishing
DEEP AS THE SKY, RED AS THE SEA BY RITA CHANG-EPPIG
I still remember standing in my local bookstore, struck by the cover of this book, and reading the summary. It had me at “Chinese pirate queen.”

In Deep as the Sky, Red as the Sea, Chang-Eppig writes a historical fantasy about Shek Yeung, a fearsome Chinese pirate who must navigate her fleet after the death of her powerful husband. She marries her late husband’s second-in-command, with the promise of bearing an heir, in order to retain power over the fleet — and stay a major player as the Chinese Emperor seeks to rid the waters of piracy.

The book isn’t paced like a thriller, so don’t make the mistake of assuming so when you start it. It’s equal parts historical exposition, strategy, and warfare — and it especially excels in its characterization of a complicated woman forced to make difficult decisions and sacrifices in order to protect her power. Fantasy can put its villains and heroes on pedestals, but Deep as the Sky, Red as the Sea never errs in its very human portrayal of Shek Yeung, and how deftly she must play this game of political chess for survival. I was riveted. —N. Clark

Cover art for Emma Törzs’ Ink Blood Sister Scribe, featuring a dripping pen growing out of the bottom of a tree against a purple background.
Image: William Morrow
INK BLOOD SISTER SCRIBE BY EMMA TÖRZS
There’s nothing cozier than a magical book about the magic of books — though this tale bends a little darker, and tells a story about witchcraft and complicated family dynamics. In Ink Blood Sister Scribe, two estranged sisters come together to solve the mystery of their family, and prevent further tragedies. In this world, blood can be concocted into ink — wielded by scribes for the creation of books with arcane powers — though the creation of such books drains a scribe’s health. When others read these books, they create magic; willing flowers to bloom, or making magical carpets that can fly in the air.

Ink Blood Sister Scribe is the perfect sister thriller to read in one sitting. It doesn’t reinvent the wheel, but it doesn’t need to — it simply delivers on a wonderfully entertaining premise. —N. Clark

Cover art for Martha Wells’ Witch King, featuring a person running across the cover while wearing a cloak and dress fitting for a fantasy setting.
Image: Tor
WITCH KING BY MARTHA WELLS
In an era where a lot of fantasy fans value quick or cozy reads, Martha Wells’ Witch King feels like a gauntlet thrown at readers’ feet. It’s a complex, meaty fantasy that opens well into what a more linear book would consider the third act, as Kai, the witch king of the title, is exhumed from a watery grave and starts exploring who betrayed him and trapped him there. Readers have to learn everything about Kai’s world as his story unfolds in multiple intertwined timelines. That includes figuring out what a “witch king” is, unwrapping the layers of what Kai actually is and why it matters. It also means being introduced to a wide variety of allies and enemies while alternately flashing back to how he met them, and slowly coming to understand the dense political machinations that shaped all their lives in the past and present.

As with Wells’ Murderbot books and her Books of the Raksura series in particular, part of the draw here is a powerful, skilled protagonist whose biggest struggles are often internal. Kai has a lot of intense emotional responses to the world, but lacks the tools to understand what to do with those feelings, or who to trust with them. Wells packs Witch King with a lot of audacious, expansive world-building for a standalone novel (albeit one that could easily invite sequels or prequels), but what makes Witch King an enjoyable read instead of a frustrating one is the way all the book’s complications and surprises are filtered through Kai’s vivid inner life, giving readers something to hold onto as they’re untangling the puzzlebox aspects of this cleverly structured novel. —TR

Cover image for Justin Lee Anderson’s The Lost War, featuring five figures walking through white grass after emerging from a dark green forest. Three of the figures wear green cloaks, while two wear white.
Image: Orbit
THE LOST WAR (THE EIDYN SAGA #1) BY JUSTIN LEE ANDERSON
Originally self-published in 2019, The Lost War is a traditional fantasy adventure that follows a rag-tag group of strangers on a mission across a war-torn country, fighting monsters and uncovering mysteries along the way. Despite the strong buzz leading up to the novel’s expanded publication by Orbit this year, I found myself hesitant to pick it up since it seemed so similar to many books I’ve read before. But while it’s true The Lost War doesn’t rewrite the genre – it’s filled with well-worn tropes and classic adventurer archetypes – Anderson’s skillful execution left me completely charmed. There is a real Dungeons and Dragons feel to The Lost War, and though the characters are familiar (the honorable paladin, the hard-drinking haunted soldier), Anderson does a fantastic job developing unique dynamics between the party members that vault the book beyond the sum of its parts. And it all builds up to a massive twist at the end that completely upends your understanding of what you’ve read and any previous expectations for where the second book will go. The delightfully unexpected ending once again has the fantasy community buzzing ahead of Anderson’s next release – only this time I’m right there with them. —SG

Cover image for Moniquill Blackgoose’s To Shape a Dragon’s Breath, a red cover with flowers and a dragon’s head/mask on it.
Image: Del Rey
TO SHAPE A DRAGON’S BREATH (NAMPESHIWEISIT #1) BY MONIQUILL BLACKGOOSE
To Shape a Dragon’s Breath’s description hooked me immediately: It’s got dragons, a magic school, and a strong teenage main character. Moniquill Blackgoose has taken several different fantasy tropes and created a fantasy novel that’s unlike anything I’ve read; To Shape a Dragon’s Breath is set in an evolving steampunk world as Anglish settlers push the Indigenous Masquapaug people out of their land and onto a remote island. Dragons had long been important cultural touchstones to the Indigenous people, but colonization has, too, pushed them away. To Shape a Dragon’s Breath begins as 15-year-old Anequs finds a dragon egg — the first to be spotted in the area in generations. Anequs is named a Nampeshiweisit, or a dragon rider, as the community helps raise and hatch the dragon’s egg.

The colonizing nation quickly finds out and forces Anequs and her dragon into the Anglish dragon school; if she resists, the dragon will be eliminated. To Shape a Dragon’s Breath is about the growing relationship between her and her dragon Kasaqua, but also about her resistance to the Anglish traditions relating to dragons. The Anglish treat dragons as something to be conquered — they use them as tools and weapons, whereas the Indigenous people have historically partnered with dragons for a relationship built on both tradition and respect.

That partnership means Anequs now has the power to take on colonialism and racism in a new way. Where To Shape a Dragon’s Breath really shines is in that growing relationship between Anequs and Kasaqua; the partnership — and power for both that comes with it — is in stark contrast to the Anglish ways. Bonus: To Shape a Dragon’s Breath has well-written, complex bisexual and neurodivergent characters, too. —Nicole Carpenter

Cover image for Melvin Burgess’s Loki, a black cover with a black snake wrapped around gold letters with the title.
Image: Pegasus
LOKI BY MELVIN BURGESS
Melvin Burgess has spent a career writing confrontationally frank children’s literature like Junk, his 1990s book about heroin-addicted teenagers. His first adult book, published at age 69, is a blistering, transgressive, and hugely entertaining reframing of the Norse myths, as told by the most unreliable narrator imaginable: Loki himself, the god of tricks, inventions, and political intrigue. But what does reliable mean, anyway, in the mutable world of myth? Burgess paints Loki (or rather, has him paint himself, as he addresses the reader directly in first person) as an eternal outsider, shaking his head sagely at the follies of the gods, and challenging their might-is-right order. But of course, that’s what he’d want us to think. Burgess’ best trick, though, is the way he rolls together the deeply weird, muddy, shape-shifting mystery of the tales themselves with a bracing modernity in characterization and language, somehow without one clashing with the other. In doing so he brings the wild, ancient power of the Norse myths to vivid life. —Oli Welsh

Cover image for Nana Kwame Adjei-Brenyah’s Chain-Gang All-Stars, featuring a scythe chopping through the words with a bright yellow background.
Image: Pantheon Books
CHAIN-GANG ALL-STARS BY NANA KWAME ADJEI-BRENYAH
In Chain-Gang All-Stars, prison inmates fight to the death in a series of gladiatorial matches — and all of it is televised to a hungry audience. It’s a program called CAPE, the Criminal Action Penal Entertainment, which promises freedom to inmates who survive three years of its brutality. The average life expectancy for anyone who enters is three months. Within this system, Loretta Thurwar and Hammara Stacker (called Hurricane Staxxx by her fans) emerge as two frontrunners.

This National Book Award finalist takes on the viciousness of the carceral system, with more than a bit of The Hunger Games’ DNA sprinkled in. “Hard action” fans salivate over matches, a self-obsessed announcer resents the fact that contestants don’t offer more banter, and the women who top the leaderboards become sex symbols in pop culture. But where other fight-to-the-death dystopias — among the greats, like Battle Royale or Lord of the Flies — spin a more fantastical yarn, Chain-Gang All-Stars is aimed right at the heart of the all-too-real cruelties of our existing for-profit penal system.

Early in the book, Thurwar kills a 16-year-old boy in a gladiator match. Fans in the stands lament not the death of the boy, but the idea that the fight wasn’t entertaining because it wasn’t a fair matchup. In a footnote, Adjei-Brenyah writes of George Stinney Jr., a 14-year-old Black boy who was convicted for murder and executed in 1944. Chain-Gang All-Stars also illustrates the ways in which imprisonment is simply “slavery by another name,” showing all manner of menial labor the contestants are forced to perform. In 2022, the ACLU reported that inmates made between 13 and 52 cents an hour, and sometimes nothing.

Critics have said this book is an “act of protest” but that it doesn’t “straightforwardly preach,” or that it’s more entertaining than “an attempt to convince its readers of the case for prison abolition has any right to be.” I understand why you’d want to say this book is “fun” despite an abolitionist message, especially in a political climate where radical writing is often appreciated only as a teaching tool. But I think that kind of delineation undercuts Adjei-Brenyah’s talent as a novelist, and his skill in heightening the real as a form of storytelling. I’d call it thrilling, over calling it fun. And the fact that it is thrilling is inextricable from its openly abolitionist values — it’s the very knowledge of real life that Adjei-Brenyah wields to craft suspense. —N. Clark

Cover image for Rebecca Yarros’ Fourth Wing, which features a circle image behind black text, with clouds and some flying creatures.
Image: Entangled
FOURTH WING BY REBECCA YARROS
This action-packed, fantasy romance feels like a grown up version of all of my favorite young adult books. It’s got all of the fun nostalgic tropes — a magical school, deadly trials, dragon riding, and a love triangle between the main character, a golden retriever love interest, and a misunderstood emo rival — but it’s also extremely horny, as all fun fantasy romance must be.

Violet Sorrengail is thrown into a series of trials in order to prove whether she can be a dragon rider. There are a few problems with this: she trained as a scribe, never thought she’d be thrust into danger, and she also must deal with Xaden Riorson, her sworn enemy (wink). She also manages a joint condition, which leaves her in chronic pain — a fact the book handles gracefully. In one of my favorite climactic moments of the book, Violet is given a mobility device to help her with her trials; those close to her remind her that it doesn’t diminish her power, but is a tool like any other, and one that allows her to flourish. I’m thrilled to read the next installment, when it comes out in November. —N. Clark

Cover art for Adrian Tchaikovsky’s Lords of Uncreation, which shows a spaceship approaching what looks like a space battle next to a planet, with exploding orbs in space and a lot of spaceships in the distance.
Image: Orbit
LORDS OF UNCREATION (THE FINAL ARCHITECTURE #3) BY ADRIAN TCHAIKOVSKY
Reading the Final Architecture series, I had to accept long ago that I would never fully grasp the nuances of some of its central concepts, even if I understood them on an instinctual level.

This acceptance set me up well for Lords of Uncreation, which revolves around concepts that even the characters find impossible to understand, and whose minds may literally break if they try to. Like looking directly into the sun, confronting the blurred space between the real and unreal (as well as the eldritch terrors that lurk within) poses a grave threat to those doing so head-on – at least to anyone other than weary intermediary Idris Tellemier, whose risk is merely reduced rather than eliminated. But the characters Adrian Tchaikovsky has populated this world with are so grounded, so emotionally rich, and so vibrant that the details of the brain-bending threats lurking within unspace become secondary to their impact on the lives of and relationships between the Vulture God’s crew.

This is not to say that Tchaikovsky does not deliver an incredibly satisfying conclusion to the mysteries of unspace (he does!). But what I’ll remember most is how he crafted the perfect emotional resolution to this intellectually intricate tale that left me in tears and has stayed with me since. —SG

Lead art for Justin Cronin’s The Ferryman, which pictures a cloudy sky over the horizon, as a single sail boat sits on the water.
Image: Ballantine Books
THE FERRYMAN BY JUSTIN CRONIN
Proctor Bennett is a ferryman, whose duty is to guide unhappy citizens from the utopian Propersa to the Nursery, where they retire their old selves before returning in younger bodies with no memories of their former lives. But when Proctor is assigned to retire his own father, the troubling encounter sends him careening off the path of conformity. He begins questioning prescribed truths and confronting the darker side of Prospera, which runs off the work of a disenfranchised support staff whose discontent is building towards a revolution that pulls Proctor into its orbit.

Though this premise may feel familiar, The Ferryman is anything but. This tightly-wound, atmospheric thriller weaves together layers of knotted mystery with Proctor’s haunting POV as he grapples with his relationship to grief, happiness, family, and identity. It’s a sharply complex mystery with a cinematic quality to it. Throughout reading, I couldn’t help but fan-cast who would star in a Christopher Nolan adaptation of it. But even if you aren’t an Inception fan, it’ll be easy to become immersed in The Ferryman’s distinct dystopian world. —SG

Cover image for Emily Tesh’s Some Desperate Glory, featuring a woman walking confidently in front of a wall opening to reveal a planetary body.
Image: Tor
SOME DESPERATE GLORY BY EMILY TESH
Around September, as the pile of unpainted plastic miniatures here in my home office began to get particularly deep, I suddenly ran out of Warhammer 40,000 Black Library audiobooks by Games Workshop that I was the least bit interested in listening to. That’s when I stumbled upon Some Desperate Glory by Emily Tesh. Billed as a space opera told from the perspective of one of humanity’s last genetically engineered super soldiers, I fell for the premise hook, line, and sinker. Then, about 50 pages in, I let it sucker-punch me right in the gut.

With Some Desperate Glory, Tesh has envisioned a deeply affecting reality where the children of a subjugated, war-torn race slowly come to realize that they have been lied to — manipulated into an amoral war of vengeance without end. Tesh shows incredible restraint throughout, reeling out a thick and binding thread of painful realizations from deep within the main character, Kyr. After grappling with my personal love for the grim darkness of the far future for quite a few years now, this book helped me come to terms with how much I despise those tropes even as I find myself drawn toward them time and time again.

Some Desperate Glory is, in my opinion, required reading for anyone who has ever painted a Space Marine in earnest – and a new fixture in the canon of queer science fiction. —Charlie Hall

Cover image for Jade Song’s Chlorine, featuring a large fin in the ocean waves.
Image: William Morrow & Company
CHLORINE BY JADE SONG
I think I have been waiting my whole life for this book — for someone to write adolescence like the body horror it is, with all of the cultural specificity of being a Chinese American girl, simply bursting at the seams with sapphic longing. Chlorine stars Ren Yu, a swimmer who believes that she is a mermaid. But she is tethered to land by her human ambition: By the parents who constantly push her to achieve, and by a swim coach who pays inappropriate attention to her — pushing her to swim faster times, while also making her feel uncomfortable in her skin.

Ren’s steadfast belief in being a mermaid feels both like a flight of fancy, and increasingly like a means of dissociating from the horrors of everyday life. Being a young girl is hard enough without having to contend with the high expectations of parents, the predation of adult men, and the casual racism of peers. Jade Song’s writing is gruesomely lyrical, contrasting the sublime with the deeply disturbing. There were several points where this book almost made me throw up, and I mean that as a high compliment. —N. Clark

A Black woman stands alone in a field, her face covered by shadow, in the cover art for Lone Women by Victor LaValle.
Image: One World
LONE WOMEN BY VICTOR LAVALLE
Adelaide Henry is traveling to Montana, where she plans on making a new life as a homesteader — leaving the flames of her California home, and the bodies of her parents, behind. But she has a heavy weight to carry. She lugs an enormous steam trunk wherever she goes; whenever the trunk opens, people around her die. In 1915, Montana is in the middle of a homestead boom, and though Adelaide aims to make a new start, not everyone is welcoming to a Black woman traveling alone.

Victor LaValle mixes horror and fantasy in this expertly paced tale. It’s satisfyingly bloody, while making incisive commentary on the price of being an outsider. The Western genre has long fixated on the white imagination, perhaps occasionally making space for the early struggle of the suffragettes. But LaValle’s vision of history emphasizes just how powerful white women are in upholding the interests of their white husbands, and how far these women will go to protect the societal structures that put them in proximity to power. Lone Women also examines how shame, and the family unit, ultimately uphold these unspoken rules — ostracizing those who might otherwise find community support.

This book was so good that I am now reading my way through every interview LaValle has given on the Lone Women press circuit, too, and then reading every book he references. What a gift! —N. Clark

Cover image of Nathan Ballingrud’s The Strange, depicting a diner on Mars.
Image: Gallery/Saga Press
THE STRANGE BY NATHAN BALLINGRUD
Nathan Ballingrud’s debut novel was added to my TBR pile after seeing it marketed as a blend of Ray Bradbury’s The Martian Chronicles and Charles Portis’ True Grit. I’m always dubious about marketing comparisons, but was thrilled when The Strange delivered on this high promise.

In an alternate history where humanity colonized Mars in the early 1900s, the red planet has lost all communication with Earth, leaving the fate of 14-year-old Annabelle Crisp’s mother unknown. When a thief steals Annabelle’s sole voice recording of her mom, she and her beloved Kitchen Engine, Watson, set off into the desert to retrieve what’s hers and see justice served. The longer Annabelle’s adventure goes on, the more she loses perspective and drifts away from righteousness in dogged pursuit of her own selfish desires. Struggling to comprehend that the world can’t be divided into binaries like right or wrong and black or white, Annabelle converts her fear into anger, lashing out and harming those around her, including those providing aid.

Annabelle can be vengeful and cruel, and though I often disagreed with her choices, Ballingrud makes it impossible not to understand and empathize with her. Annabelle Crisp isn’t a hero and she isn’t a villain, but she is an outstanding protagonist in a wonderfully original sci-fi tale. —SG

Cover image for Moses Ose Utomi’s The Lies of the Ajungo, featuring a figure walking upside down on mounds of sand as a castle lurks in front.
Image: Tor
THE LIES OF THE AJUNGO (THE FOREVER DESERT #1) BY MOSES OSE UTOMI
In his debut novella, Moses Ose Utomi wields his precise prose to tell a dark, visceral fable about a young boy from the City of Lies, a metropolis reliant on the brutal Ajungo Empire for their supply of water. But the cost of this trade is high: At 13, every child of the City of Lies has their tongue cut out and sent to the Ajungo.

Even with this gruesome tithe, the Ajungo send barely enough water for the population to survive, and far from what they’d need to do so comfortably, let alone thrive. Shortly before his thirteenth birthday, the brave Tutu sets out on a dangerous journey to save his mother and the city by finding their own water supply. As Tutu explores the outside world for the first time, his perception of truth and history is challenged, and he comes to understand how the decisions and deceptions of those in power rewrite the past and shape the future to uphold those with privilege and foster compliance in those who don’t. —SG

Cover image for Edward Ashton’s Antimatter Blues, A Mickey7 Novel. It features an astronaut from behind on a rocky planet, looking out at another planet in the distance.
Image: St. Martin’s Press
ANTIMATTER BLUES BY EDWARD ASHTON
Edward Ashton’s sequel to Mickey 7, the 2022 novel Parasite director Bong Joon-ho is adapting as a movie starring Robert Pattinson, takes up two years after the first book left off, with “Expendable”-status planetary colonist Mickey still on the outs with the leadership of his struggling colony after a gutsy bluff he made to ensure his own survival. The sixth clone of the original Mickey, who accepted life as a disposable body for suicide missions in exchange for a ticket to space, Mickey 7 has walked off that job. His ongoing draw on the colony’s resources is only tolerated because he’s exaggerated his diplomatic connections with the local aliens. Then the base commander orders him to do something impossible, or the entire colony will die.

Antimatter Blues is knottier than the first book in the series, with more to take in about the ethics of survival and humanity’s predisposition toward xenophobia and selfish, self-serving behavior. It sure isn’t a pleasant book to read: A lot of Mickey’s co-colonists are bigots, most of them are indifferent to anyone else’s suffering, and at times, the book reads as though Earth deliberately sent all the worst people into space, the better to be free of them. Even Mickey himself is, at absolute minimum, generally more focused on his own safety and comfort than on the horrific results of some of his choices. But as soon as he’s placed in what seems like an unsurvivable situation, that dynamic leads to high drama, and Antimatter Blues becomes a breathless book rocketing to a surprising conclusion. Prepare to feel sorry for various alien races who have to deal with icky humanity. —TR

Cover image for Samantha Shannon’s A Day of Fallen Night, a colorful image with a a dragon swirling around it
Image: Bloomsbury
A DAY OF FALLEN NIGHT (THE ROOTS OF CHAOS #0) BY SAMANTHA SHANNON
Samantha Shannon’s A Day of Fallen Night is her second book in the Roots of Chaos series, but a prequel to The Priory of the Orange Tree. Like The Priory of the Orange Tree, A Day of Fallen Night is an epic, far-flung fantasy novel set in a world of magic and dragons. A Day of Fallen Night is set hundreds of years before The Priory of the Orange Tree, and follows several of the original book’s ancestors as the world fears the return of an evil wyrm, the Nameless One. You don’t have to have read The Priory of the Orange Tree to enjoy A Day of Fallen Night; in fact, it’s likely a good place to start if you’ve been interested in reading Shannon’s original, massive fantasy book. Of course, this is a slow-burn 800-page book that precedes another 800-page book, so it’s definitely a time investment regardless of the path.

Though A Day of Fallen Night deals with a world-shaping, cataclysmic threat and widespread political machinations, the book is rooted within four characters from around the book’s world: Sabran, Glorian, Dumai, and Tunuva Melim. The stories of these characters intertwine as their regional beliefs tied to wyrms and dragons conflict, muddying up the necessary collaboration in fighting off the looming threat. In between all that catastrophe, Shannon gives the women of the book rich stories of personal relationships, sacrifice, and conflicting feelings. Motherhood and bodily autonomy are also strong themes throughout the book; both Sabran and Glorian (mother and daughter) have their bodily autonomy tied to the fate of their region.

It’s not easy to describe A Day of Fallen Night in a short blurb — it does so many things and goes so many places. Shannon’s created a series that has the scale of The Lord of the Rings, wrapped up in a world of queer, female power. The Roots of Chaos, as a whole, is one of my favorite fantasy series ever. —N. Carpenter

Cover image for Mariana Enriquez’s Our Share of Night, featuring a red hand with long yellow fingernails.
Image: Hogarth Press
OUR SHARE OF NIGHT BY MARIANA ENRÍQUEZ
This literary tome defies categorization, so I’ll paint a scene instead: A father (Juan) whisks his son (Gaspar) away on a trip. Juan is mercurial; at turns terrifying and violent, at turns bewilderingly tender, nearly infinite in love. But he is a closed book. And if you think you’ve seen his hands elongate, spindly fingers yielding to piercing claws — well no, you didn’t.

Slow, dreadful, and razor-sharp, Our Share of Night charts a family’s desperate attempt at escaping the clutches of a death cult in Argentina. Its members seek the secrets of immortality, and many are willing to pay any price to obtain it. Set in 1981, the novel’s supernatural terrors intertwine with those of the Dirty War, the authoritarian violence offering cover for the cult to operate uninhibited.

I will read anything Mariana Enríquez writes next, it’s an absolute joy to experience her work. —N. Clark

Cover image for Annalee Newitz’s The Terraformers, which features a futuristic cityscape with lush greenery.
Image: Tor Books
THE TERRAFORMERS BY ANNALEE NEWITZ
The Terraformers concerns itself with one question: As a species evolves, what behaviors stick around? Set more than 50,000 years in the future (yes, you read that number right), The Terraformers details the process of terraforming and developing a privatized planet into a tourism joint for the super rich. Technology has advanced in barely fathomable ways, allowing, for instance, the extension of human-level intelligence to animals and robots. But some aspects of society might seem familiar: Real estate developers who jack up rent with no warning? Local governments that abhor public transit? That every video call still has one person who can’t get the camera to work?

Equal parts prescient and absurd, The Terraformers splits its story over three novellas, each 700 years apart. One of those stars a sentient train who teams up with an investigative journalist ... who also happens to be a cat ... who’s also trying to prove this ostensibly privatized planet is in fact public land. Written by a leading science journalist of our era (author Annalee Newitz is the founder of io9 and has written for basically every major science publication under our sun), The Terraformers is unexpectedly one of the most accurate representations of the journalistic process I’ve ever read. And it all culminates in an undeniable stance: That capitalistic power must still be held in check by the truth. Even 50,000 years in the future, a free press is among society’s most essential facets. The more things change... —Ari Notis

The cover image of Adrian Tchaikovsky’s Children of Memory, which depicts a spaceship approaching a large orange planet.
Image: Orbit
CHILDREN OF MEMORY (CHILDREN OF TIME #3) BY ADRIAN TCHAIKOVSKY
Adrian Tchaikovsky’s highly anticipated third book in the Children of Time trilogy once again delves into some of science fiction’s headiest topics. There are parallels to earlier installments — Tchaikovsky once again uses another hyper-intelligent animal species to examine the idea of what being “alive” really means. But he also takes readers somewhere completely and utterly new, outside the scope of the previous titles, and incredibly difficult to describe without spoiling the premise entirely.

All I can say is hold on for the ride. This is an author who dives head first into Asimov-esque ideas, and who is willing to take the plot in fanciful directions. I still can’t believe that I have recommended a book about sentient spider colonies to so many friends, but here we are. This finale is worth your time. —N. Clark"
19,https://arxiv.org/pdf/2404.10981,"A Survey on Retrieval-Augmented Text Generation for Large Language
Models
Yizheng Huang
York University
hyz@yorku.ca
Jimmy X. Huang
York University
jhuang@yorku.ca
Abstract
Retrieval-Augmented Generation (RAG)
merges retrieval methods with deep learning
advancements to address the static limitations
of large language models (LLMs) by enabling
the dynamic integration of up-to-date external
information. This methodology, focusing
primarily on the text domain, provides a
cost-effective solution to the generation of
plausible but incorrect responses by LLMs,
thereby enhancing the accuracy and reliability
of their outputs through the use of real-world
data. As RAG grows in complexity and
incorporates multiple concepts that can
influence its performance, this paper organizes
the RAG paradigm into four categories:
pre-retrieval, retrieval, post-retrieval, and
generation, offering a detailed perspective
from the retrieval viewpoint. It outlines
RAG’s evolution and discusses the field’s
progression through the analysis of significant
studies. Additionally, the paper introduces
evaluation methods for RAG, addressing
the challenges faced and proposing future
research directions. By offering an organized
framework and categorization, the study
aims to consolidate existing research on
RAG, clarify its technological underpinnings,
and highlight its potential to broaden the
adaptability and applications of LLMs.
1 Introduction
The advent of ChatGPT has significantly impacted
both academia and industry due to its interactive
capabilities and widespread application, establishing itself as a leading artificial intelligence tool
(Laskar et al., 2023; Jahan et al., 2023; Huang
and Huang, 2024). At the core of ChatGPT is the
large language model (LLM) GPT-4, as detailed by
(OpenAI et al., 2023), which has seen numerous
enhancements to its predecessors, showcasing exceptional abilities in a variety of Natural Language
Processing (NLP) tasks (Laskar et al., 2020). Despite these advancements, the adoption of LLMs
Figure 1: An example of RAG benefits ChatGPT resolves questions that cannot be answered beyond the
scope of the training data and generates correct results.
has highlighted several critical issues primarily due
to their reliance on extensive datasets. This reliance
restricts their ability to incorporate new information post-training, leading to three primary challenges. First, the focus on broad and general data
to maximize accessibility and applicability results
in subpar performance in specialized areas. Second,
the rapid creation of online data, combined with the
significant resources required for data annotation
and model training, hinders LLMs’ ability to stay
updated. Third, LLMs are susceptible to generating convincing yet inaccurate responses, known as
“hallucinations”, which can mislead users.
Addressing these challenges is crucial for LLMs
to be effectively utilized across various domains. A
promising solution is the integration of RetrievalAugmented Generation (RAG) technology, which
supplements models by fetching external data in
response to queries, thus ensuring more accurate
and current outputs. Figure 1 illustrates how RAG
can enable ChatGPT to provide precise answers
beyond its initial training data.
Since its introduction by Lewis et al. (Lewis
et al., 2020b) in 2020, RAG technology has undergone significant advancements, particularly influenced by ChatGPT’s success. However, there is a
noticeable gap in the literature regarding a thorough
analysis of RAG’s mechanisms and the progress
made by subsequent studies. Furthermore, the field
is characterized by diverse research focuses and the
use of ambiguous terminology for similar methods,
leading to confusion. This paper aims to clarify these aspects by offering a structured overview of
RAG, categorizing various methods, and delivering an in-depth understanding of this research area.
This survey will primarily focus on textual applications of RAG, reflecting the current emphasis of
research efforts in this area.
RAG combines retrieval methods and advanced
deep learning to address two main questions: effectively retrieving relevant information and generating accurate responses. The workflow of RAG
is outlined in Section 2, categorizing the methodologies into pre-retrieval, retrieval, post-retrieval,
and generation phases. These sections, from 3 to
6, provide an in-depth analysis of the technologies
within these phases. Section 7 offers summaries of
the reviewed studies, along with the retrievers and
generators utilized. Section 8 details the evaluation
methodologies for RAG. Section 9 explores future
research directions, concentrating on text-based
studies and extending to image and multimodal
data considerations. The conclusion is presented in
Section 10.
The contributions of this paper are threefold:
This paper offers a comprehensive framework for
understanding the RAG domain, identifying areas
for improvement and challenges for future research.
It provides a detailed analysis of RAG’s core technologies, examining their strengths in addressing
retrieval and generation. Additionally, it introduces
the evaluation methods used in RAG research, highlighting current challenges and suggesting promising directions for future studies.
2 RAG Framework
The hallucinations are largely attributed to LLMs’
inability to access up-to-date information. This
limitation stems from the models’ reliance on their
training datasets. RAG proposes a solution to this
issue by supplementing the LLM’s training data
with current information from external sources
through a retrieval model, thereby enabling the generation of accurate responses. RAG presents a more
cost-effective alternative to the extensive training
and fine-tuning processes typically required for
LLMs. It allows for the dynamic incorporation
of fresh information via traditional retrieval methods or pre-trained LMs, without the need to directly
integrate this new data into the LLM. This feature
makes RAG both flexible and scalable, facilitating its application across different LLMs for various purposes. The information retrieved through
RAG is derived from real-world data, authored
by humans, which not only simplifies the generation process but also increases the reliability of
the generated responses. Figure 2 represents the
unified RAG framework with basic workflow and
paradigm.
Research by Khandelwal et al. (Khandelwal
et al., 2020) demonstrates that accessing relevant
information from the training dataset itself can significantly improve LLM performance, highlighting the effectiveness of RAG. Over time, RAG
has evolved from a means of providing supplementary information to enabling multiple interactions
between the retrieval and generation components.
This involves conducting several rounds of retrieval
to refine the accuracy of the information retrieved
and iteratively improve the quality of the generated output. Platforms such as LangChain1
and
LlamaIndex2 have modularized the RAG approach,
enhancing its adaptability and expanding its range
of applications. Despite these platforms employing
diverse methodologies to tackle different aspects of
RAG—from multiple search iterations to iterative
generation—they maintain adherence to the fundamental RAG workflow. This consistency is crucial
for understanding their operation and pinpointing
opportunities for further development.
2.1 Basic RAG Workflow
The foundational workflow of RAG begins with the
creation of an index comprising external sources.
This index serves as the basis for retrieving relevant
information through a retriever model based on a
specific query. The final step involves a generator
model, which combines the retrieved information
with the query to produce the desired output.
2.1.1 Indexing
Efficient retrieval begins with comprehensive indexing, where data preparation is key. This stage
involves text normalization processes such as tokenization, stemming, and the removal of stop words
to enhance the text’s suitability for indexing (Manning et al., 2008). Text segments are then organized
into sentences or paragraphs to facilitate more focused searches, allowing for the pinpointing of segments containing pertinent keywords. The integration of deep learning has revolutionized indexing
through the use of pretrained LMs for generating
semantic vector representations of texts. These vectors are stored, enabling rapid and precise retrieval from extensive data collections, significantly
enhancing retrieval efficiency.
2.1.2 Retrieval
While traditional retrieval methods, such as the
BM25 algorithm (Hancock-Beaulieu et al., 1996),
focus on term frequency and presence for document
ranking, they often overlook the semantic information of queries. Current strategies leverage pretrained LMs like BERT (Devlin et al., 2019), which
capture the semantic essence of queries more effectively. These models improve search accuracy by
considering synonyms and the structure of phrases,
thereby refining document ranking through the detection of semantic similarities. This is typically
achieved by measuring vector distances between
documents and queries, combining traditional retrieval metrics with semantic understanding to yield
search results that are both relevant and aligned
with user intent.
2.1.3 Generation
The generation phase is tasked with producing text
that is both relevant to the query and reflective of
the information found in the retrieved documents.
The usual method involves concatenating the query
with the retrieved information, which is then fed
into an LLM for text generation (Li et al., 2022).
Although ensuring the generated text’s alignment
and accuracy with the retrieved content presents
challenges, it is also essential to strike a balance between adhering closely to the source material and
infusing the output with creativity. The generated
text should accurately convey the information from
the retrieved documents and align with the query’s
intent, while also offering the flexibility to introduce new insights or perspectives not explicitly
contained within the retrieved data.
2.2 RAG Paradigm
The RAG paradigm organizes research within
the domain, offering a straightforward yet robust
framework to enhance LLM performance. Central to RAG is its search mechanism, crucial for
generating high-quality outcomes. Therefore, this
paradigm is structured into four main phases from
a retrieval perspective: pre-retrieval, retrieval, postretrieval, and generation. Both single-hop and
multi-hop retrieval approaches, encompassing iterative retrieve-generate cycles, follow this four-phase
structure. Figure 3 is the taxonomy tree of RAG’s
core techniques.
2.2.1 Pre-Retrieval
The pre-retrieval phase of retrieval-augmented generation lays the foundation for successful data and
query preparation, ensuring efficient information
retrieval. This phase includes essential tasks to
prepare for effective data access.
Indexing The process starts with indexing, which
establishes an organized system to enable fast and
accurate retrieval of information. The specificity
of indexing depends on the task and data type.
For example, sentence-level indexing is beneficial
for question-answering systems to precisely locate
answers, while document-level indexing is more
appropriate for summarizing documents to understand their main concepts and ideas.
Query Manipulation After indexing, query manipulation is performed to adjust user queries for
a better match with the indexed data. This involves query reformulation (Jansen et al., 2009;
Yu et al., 2020), which rewrites the query to align
more closely with the user’s intention; query expansion (Huang et al., 2013), which extends the query
to capture more relevant results through synonyms
or related terms; and query normalization, which
resolves differences in spelling or terminology for
consistent query matching.
Data Modification Data modification is also critical in enhancing retrieval efficiency. This step
includes preprocessing techniques like removing
irrelevant or redundant information to improve the
quality of results and enriching the data with additional information such as metadata to boost the
relevance and diversity of the retrieved content
(Bevilacqua et al., 2022a).
2.2.2 Retrieval
Search & Ranking The retrieval stage is the
combination of search and ranking. It focuses on
selecting and prioritizing documents from a dataset
to enhance the quality of the generation model’s
outputs. This stage employs search algorithms to
navigate through the indexed data, finding documents that match a user’s query. After identifying
relevant documents, the process of initially ranking
these documents starts to sort them according to
their relevance to the query.
2.2.3 Post-Retrieval
The post-retrieval phase serves to refine the initially
retrieved documents to improve the quality of text
generation. This phase consists of re-ranking and
filtering, each aimed at optimizing the document
selection for the final generation task.
Re-Ranking In the re-ranking step, the documents previously retrieved are reassessed, scored,
and reorganized. The objective is to more accurately highlight the documents most relevant to
the query and diminish the importance of the less
relevant ones. This step involves incorporating additional metrics and external knowledge sources
to enhance precision. In this context, pre-trained
models with superior accuracy but lower efficiency
can be effectively employed due to the limited set
of candidate documents available (Huang and Hu,
2009).
Filtering Filtering aims to remove documents
that fail to meet specified quality or relevance
standards. This can be done through several approaches, such as establishing a minimum relevance score threshold to exclude documents below
a certain relevance level. Furthermore, the use of
feedback from users or prior relevance evaluations
assists in adjusting the filtering process, guaranteeing that only the most relevant documents are
retained for text generation (Khattab and Zaharia,
2020; Huang and Huang, 2023).
2.2.4 Generation
The generation stage is a crucial component of the
RAG process, responsible for leveraging retrieved
information to enhance the quality of the generated
response. This stage encompasses several sub-steps
aimed at producing content that is readable, engaging, and informative.
Enhancing At the heart of the generation phase
is the enhancement step, where the objective is
to merge the retrieved information with the user’s
query to create a coherent and relevant response.
This includes the process of elaboration, adding
extra details to the retrieved content to enrich it. Efforts are focused on improving the output’s quality
by increasing its clarity, coherence, and stylistic
appeal through methods such as rephrasing and
restructuring. Information from various sources
is combined to offer a comprehensive perspective,
and verification is conducted to ensure the accuracy
and relevance of the content.
Customization Customization is an optional
step, involving the adjustment of content to align
with the user’s specific preferences or the context
of the request. This tailoring includes adapting the
content to meet the needs of the target audience or
the format in which it will be presented and condensing the information to succinctly convey the
essence of the content. The process also entails
creating summaries or abstracts that emphasize the
key points or arguments, ensuring the output is both
informative and concise.
3 Pre-Retrieval
3.1 Indexing
The integration of the k-nearest neighbor (kNN)
algorithm with pre-trained neural LMs, as demonstrated in kNN-LMs (Khandelwal et al., 2020), represents significant progress in language modeling. This method employs a datastore created from collections of texts, enabling the dynamic retrieval of
contextually relevant examples to improve perplexity without necessitating additional training.
Known for its efficiency, FAISS (Johnson et al.,
2021) has been adopted in many studies for indexing purposes (Khandelwal et al., 2020; Lewis et al.,
2020b; Khattab et al., 2022). Some research integrates enhancements like the Hierarchical Navigable Small World (HNSW) approximation (Malkov
and Yashunin, 2020) to achieve faster retrieval
(Lewis et al., 2020b). In addition, alternative tools
like utilizing the Bing API 3
for indexing based
on actual user search histories as outlined in Webgpt (Nakano et al., 2021), illustrate the variety of
indexing techniques under investigation.
Furthermore, MEMWALKER (Chen et al.,
2023a) introduces an innovative method to overcome the limitations of context window size in
LLMs by creating a memory tree from the input
text. This tree is formed by initially segmenting
the text into smaller pieces and then summarizing these segments into a hierarchical structure of
summary nodes, facilitating efficient indexing and
management of large volumes of information.
3.2 Query Manipulation
Studies such as FiD (Izacard and Grave, 2021),
COK(Li et al., 2023), and Query2doc (Wang et al.,
2023a) emphasize the significance of creating new
queries or refining existing ones to achieve more
3
https://www.microsoft.com/en-us/bing/apis/bing-websearch-api
pertinent retrieval results. These research efforts
highlight the necessity of efficiently gathering evidence from multiple passages and tailoring queries
to suit various knowledge sources, whether structured or unstructured. Techniques ranging from the
creation of pseudo-documents to enhance queries
have shown to bolster retrieval performance across
diverse information retrieval datasets.
Further exploration into query manipulation has
been conducted by Step-Back (Zheng et al., 2023)
and PROMPTAGATOR (Dai et al., 2023), which
focus on abstracting high-level concepts or utilizing LLMs for prompt-based query generation.
These strategies strive to better align queries with
the retrieval system’s functionality by rephrasing
tasks into more generalized versions or crafting
task-specific queries from limited examples. Such
methodologies enhance the consistency between
queries and indexed data, facilitating the retrieval
of more pertinent and insightful information.
Moreover, KnowledGPT (Wang et al., 2023b)
and Rewrite-Retrieve-Read (Ma et al., 2023) introduce approaches for query manipulation through
“program of thought” prompting and innovative
query rewriting techniques. KnowledGPT innovates by generating code to interface with knowledge bases, converting user queries into structured
search commands. In contrast, Rewrite-RetrieveRead utilizes a trainable compact LM for query
reformulation, adjusting them to more effectively
reflect the user’s intent and context.
Lastly, FLARE (Jiang et al., 2023) presents a
strategy based on confidence for query formulation, which focuses on crafting queries that precisely
reflect the information needs. This method incorporates the use of generated sentences or fragments
thereof as a foundation for search queries. By opting to directly use sentences, obscuring tokens of
low confidence, or formulating explicit questions,
this approach aims to boost the efficiency of the
retrieval process, ensuring that the retrieved information faithfully satisfies the requirements of the
generation process.
3.3 Data Modification
RA-DIT (Lin et al., 2023b) and RECITE (Sun et al.,
2023) emphasize enhancements through internal
data modifications. RA-DIT distinguishes between
fine-tuning datasets for LLMs and retrievers, aiming to bolster the LLM’s contextual comprehension
and the retriever’s ability to align with queries. RECITE, on the other hand, utilizes passage hints and
synthetic question-passage pairs to increase the
variety and relevance of its generated recitations
and responses. This approach seeks to broaden the
model’s knowledge base and improve its response
accuracy.
UPRISE (Cheng et al., 2023a) and GENREAD
(Yu et al., 2023a) target the refinement of external
data. UPRISE converts raw task data into a structured format and refines the selection of prompts
to enhance retrieval outcomes. In contrast, the
Clustering-Based Prompts method employed by
GENREAD generates documents from questions
and clusters them to eliminate irrelevant data, enriching the input with varied contextual insights.
This technique aims to improve the performance of
the generative model by providing it with a richer
set of information.
Furthermore, KnowledGPT (Wang et al., 2023b)
is dedicated to augmenting raw text data with structured, semantically rich information through entity
linking. This enrichment process not only structures the data more cohesively and makes it more
amenable to queries but also boosts the model’s
retrieval efficiency. It leverages precise, linked
knowledge to enhance the model’s understanding and its ability to generate relevant responses,
thereby improving its overall performance.
4 Retrieval
4.1 Search & Ranking
Atlas (Izacard et al., 2023) investigates few-shot
learning approaches, including Attention Distillation and Perplexity Distillation, to steer the retriever toward retrieving more relevant documents.
IRCOT (Trivedi et al., 2023) integrates retrieval
with reasoning to improve the effectiveness of retrieval. SURGE (Kang et al., 2023) employs a
subgraph retriever to extract relevant subgraphs
from a knowledge graph, while AAR (Yu et al.,
2023b) modifies search preferences to help LLMs
in fetching pertinent documents.
PRCA (Yang et al., 2023a) focuses on employing domain-specific abstractive summarization to
extract relevant and context-rich information from
documents, using a supervised learning strategy
to prioritize content crucial for accurate query responses. Meanwhile, MEMWALKER (Chen et al.,
2023a) leverages an internal search and ranking
mechanism in the constructed memory tree to identify pertinent information for long-context question
answering. Additionally, the Confidence-based Active Retrieval approach of FLARE (Jiang et al.,
2023) dynamically triggers information retrieval
based on the confidence levels of generated sentences, utilizing the insight that low-confidence
tokens signal a need for external knowledge.
5 Post-Retrieval
5.1 Re-Ranking
Re2G (Glass et al., 2022) introduces a sequencepair classification approach for re-ranking, utilizing a BERT transformer to simultaneously analyze
the query and passage. This interaction model, employing cross-attention between sequences, offers a
contrast to the representation model typically used
in initial retrieval phases. PROMPTAGATOR (Dai
et al., 2023) also employs a cross-attention model
for re-scoring. Its “Lift Yourself Up” strategy iteratively selects the best candidate from a pool for
further generation rounds, progressively improving
content quality via self-generated content.
Re-ranking is also a significant focus of InContext RALM (Ram et al., 2023). Two approaches to reranking are explored: zero-shot
reranking using language models and predictive
reranking through trained models. This step is
aimed at refining the selection of documents based
on their expected utility for improving language
model performance. ITER-RETGEN (Shao et al.,
2023), in particular, leverages knowledge distillation from the re-ranker to the dense retriever, finetuning retrieval efforts based on relevance signals
from LLM outputs. This optimization of the retrieval model aims to more accurately capture query
nuances, thereby improving document selection.
DKS-RAC (Huang et al., 2023) presents the
Dense Knowledge Similarity (DKS) for aligning
the knowledge between answers and retrieved passages at the sequence level. This approach is categorized under re-ranking due to its direct impact on
passage selection based on knowledge similarity,
refining the match between queries and documents.
FiD-light (Hofstätter et al., 2023) introduces a
listwise autoregressive re-ranking method that employs source pointers to optimize the ranking order.
This method maintains a link between the generated text and source passages, enabling a more
structured generation process. By incorporating
textual citations within the model’s output as pointers to relevant information sources, this approach
facilitates an organized retrieval and generation
process, enhancing the overall coherence and relevance of the generated content.
5.2 Filtering
COK (Li et al., 2023) presents the Progressive Rationale Correction technique, aimed at iteratively
refining rationales with retrieved knowledge. This
method constitutes a continuous optimization process, significantly enhancing the relevance and
quality of information used in content generation.
Self-RAG (Asai et al., 2023) introduces a selfreflection mechanism to efficiently filter out irrelevant content. By employing critique tokens, this
approach evaluates the relevance, supportiveness,
and utility of retrieved passages, ensuring the integration of only high-quality information into the
content generation process.
Additionally, FiD-TF (Berchansky et al., 2023)
and RECOMP (Xu et al., 2023) are dedicated to the
removal of irrelevant or redundant tokens and information from retrieved documents. FiD-TF employs
a dynamic mechanism to identify and eliminate unnecessary tokens, enhancing the efficiency of information processing. RECOMP, on the other hand,
compresses documents into concise summaries, focusing on selecting only the most pertinent content
for the generation process. These methods streamline the content generation workflow by ensuring
that only relevant and supportive information is
utilized, thereby improving the overall quality and
relevance of the generated content.
6 Generation
6.1 Enhancing
DSP (Khattab et al., 2022) introduces a framework
designed to generate multiple retrieval queries to
summarize and answer questions, drawing upon information aggregated from various passages. This
framework employs CombSUM (Fox and Shaw,
1994) to calculate a cumulative probability score
for passages across different retrieval lists, facilitating the compilation of a comprehensive response
from multiple sources.
PRCA (Yang et al., 2023a) outlines a RewardDriven Stage, wherein the distilled context is refined based on feedback from the generator. Utilizing reinforcement learning, this stage adjusts
the parameters of PRCA according to the rewards
received for providing relevant context. The objective is to fine-tune the extracted context to meet
the specific requirements of the generator, thereby
optimizing the generation process.
REPLUG (Shi et al., 2023) proposes a method
for prepending retrieved documents to the input
context before the final prediction by the black-box
LM. It introduces an ensemble strategy to encode
retrieved documents in parallel, overcoming the
limitations of LM context length and enhancing
accuracy through the allocation of increased computational resources. This approach improves the
generation process by ensuring that the LM has
access to a broader range of relevant information.
RECITE (Sun et al., 2023) implements a selfconsistency technique, which involves generating
multiple recitations independently and employing
a plurality/majority vote system to determine the
most appropriate answer. This method is designed
to increase the reliability and accuracy of the answers, thereby improving the quality and credibility
of the output.
6.2 Customization
The PKG framework, introduced by (Luo et al.,
2023), represents an approach to customizing the
output of LMs. By generating background knowledge internally using a pre-trained model, PKG
eliminates the need for traditional external retrieval
processes. This method directly integrates domainor task-specific knowledge into the generation step,
significantly enhancing the LM’s capacity to produce responses that are specifically tailored to the
given context or requirements.
Self-RAG (Asai et al., 2023) offers a strategy that incorporates reflection tokens within a customizable decoding algorithm. This technique permits dynamic adjustment of the model’s retrieval
and generation behaviors based on the specific task,
facilitating more versatile response generation. Depending on the requirements, this approach can be
tuned for accuracy or creativity, providing flexibility in generating outputs that meet diverse needs.
SURGE (Kang et al., 2023) achieves customization through the application of graph-text contrastive learning. This method ensures that the
generated dialogue responses are in tight alignment
with the knowledge contained in the retrieved subgraph, yielding responses that are specific, relevant,
and deeply rooted in the dialogue context. By maintaining consistency between the retrieved knowledge and the generated text, SURGE is capable
of producing outputs that precisely reflect the detailed knowledge of the subgraph, enhancing the
relevance and specificity of the responses.
7 Comparisons of RAG
7.1 The Comprehensive Summary of RAG
Table 1 presents a detailed analysis of the RAG
studies discussed in this paper. The analysis shows
that the majority of these studies have utilized external data sources to enrich the content of LLMs.
A preference for multiple-hop over single-hop retrieval was noted, indicating that iterative search
rounds generally yield superior results. In other
words, most methods employ dense retrieval to secure higher quality candidate documents. Compared to modifying datasets in the pre-retrieval
stage, more studies focus on manipulating the query
to improve retrieval performance. Additionally,
there is a significant emphasis on optimizing the
retrieval phase, highlighting its crucial role in the
research. However, there seems to be a scarcity
of studies concentrating on customization in the
generation stage, pointing to this as a potential area
for future exploration. Overall, while the goal of
RAG is to enhance the response quality of LLMs,
greater efforts have been directed towards improving retrieval aspects.
7.2 Retriever and Generator
In RAG, the retriever and the generator are the
primary components. Table 2 summarizes the retrievers and generators used in the studies discussed
in this paper. It is clear from the table that while
most generators utilize advanced language models,
a significant number of retrievers still employ the
traditional BM25 due to its efficiency. The method
of retrieval is a crucial aspect in RAG, highlighting the importance of exploring ways to enhance
retrieval performance without compromising efficiency. Similarly, not many studies have adopted
powerful LLMs such as LLaMA2, GPT-3.5, or
GPT-4 as their generators. LLMs like T5 remain
popular, yet fundamental models like BERT and
Transformers are rarely used in 2023. Compared
to generators, it is evident that not many IR-based
LLMs are used in retrievers, indicating a promising
direction for developing such models in the future.
8 Evaluation in RAG
To understand the effectiveness of LMs in generating more accurate, relevant, and robust responses
by leveraging external knowledge, the evaluation
of RAG systems has become a significant research
area. With the popularity of dialogue-based interactions, recent works have been focused on assessing
the performance of RAG models on such downstream tasks using established metrics like Exact
Match (EM) and F1 scores. Furthermore, a wide
array of datasets has been utilized for this purpose,
including TriviaQA (Joshi et al., 2017), HotpotQA
(Yang et al., 2018), FEVER (Thorne et al., 2018),
Natural Questions (Kwiatkowski et al., 2019), Wizard of Wikipedia (Dinan et al., 2019), and T-REX
(ElSahar et al., 2018).
However, evaluation solely from the perspective of downstream tasks falls short in addressing
the evolving needs of RAG development. Recent
research has introduced various frameworks and
benchmarks that aim to evaluate these systems
across multiple dimensions, including the quality
of the generated text, the relevance of retrieved
documents, and the model’s resilience to misinformation, as shown in Table 3. These evaluations focus on assessing specific capabilities such as noise
robustness, negative prompting, information integration, and counterfactual robustness, highlighting the complex challenges faced by RAG systems
in practical applications. The continuous development of evaluation frameworks and metrics is
crucial for advancing the field, broadening the applicability of RAG systems, and ensuring they meet
the demands of a complex and evolving information landscape.
8.1 Retrieval-based Aspect
In information retrieval, the quality of search results is typically evaluated using standard metrics
such as Mean Average Precision (MAP), Precision,
Reciprocal Rank, and Normalized Discounted Cumulative Gain (NDCG) (Radlinski and Craswell,
2010; Reimers and Gurevych, 2019; Nogueira et al.,
2019). These metrics primarily assess the relevance
of retrieved documents to a given query.
Retrieval-based Metrics in RAG focus on the effectiveness of retrieving relevant information to
support generation tasks. These include Accuracy, which measures the precision of retrieved
documents in providing correct information for answering queries, and Rejection Rate (Chen et al.,
2023b), assessing a system’s ability to decline answering when no relevant information is found.
Additionally, Error Detection Rate (Chen et al.,
2023b) evaluates the model’s capability to identify
and disregard incorrect or misleading information
from retrieved documents. Context Relevance is
another essential metric, assessing the pertinence
of the retrieved documents to the query. It’s vital to
ensure the information used to generate responses
is directly related to the query’s context. Faithfulness (Shahul et al., 2023) measures the accuracy
with which the generated content reflects the information in the retrieved documents, ensuring that
the generation process with no misinformation.
8.2 Generation-based Aspect
Evaluating the quality of text produced by LLMs
involves analyzing their performance on various
downstream tasks using standard metrics. These
metrics assess linguistic quality, coherence, accuracy, and the extent to which the generated text
reflects ground-truth data. Linguistic quality and
coherence are evaluated through metrics such as
BLEU (Papineni et al., 2002), which measures fluency and similarity to human-produced text, and
ROUGE-L (Lin, 2004), which quantifies the overlap with reference summaries to gauge the text’s
capacity to encapsulate main ideas and phrases.
Accuracy and overlap with ground-truth data are
gauged using metrics like EM and F1 Score, which
respectively determine the percentage of answers
that are entirely correct and offer a balanced assessment of precision and recall in retrieving relevant
answers while minimizing inaccuracies.
Beyond these standard metrics, the evaluation
may also incorporate task-specific criteria and
novel metrics tailored to particular applications.
For instance, in dialogue generation, perplexity
and entropy are used to evaluate response diversity and naturalness. Additionally, metrics such as
Misleading Rate and Mistake Reappearance Rate
(Liu et al., 2023) gauge a model’s ability to avoid
misinformation and inaccuracies. Other specialized metrics include Answer Relevance (Shahul
et al., 2023), assessing the precision of responses
to queries; Kendall’s tau (Saad-Falcon et al., 2023),
for evaluating the accuracy of RAG system rankings; Micro-F1 (Saad-Falcon et al., 2023), which
fine-tunes accuracy evaluation in tasks with multiple correct answers; and Prediction Accuracy, directly measuring the alignment of generated answers with expected responses, thereby offering a
direct insight into a system’s effectiveness in generating accurate content.
9 Future Directions
9.1 Retrieval Quality
The integration of RAG into LLMs faces significant
hurdles due to the vast amounts of unreliable information on the internet, including fake news. This
presents a challenge for accurately retrieving useful
knowledge, leading to the unreliable generation of
responses by LLMs. As a result, LLMs may generate content based on incorrect information, undermining their reliability. Recent research efforts
are directed towards enhancing retrieval methods
to improve the efficiency, scalability, and effectiveness of LLMs in generating accurate and reliable
responses.
Differentiable Search Indices (Tay et al., 2022)
and (Bevilacqua et al., 2022b) developed differentiable search indices that integrate the retrieval
process within a Transformer model, enabling direct mapping of text queries to document identifiers.
These approaches offer superior performance and
potential for more efficient and scalable retrieval.
Generative Models for Search GERE (Chen
et al., 2022a) can directly generate document titles
and evidence sentences for fact-verification tasks.
PARADE (Li et al., 2024) is a method for document
reranking that aggregates passage representations
into a unified document relevance score. Both of
them demonstrate significant improvements in retrieval quality over traditional methods.
Fine-tuning Pre-trained Language Models
RankT5 (Zhuang et al., 2023) is a model that finetunes the T5 framework specifically for text ranking. It leverages ranking losses to optimize performance metrics and exhibits promising zero-shot
performance on out-of-domain data.
Noise Power (Cuconasu et al., 2024) provide a
comprehensive analysis of the impact of IR components on RAG systems, revealing that the inclusion
of irrelevant documents can significantly improve
accuracy. It challenges conventional retrieval strategies and underscores the potential for developing
specialized approaches that integrate retrieval with
language generation models.
9.2 Multimodal RAG
The multimodal RAG domain has experienced significant growth, highlighting a pivotal advancement
at the confluence of text and visual comprehension.
The introduction of MuRAG (Chen et al., 2022b)
marked a breakthrough by amalgamating textual
and visual information for language generation, establishing a new standard for multimodal datasets.
This model showcased the efficacy of utilizing a
multimodal memory system to boost the accuracy
in question-answering and reasoning tasks.
After MuRAG, studies such as REVEAL (Hu
et al., 2023) and Re-Imagen (Chen et al., 2023c)
have focused on enhancing visual question answering and text-to-image generation. They achieved
this through the incorporation of dynamic retrieval
mechanisms and the improvement of image fidelity,
respectively. These advancements laid the groundwork for further models by researchers like Sarto
et al. (Sarto et al., 2022) for image captioning,
and Yuan et al. (Yuan et al., 2023) for text-to-audio
generation, broadening the scope of RAG’s application across different modalities and improving the
quality and realism of the generated outputs. Furthermore, Re-ViLM (Yang et al., 2023b) refined
image captioning capabilities through a retrievalaugmented visual language model. By fine-tuning
model parameters and implementing innovative filtering strategies, it has made strides in producing
more precise and contextually appropriate captions.
By tapping into external resources, these models
have provided significant enhancements over traditional benchmarks, highlighting the advantage of
integrating diverse sources of knowledge.
10 Conclusions
In this paper, we have presented a comprehensive framework for understanding the RAG domain, highlighting its significance in enhancing
the capabilities of LLMs. Through a structured
overview of RAG, categorizing various methods,
and an in-depth analysis of its core technologies
and evaluation methods, this study illuminates the
path for future research. It identifies crucial areas
for improvement and outlines potential directions
for advancing RAG applications, especially in textual contexts. This survey aims to elucidate the
core concepts of the RAG field from a retrieval
perspective, and it is intended to facilitate further
exploration and innovation in the accurate retrieval
and generation of information.
11 Limitations
This survey comprehensively examines existing
RAG models, summarizing their core techniques
into four main steps from a retrieval perspective. It
recognizes that some methods may encompass multiple steps and that decoupling these steps could
potentially obscure their intrinsic connections. Nevertheless, the primary objective is to simplify the
complexity of the approach, clearly delineating the
specific problems it addresses. This allows for a
clearer identification of areas ripe for further optimization and improvement. Despite the thorough investigation, the rapid evolution of the field and
page limits mean that certain aspects might not
have been fully analyzed and explored, or recent
developments could have been missed. While the
paper references evaluation methods that can aid
in the development of RAG, it also acknowledges
mature tools like LangChain and LlamaIndex as
useful resources. However, the focus of this survey
is not on detailing the evaluation pipeline or how
these tools are specifically used, but rather on illustrating how evaluation aspects can support the
advancement of RAG. This choice highlights an
area for future work, emphasizing the importance
of methodological clarity and the application of
evaluation tools in refining and enhancing RAG
models.
Acknowledgements
This work was supported by the Natural Sciences
and Engineering Research Council (NSERC) of
Canada and the York Research Chairs (YRC) program.
Acknowledgements
This work was supported by the Natural Sciences
and Engineering Research Council (NSERC) of
Canada and the York Research Chairs (YRC) program.
References
Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and
Hannaneh Hajishirzi. 2023. Self-RAG: Learning
to Retrieve, Generate, and Critique through SelfReflection. arXiv, abs/2310.11511.
Moshe Berchansky, Peter Izsak, Avi Caciularu, Ido
Dagan, and Moshe Wasserblat. 2023. Optimizing
Retrieval-augmented Reader Models via Token Elimination. In Proceedings of the 2023 Conference on
Empirical Methods in Natural Language Processing,
pages 1506–1524. Association for Computational
Linguistics.
Michele Bevilacqua, Giuseppe Ottaviano, Patrick S. H.
Lewis, Scott Yih, Sebastian Riedel, and Fabio Petroni.
2022a. Autoregressive search engines: Generating
substrings as document identifiers. In Advances in
Neural Information Processing Systems 35: Annual
Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA,
November 28 - December 9, 2022.
Michele Bevilacqua, Giuseppe Ottaviano, Patrick S. H.
Lewis, Scott Yih, Sebastian Riedel, and Fabio Petroni.
2022b. Autoregressive Search Engines: Generating Substrings as Document Identifiers. In Conference on Neural Information Processing Systems
(NeurIPS).
Sid Black, Gao Leo, Phil Wang, Connor Leahy,
and Stella Biderman. 2021. GPT-Neo: Large
Scale Autoregressive Language Modeling with MeshTensorflow.
Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,
Trevor Cai, Eliza Rutherford, Katie Millican, George
van den Driessche, Jean-Baptiste Lespiau, Bogdan
Damoc, Aidan Clark, Diego de Las Casas, Aurelia
Guy, Jacob Menick, Roman Ring, Tom Hennigan,
Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen
Simonyan, Jack W. Rae, Erich Elsen, and Laurent
Sifre. 2022. Improving Language Models by Retrieving from Trillions of Tokens. In International Conference on Machine Learning (ICML), pages 2206–
2240.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.
2020. Language Models are Few-Shot Learners. In
Conference on Neural Information Processing Systems (NeurIPS), volume abs/2005.14165.
Howard Chen, Ramakanth Pasunuru, Jason Weston, and
Asli Celikyilmaz. 2023a. Walking Down the Memory
Maze: Beyond Context Limit through Interactive
Reading. arXiv, abs/2310.05029.
Jiangui Chen, Ruqing Zhang, Jiafeng Guo, Yixing Fan,
and Xueqi Cheng. 2022a. Gere: Generative Evidence
Retrieval for Fact Verification. In Proceedings of
the 45th International ACM SIGIR Conference on
Research and Development in Information Retrieval.
ACM.
Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun.
2023b. Benchmarking large language models in retrieval-augmented generation. arXiv,
abs/2309.01431.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, and others. 2021. Evaluating
large language models trained on code. arXiv,
abs/2107.03374.
Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga,
and William Cohen. 2022b. Murag: Multimodal
Retrieval-Augmented Generator for Open Question
Answering over Images and Text. In Proceedings
of the 2022 Conference on Empirical Methods in
Natural Language Processing (EMNLP).
Wenhu Chen, Hexiang Hu, Chitwan Saharia, and
William W. Cohen. 2023c. Re-Imagen: RetrievalAugmented Text-to-Image Generator. In International Conference on Learning Representations
(ICLR).
Zhihong Chen, Feng Jiang, Junying Chen, Tiannan
Wang, Fei Yu, Guiming Chen, Hongbo Zhang, Juhao
Liang, Chen Zhang, Zhiyi Zhang, and others. 2023d.
Phoenix: Democratizing chatgpt across languages.
arXiv, abs/2304.10453.
Daixuan Cheng, Shaohan Huang, Junyu Bi, Yuefeng
Zhan, Jianfeng Liu, Yujing Wang, Hao Sun, Furu
Wei, Weiwei Deng, and Qi Zhang. 2023a. Uprise:
Universal Prompt Retrieval for Improving Zero-Shot
Evaluation. In Proceedings of the 2023 Conference
on Empirical Methods in Natural Language Processing, pages 12318–12337. Association for Computational Linguistics.
Xin Cheng, Di Luo, Xiuying Chen, Lemao Liu,
Dongyan Zhao, and Rui Yan. 2023b. Lift Yourself Up: Retrieval-augmented Text Generation with
Self-Memory. In Thirty-seventh Conference on
Neural Information Processing Systems, volume
abs/2305.02437.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, Parker Schuh, Kensen Shi,
Sasha Tsvyashchenko, Joshua Maynez, Abhishek
Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben
Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,
Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
Sunipa Dev, Henryk Michalewski, Xavier Garcia,
Vedant Misra, Kevin Robinson, Liam Fedus, Denny
Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim,
Barret Zoph, Alexander Spiridonov, Ryan Sepassi,
David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,
Rewon Child, Oleksandr Polozov, Katherine Lee,
Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark
Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy
Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,
and Noah Fiedel. 2023. Palm: Scaling Language
Modeling with Pathways. Journal of Machine Learning Research (JMLR), 24:240:1–240:113.
Hyung Won Chung, Le Hou, S. Longpre, Barret
Zoph, Yi Tay, W. Fedus, Eric Li, Xuezhi Wang,
Mostafa Dehghani, Siddhartha Brahma, Albert Webson, S. Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen,
Aakanksha Chowdhery, Dasha Valter, Sharan Narang,
Gaurav Mishra, Adams Wei Yu, Vincent Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav
Petrov, E. Chi, J. Dean, Jacob Devlin, Adam Roberts,
Denny Zhou, Quoc V. Le, and Jason Wei. 2022. Scaling Instruction-Finetuned Language Models. arXiv,
abs/2210.11416.
Alexis Conneau, Kartikay Khandelwal, Naman Goyal,
Vishrav Chaudhary, Guillaume Wenzek, Francisco
Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised
Cross-lingual Representation Learning at Scale. In
Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440–
8451. Association for Computational Linguistics.
Florin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice, Cesare Campagnano, Yoelle
Maarek, Nicola Tonellotto, and Fabrizio Silvestri.
2024. The Power of Noise: Redefining Retrieval for
RAG Systems. arXiv, abs/2401.14887.
Zhuyun Dai, Vincent Y. Zhao, Ji Ma, Yi Luan, Jianmo
Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B.
Hall, and Ming-Wei Chang. 2023. Promptagator:
Few-shot Dense Retrieval From 8 Examples. In International Conference on Learning Representations
(ICLR).
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. Bert: Pre-training of Deep
Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the
North, pages 4171–4186. Association for Computational Linguistics.
Emily Dinan, Stephen Roller, Kurt Shuster, Angela
Fan, Michael Auli, and Jason Weston. 2019. Wizard
of Wikipedia: Knowledge-Powered Conversational
Agents. In International Conference on Learning
Representations (ICLR).
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,
Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm:
General Language Model Pretraining with Autoregressive Blank Infilling. In Proceedings of the 60th
Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers). Association for
Computational Linguistics.
Hady ElSahar, Pavlos Vougiouklis, Arslen Remaci,
Christophe Gravier, Jonathon S. Hare, Frédérique
Laforest, and Elena Simperl. 2018. T-REx: A Large
Scale Alignment of Natural Language with Knowledge Base Triples. In International Conference on
Language Resources and Evaluation (LREC).
Zhangyin Feng, Xiaocheng Feng, Dezhi Zhao, Maojin Yang, and Bing Qin. 2023. Retrieval-generation
synergy augmented large language models. arXiv,
abs/2310.05149.
Edward A. Fox and Joseph A. Shaw. 1994. Combination of multiple searches. In TREC-2: Text retrieval
conference, 500215, pages 105–108.
Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.
Simcse: Simple Contrastive Learning of Sentence
Embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language
Processing, pages 6894–6910. Association for Computational Linguistics.
Michael Glass, Gaetano Rossiello, Md Faisal Mahbub
Chowdhury, Ankita Naik, Pengshan Cai, and Alfio
Gliozzo. 2022. Re2g: Retrieve, Rerank, Generate.
In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,
pages 2701–2715. Association for Computational
Linguistics.
Simon Gottschalk and Elena Demidova. 2018. EventKG: A Multilingual Event-Centric Temporal Knowledge Graph. Springer International Publishing.
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,
and Ming-Wei Chang. 2020. Retrieval Augmented
Language Model Pre-Training. In International Conference on Machine Learning (ICML), pages 3929–
3938.
William L. Hamilton. 2020. Graph representation learning. Springer International Publishing.
Micheline Hancock-Beaulieu, Mike Gatford, Xiangji
Huang, Stephen E. Robertson, Steve Walker, and
P. W. Williams. 1996. Okapi at TREC-5. In Proceedings of The Fifth Text REtrieval Conference, TREC
1996, Gaithersburg, Maryland, USA, November 20-
22, 1996, volume 500-238 of NIST Special Publication. National Institute of Standards and Technology
(NIST).
Sebastian Hofstätter, Jiecao Chen, Karthik Raman, and
Hamed Zamani. 2023. Fid-light: Efficient and effective retrieval-augmented text generation. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information
Retrieval, pages 1437–1447.
Ziniu Hu, Ahmet Iscen, Chen Sun, Zirui Wang,
Kai-Wei Chang, Yizhou Sun, Cordelia Schmid,
David A. Ross, and Alireza Fathi. 2023. Reveal:
Retrieval-Augmented Visual-Language Pre-Training
with Multi-Source Multimodal Knowledge Memory. In 2023 IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR), pages 23369–
23379. IEEE.
Jie Huang, Hanyin Shao, Kevin Chen-Chuan Chang,
Jinjun Xiong, and Wen-mei Hwu. 2022. Understanding Jargon: Combining Extraction and Generation
for Definition Modeling. In Proceedings of the 2022
Conference on Empirical Methods in Natural Language Processing. Association for Computational
Linguistics.
Jimmy Xiangji Huang, Jun Miao, and Ben He. 2013.
High performance query expansion using adaptive
co-training. Inf. Process. Manag., 49(2):441–453.
Wenyu Huang, Mirella Lapata, Pavlos Vougiouklis,
Nikos Papasarantopoulos, and Jeff Z Pan. 2023. Retrieval Augmented Generation with Rich Answer Encoding. Proc. of IJCNLP-AACL, 2023.
Xiangji Huang and Qinmin Hu. 2009. A bayesian learning approach to promoting diversity in ranking for
biomedical information retrieval. In Proceedings of
the 32nd Annual International ACM SIGIR Conference on Research and Development in Information
Retrieval, SIGIR 2009, Boston, MA, USA, July 19-23,
2009, pages 307–314. ACM.
Yizheng Huang and Jimmy Huang. 2024. Exploring
chatgpt for next-generation information retrieval: Opportunities and challenges. CoRR, abs/2402.11203.
Yizheng Huang and Jimmy X. Huang. 2023. Diversified
prior knowledge enhanced general language model
for biomedical information retrieval. In ECAI 2023 -
26th European Conference on Artificial Intelligence,
September 30 - October 4, 2023, Kraków, Poland - Including 12th Conference on Prestigious Applications
of Intelligent Systems (PAIS 2023), volume 372 of
Frontiers in Artificial Intelligence and Applications,
pages 1109–1115. IOS Press.
Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and
Edouard Grave. 2022. Unsupervised Dense Information Retrieval with Contrastive Learning. Transactions on Machine Learning Research (TMLR), 2022.
Gautier Izacard and Edouard Grave. 2021. Leveraging
Passage Retrieval with Generative Models for Open
Domain Question Answering. In Proceedings of the
16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,
pages 874–880. Association for Computational Linguistics.
Gautier Izacard, Patrick S. H. Lewis, Maria Lomeli,
Lucas Hosseini, Fabio Petroni, Timo Schick, Jane
Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and
Edouard Grave. 2023. Atlas: Few-shot Learning with
Retrieval Augmented Language Models. Journal
of Machine Learning Research (JMLR), 24:251:1–
251:43.
Israt Jahan, Md. Tahmid Rahman Laskar, Chun Peng,
and Jimmy Xiangji Huang. 2023. Evaluation of
chatgpt on biomedical tasks: A zero-shot comparison with fine-tuned generative transformers. CoRR,
abs/2306.04504.
Bernard J. Jansen, Danielle L. Booth, and Amanda
Spink. 2009. Patterns of query reformulation during web searching. J. Assoc. Inf. Sci. Technol.,
60(7):1358–1371.
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu,
Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea
Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. ACM Comput.
Surv., 55(12):248:1–248:38.
Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun,
Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie
Callan, and Graham Neubig. 2023. Active Retrieval
Augmented Generation. In Conference on Empirical
Methods in Natural Language Processing (EMNLP),
pages 7969–7992.
Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2021.
Billion-scale similarity search with gpus. IEEE
Transactions on Big Data, 7(3):535–547.
Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke
Zettlemoyer. 2017. Triviaqa: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. In Proceedings of the 55th Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601–1611. Association
for Computational Linguistics.
Minki Kang, Jin Myung Kwak, Jinheon Baek,
and Sung Ju Hwang. 2023. Knowledge
Graph-Augmented Language Models for
Knowledge-Grounded Dialogue Generation.
arXiv, abs/2305.18846.
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick
S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen,
and Wen-tau Yih. 2020. Dense Passage Retrieval for
Open-Domain Question Answering. In Conference
on Empirical Methods in Natural Language Processing (EMNLP), pages 6769–6781.
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke
Zettlemoyer, and Mike Lewis. 2020. Generalization
through Memorization: Nearest Neighbor Language
Models. In International Conference on Learning
Representations (ICLR).
O. Khattab, Keshav Santhanam, Xiang Lisa Li, David
Leo Wright Hall, Percy Liang, Christopher Potts,
and M. Zaharia. 2022. Demonstrate-Search-Predict:
Composing retrieval and language models for
knowledge-intensive NLP. arXiv, abs/2212.14024.
Omar Khattab and Matei Zaharia. 2020. Colbert - Efficient and Effective Passage Search via Contextualized Late Interaction over BERT. In Proceedings of
the 43rd International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 39–48. ACM.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew
Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob
Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural
Questions: A Benchmark for Question Answering
Research. Transactions of the Association for Computational Linguistics, 7:453–466.
Md. Tahmid Rahman Laskar, M. Saiful Bari, Mizanur
Rahman, Md Amran Hossen Bhuiyan, Shafiq Joty,
and Jimmy Xiangji Huang. 2023. A systematic study
and comprehensive evaluation of chatgpt on benchmark datasets. CoRR, abs/2305.18486.
Md. Tahmid Rahman Laskar, Enamul Hoque, and
Jimmy X. Huang. 2020. Query focused abstractive
summarization via incorporating query relevance and
transfer learning with transformer models. In Advances in Artificial Intelligence - 33rd Canadian Conference on Artificial Intelligence, Canadian AI 2020,
Ottawa, ON, Canada, May 13-15, 2020, Proceedings,
volume 12109 of Lecture Notes in Computer Science,
pages 342–348. Springer.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
Ghazvininejad, Abdelrahman Mohamed, Omer Levy,
Veselin Stoyanov, and Luke Zettlemoyer. 2020a.
Bart: Denoising Sequence-to-Sequence Pre-training
for Natural Language Generation, Translation, and
Comprehension. In Proceedings of the 58th Annual
Meeting of the Association for Computational Linguistics, pages 7871–7880. Association for Computational Linguistics.
Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman
Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,
Tim Rocktäschel, Sebastian Riedel, and Douwe
Kiela. 2020b. Retrieval-Augmented Generation for
Knowledge-Intensive NLP Tasks. In Conference on
Neural Information Processing Systems (NeurIPS).
Canjia Li, Andrew Yates, Sean MacAvaney, Ben He,
and Yingfei Sun. 2024. Parade: Passage Representation Aggregation forDocument Reranking. ACM
Transactions on Information Systems, 42(2):1–26.
Huayang Li, Yixuan Su, Deng Cai, Yan Wang, and
Lemao Liu. 2022. A Survey on Retrieval-Augmented
Text Generation. arXiv, abs/2202.01110.
Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng
Ding, Shafiq R. Joty, Soujanya Poria, and Lidong
Bing. 2023. Chain-of-Knowledge: Grounding Large
Language Models via Dynamic Knowledge Adapting
over Heterogeneous Sources. arXiv.
Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74–81, Barcelona, Spain.
Association for Computational Linguistics.
Sheng-Chieh Lin, Akari Asai, Minghan Li, Barlas
Oguz, Jimmy Lin, Yashar Mehdad, Wen-tau Yih,
and Xilun Chen. 2023a. How to Train Your Dragon:
Diverse Augmentation Towards Generalizable Dense
Retrieval. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 6385–
6400. Association for Computational Linguistics.
Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi,
Maria Lomeli, Rich James, Pedro Rodriguez, Jacob
Kahn, Gergely Szilvasy, Mike Lewis, and others.
2023b. Ra-dit: Retrieval-augmented dual instruction tuning. arXiv, abs/2310.01352.
Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu
Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth
Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav
Chaudhary, Brian O’Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, and Xian Li. 2022. Few-shot Learning with
Multilingual Generative Language Models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Association for
Computational Linguistics.
Yi Liu, Lianzhe Huang, Shicheng Li, Sishuo Chen,
Hao Zhou, Fandong Meng, Jie Zhou, and Xu Sun.
2023. Recall: A Benchmark for LLMs Robustness
against External Counterfactual Knowledge. arXiv,
abs/2311.08147.
Ziyang Luo, Can Xu, Pu Zhao, Xiubo Geng, Chongyang
Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023.
Augmented Large Language Models with Parametric
Knowledge Guiding. arXiv, abs/2305.04757.
Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao,
and Nan Duan. 2023. Query Rewriting in RetrievalAugmented Large Language Models. In Proceedings
of the 2023 Conference on Empirical Methods in
Natural Language Processing, pages 5303–5315. Association for Computational Linguistics.
Yu A. Malkov and D. A. Yashunin. 2020. Efficient
and robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 42(4):824–836.
Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze. 2008. Introduction to Information Retrieval. Cambridge University Press.
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff
Wu, Long Ouyang, Christina Kim, Christopher
Hesse, Shantanu Jain, Vineet Kosaraju, William
Saunders, and others. 2021. Webgpt: Browserassisted question-answering with human feedback.
arXiv, abs/2112.09332.
Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan,
Keith Hall, Ming-Wei Chang, and Yinfei Yang. 2022.
Large Dual Encoders Are Generalizable Retrievers.
In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages
9844–9855. Association for Computational Linguistics.
Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, and
Jimmy Lin. 2019. Multi-stage document ranking
with BERT. CoRR, abs/1910.14424.
OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal,
Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin,
Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mo Bavarian, Jeff Belgum, Irwan Bello,
Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman,
Tim Brooks, Miles Brundage, Kevin Button, Trevor
Cai, Rosie Campbell, Andrew Cann, Brittany Carey,
Chelsea Carlson, Rory Carmichael, Brooke Chan,
Che Chang, Fotis Chantzis, Derek Chen, Sully Chen,
Ruby Chen, Jason Chen, Mark Chen, Ben Chess,
Chester Cho, Casey Chu, Hyung Won Chung, Dave
Cummings, Jeremiah Currier, Yunxing Dai, Cory
Decareaux, Thomas Degry, Noah Deutsch, Damien
Deville, Arka Dhar, David Dohan, Steve Dowling,
Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna
Eloundou, David Farhi, Liam Fedus, Niko Felix,
Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik
Goel, Tarun Gogineni, Gabriel Goh, Rapha GontijoLopes, Jonathan Gordon, Morgan Grafstein, Scott
Gray, Ryan Greene, Joshua Gross, Shixiang Shane
Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris,
Yuchen He, Mike Heaton, Johannes Heidecke, Chris
Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele,
Brandon Houghton, Kenny Hsu, Shengli Hu, Xin
Hu, Joost Huizinga, Shantanu Jain, Shawn Jain,
Joanne Jang, Angela Jiang, Roger Jiang, Haozhun
Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo
Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak
Khan, Logan Kilpatrick, Jong Wook Kim, Christina
Kim, Yongjik Kim, Hendrik Kirchner, Jamie Kiros,
Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk,
Andrew Kondrich, Aris Konstantinidis, Kyle Kosic,
Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai
Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy,
Chak Ming Li, Rachel Lim, Molly Lin, Stephanie
Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe,
Patricia Lue, Anna Makanju, Kim Malfacini, Sam
Manning, Todor Markov, Yaniv Markovski, Bianca
Martin, Katie Mayer, Andrew Mayne, Bob McGrew,
Scott Mayer McKinney, Christine McLeavey, Paul
McMillan, Jake McNeil, and others. 2023. Gpt-4
Technical Report. PREPRINT.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll L. Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray,
John Schulman, Jacob Hilton, Fraser Kelton, Luke
Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe.
2022. Training language models to follow instructions with human feedback. In Conference on Neural
Information Processing Systems (NeurIPS).
Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computational Linguistics, ACL ’02, page 311–318, USA.
Association for Computational Linguistics.
Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. Instruction tuning with
gpt-4. arXiv.
Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick
Lewis, Majid Yazdani, Nicola De Cao, James Thorne,
Yacine Jernite, Vladimir Karpukhin, Jean Maillard,
Vassilis Plachouras, Tim Rocktäschel, and Sebastian
Riedel. 2021. Kilt: a Benchmark for Knowledge Intensive Language Tasks. In Proceedings of the 2021
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 2523–2544. Association for Computational Linguistics.
Filip Radlinski and Nick Craswell. 2010. Comparing
the sensitivity of information retrieval metrics. In
Proceedings of the 33rd International ACM SIGIR
Conference on Research and Development in Information Retrieval, SIGIR ’10, page 667–674, New
York, NY, USA. Association for Computing Machinery.
Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. 2020. Exploring
the Limits of Transfer Learning with a Unified Textto-Text Transformer. Journal of Machine Learning
Research (JMLR), 21:140:1–140:67.
Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,
Amnon Shashua, Kevin Leyton-Brown, and Yoav
Shoham. 2023. In-Context Retrieval-Augmented
Language Models. Transactions of the Association
for Computational Linguistics, 11:1316–1331.
Ori Ram, Gal Shachaf, Omer Levy, Jonathan Berant,
and Amir Globerson. 2022. Learning to Retrieve
Passages without Supervision. In Proceedings of the
2022 Conference of the North American Chapter of
the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics.
Nils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence Embeddings using Siamese BERTNetworks. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages
3980–3990. Association for Computational Linguistics.
Stephen Robertson and Hugo Zaragoza. 2009. The
Probabilistic Relevance Framework: Bm25 and Beyond. Foundations and Trends® in Information Retrieval, 3(4):333–389.
Jon Saad-Falcon, O. Khattab, Christopher Potts, and
Matei Zaharia. 2023. Ares: An Automated Evaluation Framework for Retrieval-Augmented Generation
Systems. arXiv, abs/2311.09476.
Sara Sarto, Marcella Cornia, Lorenzo Baraldi, and Rita
Cucchiara. 2022. Retrieval-Augmented Transformer
for Image Captioning. In International Conference
on Content-based Multimedia Indexing. ACM.
ES Shahul, Jithin James, Luis Espinosa Anke, and
S. Schockaert. 2023. Ragas: Automated Evaluation of Retrieval Augmented Generation. arXiv,
abs/2309.15217.
Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie
Huang, Nan Duan, and Weizhu Chen. 2023. Enhancing Retrieval-Augmented Large Language Models
with Iterative Retrieval-Generation Synergy. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 9248–9274. Association
for Computational Linguistics.
Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon
Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and
Wen-tau Yih. 2023. Replug: Retrieval-augmented
black-box language models. arXiv, abs/2301.12652.
Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and
Denny Zhou. 2023. Recitation-Augmented Language Models. In International Conference on Learning Representations (ICLR).
Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia,
Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara
Bahri, Tal Schuster, Huaixiu Steven Zheng, Denny
Zhou, Neil Houlsby, and Donald Metzler. 2023. Ul2:
Unifying Language Learning Paradigms. In International Conference on Learning Representations
(ICLR).
Yi Tay, Vinh Tran, Mostafa Dehghani, Jianmo Ni, Dara
Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao,
Jai Prakash Gupta, Tal Schuster, William W. Cohen,
and Donald Metzler. 2022. Transformer Memory
as a Differentiable Search Index. In Conference on
Neural Information Processing Systems (NeurIPS).
James Thorne, Andreas Vlachos, Christos
Christodoulopoulos, and Arpit Mittal. 2018.
Fever: a Large-scale Dataset for Fact Extraction
and VERification. In Proceedings of the 2018
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long Papers),
pages 809–819. Association for Computational
Linguistics.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothee Lacroix,
Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal
Azhar, and others. 2023a. Llama: Open and efficient
foundation language models. arXiv, abs/2302.13971.
Hugo Touvron, Louis Martin, Kevin Stone, Peter
Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
Shruti Bhosale, and others. 2023b. Llama 2: Open
foundation and fine-tuned chat models. arxiv,
abs/2307.09288.
Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot,
and Ashish Sabharwal. 2023. Interleaving Retrieval
with Chain-of-Thought Reasoning for KnowledgeIntensive Multi-Step Questions. In Proceedings of
the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
pages 10014–10037. Association for Computational
Linguistics.
Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is All
you Need. In Neural Information Processing Systems, pages 5998–6008.
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy,
and Samuel R. Bowman. 2019. Superglue: A Stickier Benchmark for General-Purpose Language Understanding Systems. In Conference on Neural Information Processing Systems (NeurIPS), pages 3261–
3275.
Ben Wang and Aran Komatsuzaki. 2021. GPT-J6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/
mesh-transformer-jax.
Liang Wang, Nan Yang, and Furu Wei. 2023a.
Query2doc: Query Expansion with Large Language
Models. In Proceedings of the 2023 Conference on
Empirical Methods in Natural Language Processing,
pages 9414–9423. Association for Computational
Linguistics.
Xintao Wang, Qian Yang, Yongting Qiu, Jiaqing Liang,
Qi He, Zhouhong Gu, Yanghua Xiao, and W. Wang.
2023b. Knowledgpt: Enhancing Large Language
Models with Retrieval and Storage Access on Knowledge Bases. arXiv, abs/2308.11761.
BigScience Workshop, Teven Le Scao, Angela Fan,
Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel
Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, and others. 2022. Bloom: A
176b-parameter open-access multilingual language
model. arXiv, abs/2211.05100.
Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,
Jialin Liu, Paul N. Bennett, Junaid Ahmed, and
Arnold Overwijk. 2021. Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text
Retrieval. In International Conference on Learning
Representations (ICLR).
Fangyuan Xu, Weijia Shi, and Eunsol Choi. 2023. Recomp: Improving Retrieval-Augmented LMs with
Compression and Selective Augmentation. arXiv,
abs/2310.04408.
Haoyan Yang, Zhitao Li, Yong Zhang, Jianzong Wang,
Ning Cheng, Ming Li, and Jing Xiao. 2023a. Prca:
Fitting Black-Box Large Language Models for Retrieval Question Answering via Pluggable RewardDriven Contextual Adapter. In Proceedings of the
2023 Conference on Empirical Methods in Natural
Language Processing, pages 5364–5375. Association
for Computational Linguistics.
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,
William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. Hotpotqa: A Dataset for
Diverse, Explainable Multi-hop Question Answering.
In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages
2369–2380. Association for Computational Linguistics.
Zhuolin Yang, Wei Ping, Zihan Liu, Vijay Korthikanti,
Weili Nie, De-An Huang, Linxi Fan, Zhiding Yu,
Shiyi Lan, Bo Li, Mohammad Shoeybi, Ming-Yu
Liu, Yuke Zhu, Bryan Catanzaro, Chaowei Xiao, and
Anima Anandkumar. 2023b. Re-ViLM: RetrievalAugmented Visual Language Model for Zero and
Few-Shot Image Captioning. In Findings of the Association for Computational Linguistics: EMNLP 2023,
pages 11844–11857. Association for Computational
Linguistics.
Shi Yu, Jiahua Liu, Jingqin Yang, Chenyan Xiong,
Paul N. Bennett, Jianfeng Gao, and Zhiyuan Liu.
2020. Few-shot generative conversational query
rewriting. In Proceedings of the 43rd International
ACM SIGIR conference on research and development
in Information Retrieval, SIGIR 2020, Virtual Event,
China, July 25-30, 2020, pages 1933–1936. ACM.
Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,
Mingxuan Ju, Soumya Sanyal, Chenguang Zhu,
Michael Zeng, and Meng Jiang. 2023a. Generate
rather than Retrieve: Large Language Models are
Strong Context Generators. In International Conference on Learning Representations (ICLR).
Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu,
Qingyun Wang, Heng Ji, and Meng Jiang. 2022. A
survey of knowledge-enhanced text generation. ACM
Comput. Surv., 54(11s):227:1–227:38.
Zichun Yu, Chenyan Xiong, Shi Yu, and Zhiyuan Liu.
2023b. Augmentation-Adapted Retriever Improves
Generalization of Language Models as Generic PlugIn. In Proceedings of the 61st Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 2421–2436. Association for
Computational Linguistics.
Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D
Plumbley, and Wenwu Wang. 2023. RetrievalAugmented Text-to-Audio Generation. arXiv,
abs/2309.08051.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, and others.
2022. Opt: Open pre-trained transformer language
models. arXiv, abs/2205.01068.
Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen,
Heng-Tze Cheng, E. Chi, Quoc V. Le, and Denny
Zhou. 2023. Take a Step Back: Evoking Reasoning
via Abstraction in Large Language Models. arXiv,
abs/2310.06117.
Honglei Zhuang, Zhen Qin, Rolf Jagerman, Kai Hui,
Ji Ma, Jing Lu, Jianmo Ni, Xuanhui Wang, and
Michael Bendersky. 2023. Rankt5: Fine-Tuning T5
for Text Ranking with Ranking Losses. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information
Retrieval. ACM."